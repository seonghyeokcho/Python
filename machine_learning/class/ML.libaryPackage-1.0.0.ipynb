{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import randint, uniform\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager, rc\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from imblearn.combine import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import make_classification\n",
    "from collections import Counter\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "%matplotlib inline\n",
    "\n",
    "# 한글 설정\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# font_list = fm.findSystemFonts(fontpaths=None, fontext='ttf')\n",
    "# ttf 폰트 전체개수\n",
    "# print(len(font_list))\n",
    "# [(f.name, f.fname) for f in fm.fontManager.ttflist if 'Gothic' in f.name]\n",
    "\n",
    "#font_path = 'C:\\\\WINDOWS\\\\Fonts\\\\malgunsl.ttf'\n",
    "#font_prop = fm.FontProperties(fname=font_path, size=12)\n",
    "#font_name = font_prop.get_name()\n",
    "\n",
    "#rc('font', family=font_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values 처리 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측값 확인\n",
    "# input : dataframe 객체\n",
    "#   checkData : 확인할 데이터\n",
    "#\n",
    "# output : feature 별 null 값 건수\n",
    "def have_missing_value(checkData):\n",
    "    try:       \n",
    "        \n",
    "        data = checkData.isnull().sum();\n",
    "        \n",
    "        return data;\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치형 결측치 처리\n",
    "# input : dataframe 객체\n",
    "#   checkData : 확인할 데이터\n",
    "#   cont       : 수치형 Features\n",
    "#   imputeValue : 대체할 값  \n",
    "# output : feature 별 null 값 건수\n",
    "def Impute_missing_value_numeric(checkData, cont, imputeValue):\n",
    "    try:\n",
    "        \n",
    "        data = checkData.fillna(value=imputeValue, inplace=True)\n",
    "            \n",
    "        return data;\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범주형 결측치 One-hot encoding 처리\n",
    "# input : dataframe 객체\n",
    "#   checkData : 확인할 데이터\n",
    "#   cate      : encoding 할 Feature \n",
    "# output : feature 별 null 값 건수\n",
    "def Impute_missing_value_categorical(checkData, cate):\n",
    "    try:\n",
    "        \n",
    "        for i in checkData[cate].dtypes.index:\n",
    "            \n",
    "            print(\"['\"+ i +\"'].astype('category').cat.categories\")\n",
    "            checkData[i].astype('category').cat.categories\n",
    "            \n",
    "            checkData[cate] = checkData[cate].apply(lambda x: x.astype('category').cat.codes)\n",
    "        \n",
    "        return  checkData\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>비대칭 데이터 처리</font>\n",
    "- 비대칭 데이터는 데이터 비율을 맞추면 정밀도(precision)가 향상된다.\n",
    " - 오버샘플링(Over-Sampling)  : 소수 클래스 데이터를 추가함.\n",
    " - 언더샘플링(Under-Sampling) : 다수 클래스 데이터에서 일부만 사용함. Tomek links,\n",
    " - 복합샘플링(Combining Over-and Under-Sampling) : SMOTE + Tomek\n",
    " - 비대칭 데이터 확인 :  trainData.gender.value_counts()\n",
    "- pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomek links :  언더샘플링(Under-Sampling) : 다수 클래스 데이터에서 일부만 사용함\n",
    "# input : X_train, y_train\n",
    "#  X_train : \n",
    "#  y_train\n",
    "# output : print\n",
    "def under_Sampling_DecisionTree(X_train, y_train):\n",
    "    try:\n",
    "        XX, yy = TomekLinks().fit_sample(X_train, y_train)\n",
    "        tree0 = DecisionTreeClassifier(max_depth=4, random_state=0)\n",
    "        tree0.fit(XX, yy)\n",
    "        y_pred0 = tree0.predict(X_test)\n",
    "\n",
    "        print(classification_report(y_test, y_pred0))\n",
    "\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smot+Tomek links :  복합샘플링(Combining Over-and Under-Sampling) \n",
    "# input : X_train, y_train\n",
    "#  X_train : \n",
    "#  y_train\n",
    "# output : print\n",
    "def combining_Sampling_DecisionTree(X_train, y_train):\n",
    "    try:\n",
    "        XX, yy = SMOTETomek(random_state=0).fit_sample(X_train, y_train)\n",
    "\n",
    "        tree3 = DecisionTreeClassifier(max_depth=4, random_state=0)\n",
    "        tree3.fit(XX, yy)\n",
    "        y_pred3 = tree3.predict(X_test)\n",
    "\n",
    "        print(classification_report(y_test, y_pred3))\n",
    "\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할(Split Data)\n",
    "# input : dataframe 객체\n",
    "#   objData    : 분리할 대상 데이터\n",
    "#   delCols   : 비분석 Features\n",
    "#   dfy       : target Feature\n",
    "#   testSize  : 분리할 테스트 사이즈(%)\n",
    "# output : X_train, X_test, y_train, y_test\n",
    "\n",
    "def data_split_train_test(objData, delCols, dfy, testSize):\n",
    "    try:\n",
    "        objData = objData.drop(delCols, axis=1)\n",
    "        dfY = objData[dfy]\n",
    "        dfX = objData.drop(dfy, axis=1)\n",
    "        \n",
    "        return train_test_split(dfX, dfY, test_size=testSize, random_state=0)\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree 모델 적용\n",
    "# input : 데이터 분할로 얻은 X_train, X_test, y_train, y_test\n",
    "# output : X_train, X_test, y_train, y_test\n",
    "def decisionTree_modeling(X_train, X_test, y_train, y_test):\n",
    "    try:\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        from sklearn.metrics import classification_report\n",
    "        \n",
    "        tree = DecisionTreeClassifier(max_depth=6, random_state=0)\n",
    "        tree.fit(X_train, y_train)\n",
    "        pred_tree = tree.predict(X_test); \n",
    "       # print(\"tree score ::: {}\".format(d_tree_score))\n",
    "        \n",
    "        return print(classification_report(y_test, pred_tree))\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM 모델 적용\n",
    "# input : 데이터 분할로 얻은 X_train, X_test, y_train, y_test\n",
    "# output : X_train, X_test, y_train, y_test\n",
    "def svm_modeling(X_train, X_test, y_train, y_test):\n",
    "    try:\n",
    "        from sklearn.svm import SVC \n",
    "        svm = SVC(random_state=0)\n",
    "        svm.fit(X_train, y_train)\n",
    "        pred_svm = svm.predict(X_test);\n",
    "        \n",
    "        return print(classification_report(y_test, pred_svm))\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network 모델 적용\n",
    "# input : 데이터 분할로 얻은 X_train, X_test, y_train, y_test\n",
    "# output : X_train, X_test, y_train, y_test\n",
    "def neural_network_modeling(X_train, X_test, y_train, y_test):\n",
    "    try:\n",
    "        from sklearn.neural_network import MLPClassifier \n",
    "        mlp = MLPClassifier()\n",
    "        mlp.fit(X_train, y_train)\n",
    "        pred_mlp = mlp.predict(X_test);\n",
    "        \n",
    "        return print(classification_report(y_test, pred_mlp))\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bestModel\n",
    "# input : model 객체\n",
    "#   model    : 적용 알고리즘\n",
    "#   nFolds   : 분할 개수\n",
    "#  \n",
    "# output : best parameter\n",
    "def bestModel(model, nFolds, searchCV, X_data, y_data, isScaler, isPloy, isFeatureUnion, scoring, nJobs, nIter, verbose):\n",
    "    # GridSearchCV을 위해 파라미터 값을 제한함.\n",
    "    stepss = []\n",
    "    grd_prams = {}\n",
    "\n",
    "    if isPloy == True:\n",
    "        stepss.append(('polynomialfeatures', PolynomialFeatures()))\n",
    "        grd_prams.update({'polynomialfeatures__degree':[1, 2]})\n",
    "        \n",
    "        if isFeatureUnion == True:\n",
    "            # create feature union\n",
    "            features = []\n",
    "            features.append(('pca', PCA(n_components=3)))\n",
    "            features.append(('univ_select', SelectKBest(k=10)))\n",
    "            stepss.append(('features', FeatureUnion(features)))       \n",
    "\n",
    "    if isScaler == 'STANDARD':\n",
    "        stepss.append(('standardscaler', StandardScaler()))\n",
    "    else:\n",
    "        stepss.append(('minmaxscaler', MinMaxScaler()))\n",
    "            \n",
    "    if model == 'SVC':\n",
    "        stepss.append(('svc', SVC(random_state=0, C=100)))\n",
    "        grd_prams.update({'svc__C':[0.1, 1, 10, 100], 'svc__gamma':[0.001, 0.01, 0.05, 0.1, 1, 10]})\n",
    "    elif model == 'XGB': \n",
    "        stepss.append(('xgb', XGBClassifier(random_state=0, objective='binary:logistic')))\n",
    "        grd_prams.update({'xgb__n_estimators': [300, 500],\n",
    "            'xgb__learning_rate': [0.001, 0.01],\n",
    "            'xgb__subsample': [0.5, 1],\n",
    "            'xgb__max_depth': [5, 6],\n",
    "            'xgb__colsample_bytree': [0.97, 1.24],\n",
    "            'xgb__min_child_weight': [1, 2],\n",
    "            'xgb__gamma': [0.001, 0.005],\n",
    "            'xgb__nthread': [3, 4],\n",
    "            'xgb__reg_lambda': [0.5, 1.0],\n",
    "            'xgb__reg_alpha': [0.01, 0.1]\n",
    "          })\n",
    "        \n",
    "    elif model == 'LGBM':\n",
    "        # 그래디언트 부스팅 결정 트리(GBDT) : GOSS와 EFB 적용하여 GBDT를 새롭게 구현한 알고리즘\n",
    "        stepss.append(('lgbm', LGBMClassifier(random_state=0, boosting_type='gbdt', objective='binary', metric='auc')))\n",
    "        grd_prams.update({'lgbm__max_depth': [50, 100],\n",
    "              'lgbm__learning_rate' : [0.01, 0.05],\n",
    "              'lgbm__num_leaves': [150, 200],\n",
    "              'lgbm__n_estimators': [300, 400],\n",
    "              'lgbm__num_boost_round':[4000, 5000],\n",
    "              'lgbm__subsample': [0.5, 1],\n",
    "              'lgbm__reg_alpha': [0.01, 0.1],\n",
    "              'lgbm__reg_lambda': [0.01, 0.1],\n",
    "              'lgbm__min_data_in_leaf': [20, 30],\n",
    "              'lgbm__lambda_l1': [0.01, 0.1],\n",
    "              'lgbm__lambda_l2': [0.01, 0.1]\n",
    "            })\n",
    "        \n",
    "    pipe = Pipeline(steps=stepss)\n",
    "    \n",
    "    cv = StratifiedShuffleSplit(n_splits=nFolds, test_size=nFolds, random_state=0)\n",
    "    grid = GridSearchCV(pipe, param_grid=grd_prams, n_jobs=nJobs, scoring=scoring, verbose=verbose, cv=cv)\n",
    "    \n",
    "    if searchCV == 'RANDOM':\n",
    "        grid = RandomizedSearchCV(pipe, param_distributions=grd_prams, n_iter=nIter, scoring=scoring, error_score=3, verbose=verbose, n_jobs=nJobs, cv=cv)\n",
    "\n",
    "    grid.fit(X_data, y_data)\n",
    "    \n",
    "    return grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 서치\n",
    "def bestGBDTNextModel(model, isKfold, nfold, searchCV, Xtrain, ytrain, Xtest, ytest, nIter, scoring, errScore, verbose, nJobs):\n",
    "    # GridSearchCV을 위해 파라미터 값을 제한함.\n",
    "    grd_prams = {}\n",
    "    classifier = XGBClassifier(random_state=0, objective='binary:logistic')\n",
    "    cv = KFold(n_splits=nfold, shuffle=True, random_state=0)\n",
    "    \n",
    "    if model == 'LGBM':\n",
    "        # 그래디언트 부스팅 결정 트리(GBDT)    \n",
    "        grd_prams.update({'max_depth': [50, 100],\n",
    "              'learning_rate' : [0.01, 0.05],\n",
    "              'num_leaves': [150, 200],\n",
    "              'n_estimators': [300, 400],\n",
    "              'num_boost_round':[4000, 5000],\n",
    "              'subsample': [0.5, 1],\n",
    "              'reg_alpha': [0.01, 0.1],\n",
    "              'reg_lambda': [0.01, 0.1],\n",
    "              'min_data_in_leaf': [20, 30],\n",
    "              'lambda_l1': [0.01, 0.1],\n",
    "              'lambda_l2': [0.01, 0.1]\n",
    "            })\n",
    "        \n",
    "        #grd_prams.update({'max_depth': [50, 75, 90, 100],\n",
    "        #      'learning_rate' : [0.01, 0.05, 0.07, 0.1],\n",
    "        #      'num_leaves': [300,600,900,1200],\n",
    "        #      'n_estimators': [100, 300, 500, 900],\n",
    "        #      'num_boost_round':[1000, 2000, 3000, 4000],\n",
    "        #      'num_leaves': [30, 60, 120, 150, 200],\n",
    "        #      'reg_alpha': [0.01, 0.1, 0.5, 0.7, 1.0],\n",
    "        #      'min_data_in_leaf': [50, 100, 300, 800],\n",
    "        #      'lambda_l1': [0, 0.1, 0.5, 1.0],\n",
    "        #      'lambda_l2': [0, 0.01, 1.0]})\n",
    "        \n",
    "        classifier = LGBMClassifier(random_state=0, boosting_type='gbdt', objective='binary', metric='auc')\n",
    "        \n",
    "    elif model == 'XGB':\n",
    "        grd_prams.update({'n_estimators': [300, 500],\n",
    "            'learning_rate': [0.001, 0.01],\n",
    "            'subsample': [0.5, 1],\n",
    "            'max_depth': [5, 6],\n",
    "            'colsample_bytree': [0.97, 1.24],\n",
    "            'min_child_weight': [1, 2],\n",
    "            'gamma': [0.001, 0.005],\n",
    "            'nthread': [3, 4],\n",
    "            'reg_lambda': [0.5, 1.0],\n",
    "            'reg_alpha': [0.01, 0.1]\n",
    "          })\n",
    "        \n",
    "        #grd_prams.update({'n_estimators': [300, 500, 700],\n",
    "        #    'learning_rate': [0.01, 0.03, 0.05, 0.07, 0.09],\n",
    "        #    'subsample': [0.5, 1],\n",
    "        #    'max_depth': [4, 5, 6, 7, 8, 9, 10],\n",
    "        #    'colsample_bytree': [0.52, 0.97, 1,55, 2.32, 3.46],\n",
    "        #    'min_child_weight': [1, 2, 3, 4],\n",
    "        #    'gamma': [0.001, 0.01, 0.1, 0, 1],\n",
    "        #    'nthread': [3, 4, 5],\n",
    "        #    'reg_lambda': [0.01, 0.1, 0.5, 0.7, 1.0],\n",
    "        #    'reg_alpha': [0.01, 0.1, 0.5, 0.7, 1.0]\n",
    "        #  })\n",
    "    \n",
    "    if isKfold == False:\n",
    "        cv = StratifiedShuffleSplit(n_splits=nfold, test_size=0.2, random_state=0)\n",
    "    \n",
    "    grid_ = RandomizedSearchCV(classifier, param_distributions=grd_prams, n_iter=nIter, scoring=scoring, error_score=errScore, verbose=verbose, n_jobs=nJobs, cv=cv)\n",
    "\n",
    "    # 속도 이슈\n",
    "    if searchCV == 'GRID': \n",
    "        grid_ = GridSearchCV(classifier, param_grid=grd_prams, n_jobs=nJobs, scoring=scoring, verbose=verbose, cv=cv)\n",
    "    \n",
    "    grid_.fit(Xtrain, ytrain)\n",
    "    score_ = grid_.score(Xtest, ytest)\n",
    "    \n",
    "    #best = {\"best_param\":grid_.best_params_, \n",
    "    #        \"best_score\":grid_.best_score_, \n",
    "    #        \"best_estimator\":grid_.best_estimator_,\n",
    "    #        \"test_score\":score_\n",
    "    #       }\n",
    "    \n",
    "    print(\"{} grid_.best_score {}\".format(model, np.round(grid_.best_score_,3)))\n",
    "    print(\"{} grid_.best_score {}\".format(model, np.round(score_,3)))\n",
    "    print(\"{} best_estimator {}\".format(model, grid_.best_estimator_))\n",
    "\n",
    "    return grid_.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawSMOTETomek(X_org, y_org, X_pca, y_resampled, title1, title2, xlim, ylim, xticks, yticks):\n",
    "    # Two subplots, unpack the axes array immediately\n",
    "    f, (ax1, ax2) = plt.subplots(1,2,figsize=(18,8))\n",
    "\n",
    "    c0 = ax1.scatter(X_org[y_org == 0, 0], X_org[y_org == 0, 1], label=\"Class #0\", alpha=0.5)\n",
    "    c1 = ax1.scatter(X_org[y_org == 1, 0], X_org[y_org == 1, 1], label=\"Class #1\", alpha=0.5)\n",
    "    ax1.set_title(title1)\n",
    "\n",
    "    ax2.scatter(X_pca[y_resampled == 0, 0], X_pca[y_resampled == 0, 1], label=\"Class #0\", alpha=0.5)\n",
    "    ax2.scatter(X_pca[y_resampled == 1, 0], X_pca[y_resampled == 1, 1], label=\"Class #1\", alpha=0.5)\n",
    "    ax2.set_title(title2)\n",
    "\n",
    "    # make nice plotting\n",
    "    for ax in (ax1, ax2):\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.get_xaxis().tick_bottom()\n",
    "        ax.get_yaxis().tick_left()\n",
    "        ax.spines['left'].set_position(('outward', 10))\n",
    "        ax.spines['bottom'].set_position(('outward', 10))\n",
    "        ax.set_xlim(xlim)      # [-1, 10]\n",
    "        ax.set_ylim(ylim)      # [-5, 20]\n",
    "        ax.set_xticks(xticks)  # range(-1, 10)\n",
    "        ax.set_yticks(yticks)  # range(-5, 20)\n",
    "\n",
    "    plt.figlegend((c0, c1), ('Class #0', 'Class #1'), loc='lower center', ncol=2, labelspacing=0.)\n",
    "    plt.tight_layout(pad=3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
