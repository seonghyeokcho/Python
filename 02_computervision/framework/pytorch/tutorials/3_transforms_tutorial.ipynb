{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n`\ud30c\uc774\ud1a0\uce58(PyTorch) \uae30\ubcf8 \uc775\ud788\uae30 <intro.html>`_ ||\n`\ube60\ub978 \uc2dc\uc791 <quickstart_tutorial.html>`_ ||\n`\ud150\uc11c(Tensor) <tensorqs_tutorial.html>`_ ||\n`Dataset\uacfc Dataloader <data_tutorial.html>`_ ||\n**\ubcc0\ud615(Transform)** ||\n`\uc2e0\uacbd\ub9dd \ubaa8\ub378 \uad6c\uc131\ud558\uae30 <buildmodel_tutorial.html>`_ ||\n`Autograd <autogradqs_tutorial.html>`_ ||\n`\ucd5c\uc801\ud654(Optimization) <optimization_tutorial.html>`_ ||\n`\ubaa8\ub378 \uc800\uc7a5\ud558\uace0 \ubd88\ub7ec\uc624\uae30 <saveloadrun_tutorial.html>`_\n\n\ubcc0\ud615(Transform)\n==========================================================================\n\n\ub370\uc774\ud130\uac00 \ud56d\uc0c1 \uba38\uc2e0\ub7ec\ub2dd \uc54c\uace0\ub9ac\uc998 \ud559\uc2b5\uc5d0 \ud544\uc694\ud55c \ucd5c\uc885 \ucc98\ub9ac\uac00 \ub41c \ud615\ud0dc\ub85c \uc81c\uacf5\ub418\uc9c0\ub294 \uc54a\uc2b5\ub2c8\ub2e4.\n**\ubcc0\ud615(transform)** \uc744 \ud574\uc11c \ub370\uc774\ud130\ub97c \uc870\uc791\ud558\uace0 \ud559\uc2b5\uc5d0 \uc801\ud569\ud558\uac8c \ub9cc\ub4ed\ub2c8\ub2e4.\n\n\ubaa8\ub4e0 TorchVision \ub370\uc774\ud130\uc14b\ub4e4\uc740 \ubcc0\ud615 \ub85c\uc9c1\uc744 \uac16\ub294, \ud638\ucd9c \uac00\ub2a5\ud55c \uac1d\uccb4(callable)\ub97c \ubc1b\ub294 \ub9e4\uac1c\ubcc0\uc218 \ub450\uac1c\n( \ud2b9\uc9d5(feature)\uc744 \ubcc0\uacbd\ud558\uae30 \uc704\ud55c ``transform`` \uacfc \uc815\ub2f5(label)\uc744 \ubcc0\uacbd\ud558\uae30 \uc704\ud55c ``target_transform`` )\ub97c \uac16\uc2b5\ub2c8\ub2e4\n`torchvision.transforms <https://pytorch.org/vision/stable/transforms.html>`_ \ubaa8\ub4c8\uc740\n\uc8fc\ub85c \uc0ac\uc6a9\ud558\ub294 \uba87\uac00\uc9c0 \ubcc0\ud615(transform)\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.\n\nFashionMNIST \ud2b9\uc9d5(feature)\uc740 PIL Image \ud615\uc2dd\uc774\uba70, \uc815\ub2f5(label)\uc740 \uc815\uc218(integer)\uc785\ub2c8\ub2e4.\n\ud559\uc2b5\uc744 \ud558\ub824\uba74 \uc815\uaddc\ud654(normalize)\ub41c \ud150\uc11c \ud615\ud0dc\uc758 \ud2b9\uc9d5(feature)\uacfc \uc6d0-\ud56b(one-hot)\uc73c\ub85c \ubd80\ud638\ud654(encode)\ub41c \ud150\uc11c \ud615\ud0dc\uc758\n\uc815\ub2f5(label)\uc774 \ud544\uc694\ud569\ub2c8\ub2e4. \uc774\ub7ec\ud55c \ubcc0\ud615(transformation)\uc744 \ud558\uae30 \uc704\ud574 ``ToTensor`` \uc640 ``Lambda`` \ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor, Lambda\n\nds = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor(),\n    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ToTensor()\n------------------------------------------------------------------------------------------\n\n`ToTensor <https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ToTensor>`_\n\ub294 PIL Image\ub098 NumPy ``ndarray`` \ub97c  ``FloatTensor`` \ub85c \ubcc0\ud658\ud558\uace0, \uc774\ubbf8\uc9c0\uc758 \ud53d\uc140\uc758 \ud06c\uae30(intensity) \uac12\uc744 [0., 1.] \ubc94\uc704\ub85c\n\ube44\ub840\ud558\uc5ec \uc870\uc815(scale)\ud569\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lambda \ubcc0\ud615(Transform)\n------------------------------------------------------------------------------------------\n\nLambda \ubcc0\ud615\uc740 \uc0ac\uc6a9\uc790 \uc815\uc758 \ub78c\ub2e4(lambda) \ud568\uc218\ub97c \uc801\uc6a9\ud569\ub2c8\ub2e4. \uc5ec\uae30\uc5d0\uc11c\ub294 \uc815\uc218\ub97c \uc6d0-\ud56b\uc73c\ub85c \ubd80\ud638\ud654\ub41c \ud150\uc11c\ub85c \ubc14\uafb8\ub294\n\ud568\uc218\ub97c \uc815\uc758\ud569\ub2c8\ub2e4.\n\uc774 \ud568\uc218\ub294 \uba3c\uc800 (\ub370\uc774\ud130\uc14b \uc815\ub2f5\uc758 \uac1c\uc218\uc778) \ud06c\uae30 10\uc9dc\ub9ac \uc601 \ud150\uc11c(zero tensor)\ub97c \ub9cc\ub4e4\uace0,\n`scatter_ <https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html>`_ \ub97c \ud638\ucd9c\ud558\uc5ec\n\uc8fc\uc5b4\uc9c4 \uc815\ub2f5 ``y`` \uc5d0 \ud574\ub2f9\ud558\ub294 \uc778\ub371\uc2a4\uc5d0 ``value=1`` \uc744 \ud560\ub2f9\ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "target_transform = Lambda(lambda y: torch.zeros(\n    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------------------------\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ub354 \uc77d\uc5b4\ubcf4\uae30\n~~~~~~~~~~~~~~~~~\n- `torchvision.transforms API <https://pytorch.org/vision/stable/transforms.html>`_\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}