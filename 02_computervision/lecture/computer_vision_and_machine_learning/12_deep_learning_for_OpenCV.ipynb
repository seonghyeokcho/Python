{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"#CC3D3D\">Case #1: Deep Learning</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Step #1: Concept\n",
    "<hr>\n",
    "\n",
    "- 딥러닝(Deep Learning)\n",
    "    - 2000년대부터 사용되고 있는 심층 신경망(deep neural network)의 또 다른 이름이다.\n",
    "    - \"Deep learning is part of a broader family of machone learning methods based on artificial<br>\n",
    "    neural networks.\" - [wikipedia, Deep_learning](https://en.wikipedia.org/wiki/Deep_learning)\n",
    "    - <img src=\"images/markdown/.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Step #2: ML vs. DL\n",
    "<hr>\n",
    "\n",
    "- <img src=\"images/markdown/.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Step #3: Deep Learning Application\n",
    "<hr>\n",
    "\n",
    "- <img src=\"images/markdown/.png\" width=\"600\">\n",
    "- <img src=\"images/markdown/.png\" width=\"600\">\n",
    "- <img src=\"images/markdown/.png\" width=\"600\">\n",
    "- <img src=\"images/markdown/.png\" width=\"600\">\n",
    "- <img src=\"images/markdown/.png\" width=\"600\">\n",
    "- <img src=\"images/markdown/.png\" width=\"600\">\n",
    "- <img src=\"images/markdown/.png\" width=\"600\">\n",
    "- <img src=\"images/markdown/.png\" width=\"600\">\n",
    "- <img src=\"images/markdown/.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Step #4: Neural Network Basics\n",
    "<hr>\n",
    "\n",
    "- 퍼셉트론(perceptron)\n",
    "    - 다수의 입력으로부터 가중합을 계산하고, 이를 이용하여 하나의 출력을 만들어내는 구조(1950년대)\n",
    "    - <img src=\"images/markdown/.png\" width=\"600\">\n",
    "    - <img src=\"images/markdown/.png\" width=\"600\">\n",
    "\n",
    "<hr>\n",
    "\n",
    "- 퍼셉트론에 의한 OR 연산 구현\n",
    "    - <img src=\"images/markdown/.png\" width=\"600\">\n",
    "\n",
    "<hr>\n",
    "\n",
    "- 활성화 함수(activation function)\n",
    "    - 생물학적 뉴런(neuron)에서 입력 신호가 `일정 크기 이상일 때만 신호를 전달`하는 메커니즘을 모방했다.\n",
    "    - 비선형 함수를 사용한다.\n",
    "    - <img src=\"images/markdown/.png\" width=\"600\">\n",
    "\n",
    "<hr>\n",
    "\n",
    "- 퍼셉트론에 의한 XOR 연산 구현\n",
    "    - XOR는 단순한 입력-출력 구조의 퍼셉트론으로 구현 불가\n",
    "        - i.e., XOR는 선형이 아니다.\n",
    "    - <img src=\"images/markdown/.png\" width=\"600\">\n",
    "\n",
    "<hr>\n",
    "\n",
    "- (단층) 퍼셉트론 vs. 다층 퍼셉트론 vs. 심층 신경망\n",
    "    - 다층 퍼셉트론(MLP, Multi Layer Perceptron): 은닉층(hidden layer)이 한 개 이상\n",
    "    - 심층 신경망(DNN, Deep Neural Network): 은닉층(hidden layer)이 두 개 이상\n",
    "    - <img src=\"images/markdown/.png\" width=\"600\">\n",
    "\n",
    "<hr>\n",
    "\n",
    "- 신경망 학습\n",
    "    - 신경망 학습이란 좋은 `가중치(weight)`를 결정하는 방법을 의미한다. 이는 곧 `신경망 학습의 결과`이다.\n",
    "    - 신경망 학습 방법\n",
    "        - 학습의 기준이 되는 `비용(cost)` 또는 `손실(loss)` 함수를 정의한 후, 비용을 `최소화`하는 방향으로 학습을 진행한다.\n",
    "        - `경사 하강법(gradient descent)` 또는 `오류역전파(error backpropagation)` 알고리즘을 사용한다.\n",
    "            - 경사 하강법: 그래디언트 반대 방향으로 이동하면서 최솟값 위치를 찾는 방법\n",
    "            - 오류 역전파: 미분의 체인룰(chain-rule)을 이용하여 전체 가중치를 업데이트하는 방법\n",
    "\n",
    "$$\\cdot\\ e.g.,\\ cost={\\textstyle\\sum_{i}}(y_i-x_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Step #4-1: Problem & Solution of DNN\n",
    "<hr>\n",
    "\n",
    "- 학습이 제대로 안 되는 현상\n",
    "    - `P`: 사라지는 기울기(Vanishing Gradient) -> `S`: `ReLU(Rectified LInear Units)`\n",
    "    - `P`: 과적합(Overfitting) -> `S`: Regularization(Dropout, Batch Normalization)\n",
    "    - `P`: 가중치 초기화(Weight initialization) -> `S`: Random initialization, Xavier method, etc.\n",
    "- 학습이 너무 느린 점\n",
    "    - `P`: 하드웨어 성능이 뒷받침 되지 않음 -> `S`: CPU, `GPU` 성능 발전\n",
    "    - `P`: 경사 하강법의 느린 속도 -> `S`: SGD, Mini-Batch, Adam method\n",
    "- 부족한 데이터 셋\n",
    "    - e.g., 일반적인 영상 인식에 쓰이는 영상 데이터 셋의 크기는 10만 ~ 100만이다.\n",
    "    -> `S`: 디지털 카메라 보급, 인터넷 발전, `Large dataset(ImageNet, COCO, etc.)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"#CC3D3D\">Case #2: CNN</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Step #1: Concept\n",
    "<hr>\n",
    "\n",
    "- 컨볼루션 신경망(Convolutional Neural Network)\n",
    "    - 영상 인식 등을 위한 딥러닝에 특화된 네트워크 구조\n",
    "        - 일반적인 구조: 컨볼루션(convolution) + 풀링(pooling) + ... + 완전 연결 레이어(FC, Fully Connected Layer)\n",
    "        - <img src=\"images/markdown/.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Step #2: About CNN\n",
    "<hr>\n",
    "\n",
    "- 컨볼루션 레이어(Convolution Layer)\n",
    "    - 2차원 영상에서 유효한 `특징(feature)`을 찾아내는 역할을 담당한다.\n",
    "    - 유용한 필터 마스크가 학습에 의해 결정된다.\n",
    "    - 보통 ReLU 활성화 함수(activation function)를 함께 사용한다.\n",
    "    - <img src=\"images/markdown/.png\" width=\"600\">\n",
    "    - > image from: [http://cs231n.stanford.edu/](http://cs231n.stanford.edu/)\n",
    "\n",
    "<hr>\n",
    "\n",
    "- 풀링 레이어(Pooling Layer)\n",
    "    - 유용한 정보는 유지하면서 입력 크기를 줄임으로써 `과적합(overfitting)을 예방`하고 `계산량을 감소`시키는 효과를 낸다.\n",
    "    - `최대 풀링(max pooling)` 또는 `평균 풀링(average pooling)`을 사용한다.\n",
    "    - 학습이 따로 필요 없다.\n",
    "    - <img src=\"images/markdown/.png\" width=\"600\">\n",
    "    - > image from: [http://cs231n.stanford.edu/](http://cs231n.stanford.edu/)\n",
    "\n",
    "<hr>\n",
    "\n",
    "- 완전 연결 레이어(Fully Connected Layer)\n",
    "    - 3차원 구조의 `activation map(H x W x C)`의 모든 값을 일렬로 이어 붙인다.\n",
    "    - `인식(recognition)`의 경우, `소프트맥스(softmax) 레이어`를 추가하여 각 `클래스에 대한 확률 값`을 결과로 얻는다.\n",
    "    - <img src=\"images/markdown/.png\" width=\"600\">\n",
    "    - > image from: [https://www.slideshare.net/JinwonLee9/ss-70446412](https://www.slideshare.net/JinwonLee9/ss-70446412)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Step #3: Examples CNN\n",
    "<hr>\n",
    "\n",
    "- 학습된 컨볼루션 레이어 필터의 예\n",
    "    - <img src=\"images/markdown/.png\" width=\"600\">\n",
    "\n",
    "<hr>\n",
    "\n",
    "- 필기체 숫자 인식을 위한 LeNet-5(LeCun et al., 1998)\n",
    "    - CNN 원조\n",
    "    - 28x28 필기체 숫자 영상을 32x32로 확장하여 만든 입력 데이터를 사용한다.\n",
    "    - 전체는 7개의 레이어로 이루어져 있다.\n",
    "        - Conv-Pool-Conv-Pool-FC-FC-FC\n",
    "    - <img src=\"images/markdown/.png\" width=\"600\">\n",
    "\n",
    "<hr>\n",
    "\n",
    "- AlexNet(Krizhevsky et al., 2012)\n",
    "    - 2012년 ILSVRC(ImageNet Large Scale Visual Recognition Challenge) 영상 인식 분야에서 1위를 차지한 알고리즘이다.\n",
    "        - 1000개의 카테고리, 120만개의 훈련 영상, 15만개의 테스트 영상을 사용했다.\n",
    "    - Top-5 Error: `15.4%` (다른 컴퓨터 비전 기반 방법들 > 25%)\n",
    "    - 하드웨어의 제약으로 2개의 GPU 사용\n",
    "    - <img src=\"images/markdown/.png\" width=\"600\">\n",
    "    - > image from: [https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n",
    "\n",
    "<hr>\n",
    "\n",
    "- VGG-16(Simonyan and Zisserman, 2014)\n",
    "    - 2014년 ILSVRC(ImageNet Large Scale Visual Recognition Challenge) 영상 인식 분야에서 2위를 차지한 알고리즘이다.\n",
    "    - Top-5 Error: `7.3%`\n",
    "        - 후에 알고리즘을 업그레이드하여 `6.8%`까지 오차율을 내린 결과를 논문으로 발표하였다.\n",
    "    - 컨볼루션 레이어에서 3x3 필터만 사용했다.\n",
    "    - 총 16개의 레이어로 구성되었다.\n",
    "    - <img src=\"images/markdown/.png\" width=\"600\">\n",
    "    - > image from: [https://excelsior-cjh.tistory.com/160](https://excelsior-cjh.tistory.com/160)\n",
    "    - > thesis: [https://arxiv.org/pdf/1409.1556.pdf](https://arxiv.org/pdf/1409.1556.pdf)\n",
    "\n",
    "<hr>\n",
    "\n",
    "- GoogLeNet(Szegedy er al., 2014)\n",
    "    - 2014년 ILSVRC(ImageNet Large Scale Visual Recognition Challenge) 영상 인식 분야에서 1위를 차지한 알고리즘이다.\n",
    "    - Top-5 Error: `6.7%`\n",
    "        - i.e., 사람의 Top-5 Error는 `5.1%`이다.\n",
    "    - 총 22개의 레이어로 구성되었다.\n",
    "    - Inception 모듈\n",
    "    - <img src=\"images/markdown/.png\" width=\"600\">\n",
    "    - > image from: [https://arxiv.org/pdf/1409.4842.pdf](https://arxiv.org/pdf/1409.4842.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Step #4: Implementation DNN\n",
    "<hr>\n",
    "\n",
    "- Tensorflow 1.x 로 필기체 숫자 인식 학습하기\n",
    "    - OpenCV DNN 모듈에서 이용할 목적으로 Tensorflow를 이용하여 필기체 숫자 인식을 학습하고, 그 결과를 pb 파일로 저장하기\n",
    "    - 네트워크 구조: [Conv-Pool-Conv-Pool-FC-FC-FC]\n",
    "    - 학습 데이터: `MNIST 데이터셋`\n",
    "        - Yann LeCun 교수가 필기체 숫자 인식을 위해 사용했던 데이터셋\n",
    "        - 각각의 숫자는 28x28 크기의 0~1 사이의 실수값으로 구성된 영상 데이터\n",
    "        - 60,000개의 훈련용 영상과 10,000개의 테스트 영상으로 구성되어 있다.\n",
    "\n",
    "<hr>\n",
    "\n",
    "- 준비 사항\n",
    "    - Tensorflow 1.13.1 설치(어떤 임의의 버전을 사용해도 좋음)\n",
    "        - > terminal or CMD -> pip install tensorflow==1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting ./MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting ./MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow MNIST 학습: 네트워크 모델 만들기\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./MNIST_data/\", one_hot=True)\n",
    "\n",
    "# set hyper parameters\n",
    "learning_rate = 1e-3\n",
    "training_epochs = 20\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, 28, 28, 1], name=\"data\")\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "conv1 = tf.layers.conv2d(inputs=X, filters=10, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
    "pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2, padding=\"same\")\n",
    "\n",
    "conv2 = tf.layers.conv2d(pool1, 20, [3, 3], padding=\"same\", activation=tf.nn.relu)\n",
    "pool2 = tf.layers.max_pooling2d(conv2, [2, 2], 2, \"same\")\n",
    "\n",
    "fc1 = tf.contrib.layers.flatten(pool2)\n",
    "fc2 = tf.layers.dense(inputs=fc1, units=200, activation=tf.nn.relu)\n",
    "logits = tf.layers.dense(fc2, 10, tf.nn.relu)\n",
    "output = tf.nn.softmax(logits, name=\"probability\")\n",
    "\n",
    "# 비용 함수(cost func) or 손실 함수(loss func) 지정\n",
    "cost = tf.reduce_mean(input_tensor=tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "\n",
    "# 최적화기 지정\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss=cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-15 00:11:37.588531: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start learning!\n",
      "Epoch: 1, Avg. Cost = 0.7062\n",
      "Epoch: 2, Avg. Cost = 0.5035\n",
      "Epoch: 3, Avg. Cost = 0.4835\n",
      "Epoch: 4, Avg. Cost = 0.4743\n",
      "Epoch: 5, Avg. Cost = 0.4661\n",
      "Epoch: 6, Avg. Cost = 0.4628\n",
      "Epoch: 7, Avg. Cost = 0.4566\n",
      "Epoch: 8, Avg. Cost = 0.4534\n",
      "Epoch: 9, Avg. Cost = 0.4517\n",
      "Epoch: 10, Avg. Cost = 0.4492\n",
      "Epoch: 11, Avg. Cost = 0.4478\n",
      "Epoch: 12, Avg. Cost = 0.4473\n",
      "Epoch: 13, Avg. Cost = 0.4451\n",
      "Epoch: 14, Avg. Cost = 0.4445\n",
      "Epoch: 15, Avg. Cost = 0.4437\n",
      "Epoch: 16, Avg. Cost = 0.4421\n",
      "Epoch: 17, Avg. Cost = 0.4416\n",
      "Epoch: 18, Avg. Cost = 0.4424\n",
      "Epoch: 19, Avg. Cost = 0.4427\n",
      "Epoch: 20, Avg. Cost = 0.4406\n",
      "Learning finished!\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "sess = tf.Session()\n",
    "sess.run(fetches=tf.global_variables_initializer())\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "print(\"Start learning!\")\n",
    "for epoch in range(training_epochs):\n",
    "    total_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        batch_xs = batch_xs.reshape(-1, 28, 28, 1)\n",
    "        _, cost_val = sess.run([optimizer, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "        total_cost += cost_val\n",
    "    \n",
    "    print(\"Epoch: {}, Avg. Cost = {:.4f}\".format(epoch + 1, total_cost/total_batch))\n",
    "\n",
    "print(\"Learning finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9019\n"
     ]
    }
   ],
   "source": [
    "# Test the results\n",
    "is_correct = tf.equal(x=tf.argmax(input=logits, axis=1), y=tf.argmax(Y, 1))\n",
    "acc = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "accuracy = sess.run(acc, {X: mnist.test.images.reshape(-1, 28, 28, 1), Y: mnist.test.labels})\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_cnn.pb file is created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Freeze variables and save pb file\n",
    "output_graph_def = graph_util.convert_variables_to_constants(\n",
    "    sess=sess, input_graph_def=sess.graph_def, output_node_names=[\"probability\"])\n",
    "with gfile.FastGFile(\"./models/mnist_cnn.pb\", \"wb\") as f:\n",
    "    f.write(output_graph_def.SerializeToString())\n",
    "\n",
    "print(\"mnist_cnn.pb file is created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"#CC3D3D\">Case #3: OpenCV DNN Module</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Step  #1: About Module\n",
    "<hr>\n",
    "\n",
    "- OpenCV DNN(Deep Neural Network) 모듈\n",
    "    - 미리 학습된 딥러닝 모델을 이용하여 `실행(forward pass, inference)` 하는 기능\n",
    "    - 학습은 지원하지 않음\n",
    "    - OpenCV 3.3 버전부터 기본 기능으로 제공한다.\n",
    "    - OpenCV 4.3 버전부터 GPU(CUDA) 지원(소스 코드 직접 빌드 필요)\n",
    "    - 참고: [https://github.com/opencv/opencv/wiki/Deep-Learning-in-OpenCV](https://github.com/opencv/opencv/wiki/Deep-Learning-in-OpenCV)\n",
    "\n",
    "<hr>\n",
    "\n",
    "- 지원하는 딥러닝 프레임워크(framework)\n",
    "    - Caffe\n",
    "    - Keras\n",
    "    - Tensorflow\n",
    "        - Keras 기반\n",
    "    - PyTorch\n",
    "    - Darknet\n",
    "    - ONNX(OpeN Network eXchange)\n",
    "    - <img src=\"images/markdown/.png\" width=\"600\">\n",
    "\n",
    "- 검증된 딥러닝 네트워크\n",
    "    - <img src=\"images/markdown/.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Step  #2: OpenCV function\n",
    "<hr>\n",
    "\n",
    "> `네트워크 불러오기`\n",
    "\n",
    "$$\\mathsf{{\\color{RoyalBlue}cv2.dnn.}{\\color{Tan}readNet}(model, config, framework)\\rightarrow retval}$$\n",
    "- model: 훈련된 가중치를 저장하고 있는 이진 파일 이름(겅로 포함)\n",
    "- config: 네트워크 구성을 저장하고 있는 텍스트 파일 이름(경로 포함)\n",
    "- framework: 명시적인 딥러닝 프레임워크 이름\n",
    "- retval: `cv2.dnn_Net` 클래스 객체\n",
    "- `참고사항:`\n",
    "    - model:\n",
    "        - <img src=\"images/markdown/.png\" width=\"600\">\n",
    "\n",
    "<hr>\n",
    "\n",
    "> `네트워크 입력 블롭(blob) 만들기`\n",
    "\n",
    "$$\\mathsf{{\\color{RoyalBlue}cv2.dnn.}{\\color{Tan}blobFromImage}(image, scalefactor, size, mean, swapRB, crop, ddepth)\\rightarrow retval}$$\n",
    "- image: 단일 또는 3채널의 입력 영상\n",
    "- scalefactor: 입력 영상 픽셀 값에 곱할 값\n",
    "- size: 출력 영상의 크기\n",
    "- mean: 입력 영상 각 채널에서 뺄 평균 값\n",
    "- swapRB: R과 B 채널을 서로 바꿀 것인지를 결정하는 플래그\n",
    "- crop: 크롭(crop) 수행 여부\n",
    "- ddepth: 출력 블록의 깊이\n",
    "- retval: 영상으로부터 구한 블롭 객체\n",
    "- `참고사항:`\n",
    "    - scalefactor: 0~1 사이로 정규화된 픽셀 값을 사용할 때는 `1 / 255.` 를 지정한다.\n",
    "        - `기본값은 1`\n",
    "    - size: (H, W) 튜플 형태\n",
    "        - `기본값은 (0, 0)`\n",
    "    - mean: `기본값은 (0, 0, 0, 0)`\n",
    "    - swapRB: `기본값은 False`\n",
    "    - crop: `기본값은 False`\n",
    "    - ddepth: `CV_32F` or `CV_8U`\n",
    "        - `기본값은 CV_32F`\n",
    "    - retval:\n",
    "        - type=`numpy.ndarray`\n",
    "        - shape=`(N, C, H, W)`\n",
    "        - dtype=`numpy.float32`(ddepth 파라미터가 기본값일 떄)\n",
    "\n",
    "<hr>\n",
    "\n",
    "> `네트워크 입력 설정하기`\n",
    "\n",
    "$$\\mathsf{{\\color{RoyalBlue}cv2.dnn\\_Net.}{\\color{Tan}setInput}(blob, name, scalefactor, mean)\\rightarrow None}$$\n",
    "- blob: cv2.dnn.blobFromImage() 함수로 정의한 블롭 객체\n",
    "- name: 입력 레이어 이름\n",
    "- scalefactor: 추가적으로 픽셀 값에 곱할 값\n",
    "- mean: 추가적으로 픽셀 값에서 뺄 평균 값\n",
    "\n",
    "<hr>\n",
    "\n",
    "> `네트워크 순방향 실행(추론)`\n",
    "\n",
    "$$\\mathsf{{\\color{RoyalBlue}cv2.dnn\\_Net.}{\\color{Tan}forward}(outputName)\\rightarrow retval}$$\n",
    "$$\\mathsf{{\\color{RoyalBlue}cv2.dnn\\_Net.}{\\color{Tan}forward}(outputNames, outputBlobs)\\rightarrow outputBlobs}$$\n",
    "- outputName: 출력 레이어 이름\n",
    "- retval: 지정한 레이어의 출력 블롭\n",
    "- outputNames: 출력 레이어 이름 리스트\n",
    "- outputBlobs: 지정한 레이어의 출력 블롭 리스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Step #3: Implementation example\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN 모듈을 이용하여 숫자 인식 예제\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "old_x, old_y = -1, -1\n",
    "\n",
    "def on_mouse(event, x, y, flags, _):\n",
    "    global old_x, old_y\n",
    "    \n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        old_x, old_y = x, y\n",
    "    elif event == cv2.EVENT_LBUTTONDOWN:\n",
    "        old_x, old_y = -1, -1\n",
    "    elif event == cv2.EVENT_MOUSEMOVE:\n",
    "        if flags & cv2.EVENT_FLAG_LBUTTON:\n",
    "            cv2.line(image, (old_x, old_y), (x, y), (255, 255, 255), 40, cv2.LINE_AA)\n",
    "            old_x, old_y = x, y\n",
    "            cv2.imshow(\"image\", image)\n",
    "\n",
    "\n",
    "def norm_digit(image):\n",
    "    moments = cv2.moments(image)\n",
    "    cx = moments['m10'] / moments['m00']\n",
    "    cy = moments['m01'] / moments['m00']\n",
    "    h, w = image.shape[:2]\n",
    "    affine_matrix = np.array([[1, 0, w/2 - cx], [0, 1, h/2 - cy]], dtype=np.float32)\n",
    "    destination = cv2.warpAffine(image, affine_matrix, (0, 0))\n",
    "    return destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case #2에서 만든 \"mnist_cnn.pb\" 모델 불러오기\n",
    "net = cv2.dnn.readNet(\"models/mnist_cnn.pb\")\n",
    "\n",
    "if net.empty():\n",
    "    print(\"Network load failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 (99.53)\n",
      "0 (10.00)\n",
      "0 (10.00)\n",
      "0 (10.00)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = np.zeros((400, 400), np.uint8)\n",
    "\n",
    "cv2.imshow(\"image\", image)\n",
    "cv2.setMouseCallback(\"image\", on_mouse)\n",
    "\n",
    "while True:\n",
    "    key = cv2.waitKey()\n",
    "    \n",
    "    if key == 27:\n",
    "        break\n",
    "    elif key == ord(\" \"):\n",
    "        # MNIST 모델은 입력 데이터를 0~1 사이로 정규화하여 입력을 받기 때문에 여기서도 입력 픽셀 값을 0~1 사이로 정규화 한다.\n",
    "        blob = cv2.dnn.blobFromImage(norm_digit(image), 1/255., (28, 28))\n",
    "        net.setInput(blob)\n",
    "        probability = net.forward()\n",
    "        \n",
    "        # return: min value, max value, min location, max location\n",
    "        _, max_value, _, max_loc = cv2.minMaxLoc(probability)\n",
    "        digit = max_loc[0]\n",
    "        \n",
    "        print(f\"{digit} ({max_value * 100:4.2f})\")\n",
    "        \n",
    "        image.fill(0)\n",
    "        cv2.imshow(\"image\", image)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"#CC3D3D\">Case #4: pre-trained model</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Step #1: GoogLeNet\n",
    "<hr>\n",
    "\n",
    "- GoogLeNet 영상 인식\n",
    "    - 2014년 ILSVRC(ImageNet Large Scale Visual Recognition Challenge) 영상 인식 분야에서 1위를 차지한 알고리즘이다.\n",
    "        - 1000개의 카테고리, 120만개의 훈련 영상, 15만개의 테스트 영상\n",
    "    - 입력: `224x224`, `BGR 컬러 영상`, `평균 값 = (104, 117, 123)`\n",
    "    - 출력: `1x1000 행렬`, 1000개 클래스에 대한 확률 값\n",
    "    - <img src=\"images/markdown/.png\" width=\"600\">\n",
    "\n",
    "<hr>\n",
    "\n",
    "- 미리 학습된 GoogLeNet 학습 모델 및 구성 파일 다운로드\n",
    "    - Caffe Model Zoo: [https://github.com/BVLC/caffe](https://github.com/BVLC/caffe)\n",
    "        - 모델 파일: [http://dl.caffe.berkeleyvision.org/bvlc_googlenet.caffemodel](http://dl.caffe.berkeleyvision.org/bvlc_googlenet.caffemodel)\n",
    "        - 설정 파일: [https://github.com/BVLC/caffe/blob/master/models/bvlc_googlenet/deploy.prototxt](https://github.com/BVLC/caffe/blob/master/models/bvlc_googlenet/deploy.prototxt)\n",
    "    - ONNX Model Zoo: [https://github.com/onnx/models](https://github.com/onnx/models)\n",
    "        - 모델 파일: [https://github.com/onnx/models/tree/main/vision/classification/inception_and_googlenet/googlenet](https://github.com/onnx/models/tree/main/vision/classification/inception_and_googlenet/googlenet)\n",
    "    - 클래스 이름 파일\n",
    "        - 1 ~ 1000번 클래스에 대한 설명을 저장한 텍스트 파일\n",
    "            - [https://github.com/opencv/opencv/blob/4.1.0/samples/data/dnn/classification_classes_ILSVRC2012.txt](https://github.com/opencv/opencv/blob/4.1.0/samples/data/dnn/classification_classes_ILSVRC2012.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Step #2: Implementation example\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구글넷 영상 인식 예제\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# image = cv2.imread(\"images/space_shuttle.jpg\")\n",
    "# image = cv2.imread(\"images/apple2.png\")\n",
    "image = cv2.imread(\"images/beagle.jpg\")\n",
    "# image = cv2.imread(\"images/building.jpg\")\n",
    "\n",
    "if image is None:\n",
    "    print(\"Image load failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네트워크 불러오기\n",
    "\n",
    "# Caffe\n",
    "# model = \"pre-trained/googlenet/bvlc_googlenet.caffemodel\"\n",
    "# config = \"pre-trained/googlenet/deploy.prototxt\"\n",
    "\n",
    "# ONNX\n",
    "model = \"pre-trained/googlenet/googlenet-9.onnx\"\n",
    "config = \"\"\n",
    "\n",
    "net = cv2.dnn.readNet(model, config)\n",
    "\n",
    "if net.empty():\n",
    "    print(\"Network load failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 이름 불러오기(1000개의 카테고리)\n",
    "classNames = None\n",
    "with open(\"pre-trained/googlenet/classification_classes_ILSVRC2012.txt\", \"rt\") as f:\n",
    "    classNames = f.read().rstrip(\"\\n\").split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference(추론)\n",
    "\n",
    "# blob: shape=(1, 3, 224, 224), dtype=float32\n",
    "blob = cv2.dnn.blobFromImage(image, scalefactor=1, size=(224, 224), mean=(104, 117, 123))\n",
    "net.setInput(blob)\n",
    "# probability: shape=(1, 1000), dtype=float32\n",
    "probability = net.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference: beagle (78.35)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 추론 결과 확인 & 화면 출력\n",
    "out = probability.flatten()\n",
    "classId = np.argmax(out)  # 확률의 최댓값 인덱스\n",
    "confidence = out[classId]  # 확률의 최댓값\n",
    "\n",
    "text = f\"{classNames[classId]} ({confidence * 100:4.2f})\"\n",
    "print(\"inference:\",text)\n",
    "cv2.putText(image, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "cv2.imshow(\"image\", image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"#CC3D3D\">Case #5: Deep Learning application - Object Detection</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Step #1: OpenCV DNN - Face Detection\n",
    "<hr>\n",
    "\n",
    "- OpenCV DNN 얼굴 검출 예제\n",
    "    - OpenCV 예제에서 DNN 모듈을 사용한 얼굴 검출 기능을 지원한다.\n",
    "        - SSD(Single Shot MultiBox Detector) 기반 얼굴 검출 네트워크\n",
    "            - > [https://github.com/opencv/opencv/tree/master/samples/dnn/face_detector](https://github.com/opencv/opencv/tree/master/samples/dnn/face_detector)\n",
    "    - 기존의 Haar-Cascade 방법보다 속도 & 정확도 면에서 더 좋은 성능을 나타낸다.\n",
    "        - <img src=\"images/markdown/.png\" width=\"600\">\n",
    "        - > table from: [https://dl.opencv.org/present/cvpr_opencv.pdf](https://dl.opencv.org/present/cvpr_opencv.pdf)\n",
    "\n",
    "<hr>\n",
    "\n",
    "- SSD(Single Shot MultiBox Detector)(W. Liu, et. al, 2016)\n",
    "    - 동시대 다른 객체 검출 알고리즘과 비교하여 성능과 속도 두 가지를 모두 만족시킨 알고리즘이다.\n",
    "    - <img src=\"images/markdown/.png\" width=\"600\">\n",
    "    - > [https://arxiv.org/pdf/1512.02325.pdf](https://arxiv.org/pdf/1512.02325.pdf)\n",
    "\n",
    "<hr>\n",
    "\n",
    "- OpenCV Face 검출 모델 & 설정 파일 다운로드\n",
    "    - 모델 파일\n",
    "        - OpenCV 제공 스크립트 사용 방법\n",
    "            - `pre-trained/opencv_face_detector` 폴더에서 터미널을 열고 `download_weights.py` 파일 실행\n",
    "        - 모델 파일 직접 다운로드\n",
    "            - Caffe(FP16): [https://raw.githubusercontent.com/opencv/opencv_3rdparty/dnn_samples_face_detector_20180205_fp16/res10_300x300_ssd_iter_140000_fp16.caffemodel](https://raw.githubusercontent.com/opencv/opencv_3rdparty/dnn_samples_face_detector_20180205_fp16/res10_300x300_ssd_iter_140000_fp16.caffemodel)\n",
    "            - TensowFlow(uint8): [https://raw.githubusercontent.com/opencv/opencv_3rdparty/dnn_samples_face_detector_20180220_uint8/opencv_face_detector_uint8.pb](https://raw.githubusercontent.com/opencv/opencv_3rdparty/dnn_samples_face_detector_20180220_uint8/opencv_face_detector_uint8.pb)\n",
    "    - 구성 파일 다운로드\n",
    "        - [https://github.com/opencv/opencv/tree/master/samples/dnn/face_detector](https://github.com/opencv/opencv/tree/master/samples/dnn/face_detector) 에서<br>\n",
    "        `deploy.prototxt, opencv_face_detector.pbtxt` 파일 다운로드\n",
    "    - 다운로드 받은 파일을 `pre-trained/opencv_face_detector` 폴더로 이동\n",
    "\n",
    "<hr>\n",
    "\n",
    "- OpenCV DNN 얼굴 검출(SSD) 입력\n",
    "    - Size: (300, 300)\n",
    "    - Scale: 1 (0 ~ 255)\n",
    "    - Mean: (104, 177, 123)\n",
    "    - RGB: False\n",
    "- OpenCV DNN 얼굴 검출(SSD) 출력\n",
    "    - out.shape = (1, 1, 200, 7)\n",
    "    - `detect = out[0, 0, :, :]`\n",
    "        - -> <img src=\"images/markdown/.png\" width=\"600\">\n",
    "        - c: confidence(face일 확률), x1~y2: 0~1 사이로 `정규화`된 바운딩 박스의 `좌상단, 우하단 점의 좌표`\n",
    "            - x1,y1,x2,y2: 입력 영상에서의 실제 좌표를 구하려면 정규화된 값에 해당 영상의 width와 height를 곱해줘야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Step #1-1: Implementation example\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OpenCV DNN 얼굴 검출 예제\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "model = \"pre-trained/opencv_face_detector/res10_300x300_ssd_iter_140000_fp16.caffemodel\"\n",
    "config = \"pre-trained/opencv_face_detector/deploy.prototxt\"\n",
    "# model = \"pre-trained/opencv_face_detector/opencv_face_detector_uint8.pb\"\n",
    "# config = \"pre-trained/opencv_face_detector/opencv_face_detector.pbtxt\"\n",
    "\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "if not capture.isOpened():\n",
    "    print(\"Camera open failed!\")\n",
    "\n",
    "network = cv2.dnn.readNet(model, config)\n",
    "\n",
    "if network.empty():\n",
    "    print(\"Network open failed!\")\n",
    "\n",
    "while True:\n",
    "    retval, frame = capture.read()\n",
    "    \n",
    "    if not retval:\n",
    "        break\n",
    "    \n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    blob = cv2.dnn.blobFromImage(frame, 1, (300, 300), (104, 177, 123))\n",
    "    network.setInput(blob)\n",
    "    out = network.forward()\n",
    "    \n",
    "    # detect.shape=(1, 1, N, 7)이고, 이중 뒤쪽 두 개의 차원에 검출 정보가 저장되므로 편의상 2차원 행렬로 변환하여 사용한다.\n",
    "    detect = out[0, 0, :, :]\n",
    "    HEIGHT, WIDTH = frame.shape[:2]\n",
    "    \n",
    "    for i in range(detect.shape[0]):\n",
    "        confidence = detect[i, 2]\n",
    "        \n",
    "        if confidence < 0.5:\n",
    "            break\n",
    "        \n",
    "        x1 = int(detect[i, 3] * WIDTH)\n",
    "        y1 = int(detect[i, 4] * HEIGHT)\n",
    "        x2 = int(detect[i, 5] * WIDTH)\n",
    "        y2 = int(detect[i, 6] * HEIGHT)\n",
    "        \n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0))\n",
    "        \n",
    "        label = f\"Face: {confidence:.2f}\"\n",
    "        cv2.putText(frame, label, (x1, y1 - 1), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.imshow(\"frame\", frame)\n",
    "    \n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Step #2: YOLOv3 Object Detection\n",
    "<hr>\n",
    "\n",
    "- YOLO\n",
    "    - You Only Look Once\n",
    "    - 실시간 객체 검출 딥러닝 기반 알고리즘\n",
    "        - <img src=\"images/markdown/.png\" width=\"600\">\n",
    "        - > image from: [https://pjreddie.com/darknet/yolo/](https://pjreddie.com/darknet/yolo/)\n",
    "\n",
    "<hr>\n",
    "\n",
    "- YOLOv3\n",
    "    - 2018년 4월 발표된 Tech Report\n",
    "        - [https://arxiv.org/abs/1804.02767](https://arxiv.org/abs/1804.02767)\n",
    "    - 기존 객체 검출 방법과 성능은 비슷하고 속도는 훨씬 빨라졌다.\n",
    "    - [COCO 데이터셋](https://cocodataset.org/#home) 사용\n",
    "        - 80개 클래스 객체 검출\n",
    "    - <img src=\"images/markdown/.png\" width=\"600\">\n",
    "\n",
    "<hr>\n",
    "\n",
    "- YOLOYv3 네트워크 구조\n",
    "    - <img src=\"images/markdown/.png\" width=\"600\">\n",
    "    - > image from: [https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b](https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Step #2-1: Implementation example\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$$\\mathsf{{\\color{RoyalBlue} }{\\color{Tan} }()\\rightarrow }$$\n",
    "- <img src=\"images/markdown/.png\" width=\"600\">"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "38cc0cca68228d4d18227abe74ed813685db17db46984279e4aabdddebf5f1ba"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
