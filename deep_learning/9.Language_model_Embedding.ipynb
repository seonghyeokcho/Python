{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 언어 모델기반 임베딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 언어 모델은 단어 시퀀스에 대한 확률 분포\n",
    "- 주어진 특정 단어 시퀀스에 대해 다음 단어를 예측해볼 수 있다.(언어 모델이 양방향인 경우 이전 단어도)\n",
    "- 대규모 자연 문법 구조를 활용하기 때문에 능동 레이블을 포함하지 않으므로 어떤의미에서는 비지도학습\n",
    "- 단어 임베딩으로서의 언어 모델과 좀 더 전통적인 임베딩의 주된 차이점은 전통적인 임베딩은 단일 초기 변환으로 데이터에 적용된 다음 특정 작업에 특화돼 미세조정된다. 반면 언어 모델은 대규모 외부 말뭉치로 훈련받으며 특정언어 모델을 나타냄. 이를 사전훈련이라고 한다.\n",
    "- 하지만 이러한 언어 모델을 사전 훈련하는 연산비용은 대개 상당히 높다. 그리하여 공개된 사전훈련모델을 미세조정 하면된다.\n",
    "1. 자신의 분야에 맞는 텍스트를 사용해 마지막 몇 개의 계층을 재교육\n",
    "2. 언어 모델의 마지막 계층을 제거하고 이를 하나 또는 두개의 완전 연결망으로 대체. 이 연결망은 과제에 따라 입력의 언어모델 임베딩을 최종 범주형 또는 회귀로 변환하는 일을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'bert'...\n",
      "remote: Enumerating objects: 340, done.\u001b[K\n",
      "remote: Total 340 (delta 0), reused 0 (delta 0), pack-reused 340\u001b[K\n",
      "\u001b[KReceiving objects: 100% (340/340), 317.20 KiB | 525.00 KiB/s, done.\n",
      "\u001b[KResolving deltas: 100% (185/185), done.\n"
     ]
    }
   ],
   "source": [
    "#!git clone https://github.com/google-research/bert.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT를 자신의 신경망 일부로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-2.11.0-py3-none-any.whl (674 kB)\n",
      "\u001b[K     |████████████████████████████████| 674 kB 665 kB/s \n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2020.6.8.tar.gz (690 kB)\n",
      "\u001b[K     |████████████████████████████████| 690 kB 2.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.7/site-packages (from transformers) (4.42.1)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.7/site-packages (from transformers) (20.1)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "\u001b[K     |████████████████████████████████| 883 kB 2.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/anaconda3/lib/python3.7/site-packages (from transformers) (1.18.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Collecting tokenizers==0.7.0\n",
      "  Downloading tokenizers-0.7.0-cp37-cp37m-macosx_10_10_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 3.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/anaconda3/lib/python3.7/site-packages (from transformers) (2.22.0)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.91-cp37-cp37m-macosx_10_6_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 3.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from packaging->transformers) (2.4.6)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.7/site-packages (from packaging->transformers) (1.14.0)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/anaconda3/lib/python3.7/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.7/site-packages (from requests->transformers) (1.25.8)\n",
      "Building wheels for collected packages: regex, sacremoses\n",
      "  Building wheel for regex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for regex: filename=regex-2020.6.8-cp37-cp37m-macosx_10_9_x86_64.whl size=283189 sha256=9897d850c7ac0f1b4cc05519a221db6c65c70fa87de822ad7dcfa8e118b290ac\n",
      "  Stored in directory: /Users/HumanRevolution/Library/Caches/pip/wheels/46/f1/0b/a372e98f7103934a3573301c71b475143baf8ba6f6dffc876c\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=11b0fd52d825b8def4ec37c0df388f0657130f3d0b2cec2aae64173ce4362ecc\n",
      "  Stored in directory: /Users/HumanRevolution/Library/Caches/pip/wheels/69/09/d1/bf058f7d6fa0ecba2ce7c66be3b8d012beb4bf61a6e0c101c0\n",
      "Successfully built regex sacremoses\n",
      "Installing collected packages: regex, sacremoses, tokenizers, sentencepiece, transformers\n",
      "Successfully installed regex-2020.6.8 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org 에서 Quick Start Locally라는 제목 부분을 찾아 코드를 cmd에 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, BertForSequenceClassification, glue_convert_examples_to_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 후밤ㄴ부에 사용하게 될 몇가지 상수를 선언\n",
    "BATCH_SIZE = 32\n",
    "FINE_TUNED_MODEL_DIR = './bert/model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from /Users/HumanRevolution/tensorflow_datasets/glue/mrpc/1.0.0\n",
      "INFO:absl:Reusing dataset glue (/Users/HumanRevolution/tensorflow_datasets/glue/mrpc/1.0.0)\n",
      "INFO:absl:Constructing tf.data.Dataset for split None, from /Users/HumanRevolution/tensorflow_datasets/glue/mrpc/1.0.0\n"
     ]
    }
   ],
   "source": [
    "# 텐서플로 Datasets를 통해 데이터셋 로드\n",
    "data, info = tfds.load('glue/mrpc', with_info=True)\n",
    "num_train = info.splits['train'].num_examples\n",
    "num_valid = info.splits['validation'].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLUE용 데이터셋을 tf.data.Dataset 인스턴스로 준비\n",
    "Xtrain = glue_convert_examples_to_features(data['train'], tokenizer, 128, 'mrpc')\n",
    "Xtrain = Xtrain.shuffle(128).batch(BATCH_SIZE).repeat(-1)\n",
    "Xvalid = glue_convert_examples_to_features(data['validation'], tokenizer, 128, 'mrpc')\n",
    "Xvalid = Xvalid.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RepeatDataset shapes: ({input_ids: (None, None), attention_mask: (None, None), token_type_ids: (None, None)}, (None,)), types: ({input_ids: tf.int32, attention_mask: tf.int32, token_type_ids: tf.int32}, tf.int64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ({input_ids: (None, None), attention_mask: (None, None), token_type_ids: (None, None)}, (None,)), types: ({input_ids: tf.int32, attention_mask: tf.int32, token_type_ids: tf.int32}, tf.int64)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xvalid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다음으로 손실함수, 최적화기와 행렬을 정의하고 몇 훈련 에폭동안 모델을 적합화함.\n",
    "- 모델을 미세 조정하고 있으므로 에폭의 수는 2개에 불과하며 학습률도 매우 작다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "114/114 [==============================] - 1465s 13s/step - loss: 0.5846 - accuracy: 0.6796 - val_loss: 0.4892 - val_accuracy: 0.7865\n",
      "Epoch 2/2\n",
      "114/114 [==============================] - 1315s 12s/step - loss: 0.3855 - accuracy: 0.8193 - val_loss: 0.3712 - val_accuracy: 0.8438\n"
     ]
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=opt, loss=loss, metrics=[metric])\n",
    "train_steps = num_train // BATCH_SIZE\n",
    "valid_steps = num_valid // BATCH_SIZE\n",
    "history = model.fit(Xtrain, epochs=2, steps_per_epoch=train_steps,\n",
    "    validation_data=Xvalid, validation_steps=valid_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미세조정된 모델을 'bert/data'에 저장\n",
    "model.save_pretrained(FINE_TUNED_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 한 쌍의 문장이 서로의 문단임을 예측하고자 모델을 파이토치 모델로 다시 로드한다.\n",
    "- from_tf = True 매개변수는 저장된 모델이 텐서플로 체크포인트임을 나타낸다.\n",
    "- 다음은 문장의 쌍이 각각의 문단인 (sentence_0, sentence_1)과 그렇지 않은 쌍인 (sentence_0, sentence_2)를 사용해 저장된 모델을 테스트한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모세조정되어 저장된 모델 로드\n",
    "saved_model = BertForSequenceClassification.from_pretrained(FINE_TUNED_MODEL_DIR, from_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence_1 is a paraphrase of sentence_0\n",
      "Sentence_2 is not a paraphrase of sentence_0\n",
      "Sentence_3 is not a paraphrase of sentence_0\n"
     ]
    }
   ],
   "source": [
    "def print_result(id1, id2, pred):\n",
    "    if pred == 1:\n",
    "        print(\"Sentence_{:d} is a paraphrase of sentence_{:d}\".format(id2, id1))\n",
    "    else:\n",
    "        print(\"Sentence_{:d} is not a paraphrase of sentence_{:d}\".format(id2, id1))\n",
    "\n",
    "sentence_0 = \"At least 12 people were killed in the battle last week.\"\n",
    "sentence_1 = \"At least 12 people lost their lives in last weeks fighting.\"\n",
    "sentence_2 = \"The fires burnt down the houses on the street.\"\n",
    "sentence_3 = \"A bomb is planted in my ear\"\n",
    "\n",
    "inputs_1 = tokenizer.encode_plus(sentence_0, sentence_1, add_special_tokens=False, return_tensors=\"pt\")\n",
    "inputs_2 = tokenizer.encode_plus(sentence_0, sentence_2, add_special_tokens=False, return_tensors=\"pt\")\n",
    "inputs_3 = tokenizer.encode_plus(sentence_0, sentence_3, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "pred_1 = saved_model(**inputs_1)[0].argmax().item()\n",
    "pred_2 = saved_model(**inputs_2)[0].argmax().item()\n",
    "pred_3 = saved_model(**inputs_3)[0].argmax().item()\n",
    "\n",
    "print_result(0, 1, pred_1)\n",
    "print_result(0, 2, pred_2)\n",
    "print_result(0, 3, pred_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
