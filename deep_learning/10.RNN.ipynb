{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN의 변형 LSTM 의 또다른 변형 핍홀 LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 핍홀 LSTM의 실험적 구성을 작성해본다.\n",
    "- 이를 자신의 RNN계층에서 사용하려면 다음의 코드처럼 RNN래퍼에서 셀을 래핑해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import experimental, layers\n",
    "\n",
    "hidden_dim = 256\n",
    "peephole_cell = tf.keras.experimental.PeepholeLSTMCell(hidden_dim)\n",
    "rnn_layer = tf.keras.layers.RNN(peephole_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 양방향 RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 양방향 LSTM은 본질적으로 두 개의 RNN을 서로의 위에 쌓은 것인데, 하나는 왼쪽에서 오른쪽으로 입력을 읽고 다른 하나는 오른쪽에서 왼쪽으로 입력을 읽는다.\n",
    "- 양방향 RNN을 사용하면 신경망이 시퀀스의 시작과 끝을 동일하게 강조할 수 있으므로 일반적으로 성능이 향상됨.\n",
    "- RNN계층을 양방향으로 만들려면 다음과 같은 래퍼 계층으로 계층을 래핑하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-175bb535c243>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m self.lstm = tf.keras.layers.Bidirectional(\n\u001b[1;32m      5\u001b[0m     tf.keras.layers.LSTM(10, return_sequences=True,\n\u001b[0;32m----> 6\u001b[0;31m     input_shape=(5, 10)))\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "self.lstm = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(10, return_sequences=True,\n",
    "    input_shape=(5, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 일대다 텍스트 생성 학습 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단일 입력에 대해 시퀀스를 출력\n",
    "- 이러한 신경망의 예는 이미지애서 택스트 태그를 생성하는것으로, 이미지의 여러 측면에 관한 간단한 텍스트 설명을 포함한다.(ex. cat -> 1)\n",
    "- 이러한 신경망은 이미지 입력과 이미지 태그를 나타내는 레이블된 텍스트 시퀀스로 훈련된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "DATA_DIR = './data'\n",
    "CHECKPOINT_DIR = \"./checkpoint/RNN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://www.gutenberg.org/cache/epub/28885/pg28885.txt\n",
      "180224/177428 [==============================] - 1s 7us/step\n",
      "Downloading data from https://www.gutenberg.org/files/12/12-0.txt\n",
      "196608/193607 [==============================] - 1s 3us/step\n"
     ]
    }
   ],
   "source": [
    "def download_and_read(urls):\n",
    "    texts = []\n",
    "    for i, url in enumerate(urls):\n",
    "        p = tf.keras.utils.get_file('ex1-{:d}.txt'.format(i), url, cache_dir='.')\n",
    "        text = open(p, 'r').read()\n",
    "        # 바이트 순서 표시 문자 제거\n",
    "        text = text.replace('\\ufeff', '')\n",
    "        # 줄 바꿈 표시 문자 제거\n",
    "        text = text.replace('\\n', '')\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # 리스트에 추가\n",
    "        texts.extend(text)\n",
    "    return texts\n",
    "\n",
    "texts = download_and_read([\n",
    "    \"http://www.gutenberg.org/cache/epub/28885/pg28885.txt\",\n",
    "    \"https://www.gutenberg.org/files/12/12-0.txt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다음은 어휘를 만든다. 이 경우 어휘에는 대문자와 소문자 알파벳, 숫자 및 특수문자로 구성된 90개의 고유한 문자가 포함된다. 또한 각 어휘 문자를 고유한 정수 또는 그 반대로 변환하고자 일부 매핑 사전을 작성함\n",
    "- 이 신경망의 입출력은 문자 시퀀스이다. 그러나 신경망의 실제 입출력은 정수 시퀀스이기 때문에 매핑 사전을 사용해 이런 변환을 처리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 90\n"
     ]
    }
   ],
   "source": [
    "# 어휘 생성\n",
    "vocab = sorted(set(texts))\n",
    "print('vocab size: {:d}'.format(len(vocab)))\n",
    "\n",
    "# 어휘 문자에서 정수로 매핑 생성\n",
    "char2idx = {c:i for i, c in enumerate(vocab)}\n",
    "idx2char = {i:c for c, i in char2idx.items()}\n",
    "\n",
    "# 텍스트를 수치화\n",
    "texts_as_ints = np.array([char2idx[c] for c in texts])\n",
    "data = tf.data.Dataset.from_tensor_slices(texts_as_ints)\n",
    "\n",
    "# 예측하기 전에 보여줄 문자 개수\n",
    "# sequences: [None, 100]\n",
    "seq_length = 100\n",
    "sequences = data.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "def split_train_labels(sequence):\n",
    "    input_seq = sequence[0:-1]\n",
    "    output_seq = sequence[1:]\n",
    "    return input_seq, output_seq\n",
    "\n",
    "sequences = sequences.map(split_train_labels)\n",
    "# 훈련을 위한 설정\n",
    "# 배치: [None, 64, 100]\n",
    "batch_size = 64\n",
    "steps_per_epoch = len(texts) // seq_length // batch_size\n",
    "dataset = sequences.shuffle(10000).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이제 신경망을 정의한다.\n",
    "- 크기가 100인 정수(num_timestep)의 시퀀스를 입력으로 취해 임베딩 계층을 통해 전달해서 시퀀스의 각 정수가 256크기의 벡터(embedding_dim)로 변환되게 한다. 따라서 배치 크기가 64라고 가정하면 입력 시퀀스 크기(64,100)의 임베딩 계층 출력은 (64,100,256)형태의 행렬이다.\n",
    "- 다음 계층은 100개의 타임 스텝을 가진 RNN 계층이다. 구현방식은 GRU, 각각의 타임 스텝에서 크기(256,)인 벡터를 취하고 (1024,)형태의 벡터(rnn_output_dim)를 출력한다. 이 RNN모델은 상태저장이다. 또한 return_sequence = True 플래그는 RNN이 마지막 타임 스텝의 집계 출력이 아니라 각 타임스텝에서 출력됨을 나타냄.\n",
    "- 마지막으로 각 타임스텝은 (1024,)형태의 벡터를 밀집계층으로 방출해 형태가 (90,)인 벡터(vocab_size)를 출력한다. 이 계층의 출력은 모양이(64,100,90)인 텐서가 된다.\n",
    "- 출력 벡터의 각 위치는 어휘의 문자에 해당하고 값은 해당 출력 위치에서 해당 문자가 발생할 확률에 해당한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"char_gen_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  23040     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    multiple                  107400    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  9090      \n",
      "=================================================================\n",
      "Total params: 139,530\n",
      "Trainable params: 139,530\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJwAAAA8CAYAAACTpNOjAAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAnKADAAQAAAABAAAAPAAAAAAf8BBOAAAKh0lEQVR4Ae1cBagVTRQ+z+7uLsTuwhYUW+zGVhTFVkRUVGywEANsERUVAxUDxGc3dncgdnfNf77zM8uN59297+5779/fOXDv7s6eOTPz7dlTe/dGKSYyZBCIJwQSxdM4ZhiDgCBgFM4oQrwiYBQuXuE2gxmFMzoQrwgkCRztxIkTNGfOnMBmc2wQCBuBMmXK0Lhx4/z6BVm4R48e0aZNm/yYzIFBIFwEzpw5Q4cOHQrqFmThNMfGjRv1rtkaBMJGoF+/fnT//v2gfkEWLojDNBgEXETAKJyLYBpR9ggYhbPHyHC4iIBROBfBNKLsETAKZ4+R4XARAaNwLoJpRNkjYBTOHiPD4SICRuFcBNOIskfAKJw9RobDRQSMwrkIphFlj4BROHuMDIeLCBiFcxFMI8oeAaNw9hgZDhcRMArnIphGlD0CRuHsMTIcLiIQ5wp38OBBGjFiBBUrVszFaRtR4SLw+PFjWrJkCTVr1oymT5/uqPu5c+do/PjxVL58eZo8ebKjPnZMca5w+BHeqlWr6MaNG3ZzMefjEIHnz5/T0aNHadeuXfTr1y9HI338+JGuXbtG58+fJ7deX45zhevevTuVLVvW0QINU9whUKFCBerWrZsMkDhxYkcD1apVi3r16uWI1ylTnCscJuJ0gU4nbfhih0CiRP9ebr11IiVJkj++heCkexCPa9JOnTpFy5Yto+vXr1OpUqWodevWVL9+fb8BYdZXrlxJ+/bto0KFCtGUKVMoe/bsFs+xY8do69atBFlp0qSh/v37U/PmzeX8gwcPaM2aNZQiRQrC20Bz586lxo0b05AhQ6z+djsnT56kefPmyW/tEVNWqVJFrC/GgkzQixcvaP78+eJG0qZNS7DQjRo1knNv376lnTt30pYtW4Rn27ZttGPHDsqTJw9NmzaNcuTIIXxOvl6/fk0bNmygN2/eiOVZsGCBYNe+fXvq1KmTzHHdunWEOdeoUYPGjBlDUVFRlmi8Xbd27VrpkzdvXpljhw4drPPY+fLlC82YMUNcadKkSSl//vxy3ldOqPX6CXPrAP8t4kv88gz+a8S3yXafYzTFoKvDhw8rXqSqW7euyDhy5Ij0ZcWT45IlSyoGRdWrV0+OmzRpYskGL995CrK+ffumBg4cqNgyKo791P79+xWDKn1YURQrq2IAFbtqq7/dDuTz3ar4Iiu+yKp69eoiD/NmVyPd7969qwoXLqw4uFYc71jrWL58uZyfOnWqYuW05tGmTRtLDgfjdlOwznNcZPXDusqVK6eGDRsmW2DfuXNnhXXyiygqc+bMMt7ChQut/osWLVLJkiVTK1asUBwjq9GjRwtPu3btLJ7379+rihUrKswL68WYwA3yZ82aJXx26wXT3r17pc/EiRMt2U52+vbtqxo0aBDEGqRZ4SocZz9yESZMmGAJZysgyqNB0gqHi6iJLZtKnz69PlScDcnCzp49K22QAXD4LpbjAwcOyHGuXLlEqfl1RoWPUwLwUOCfP39Kl+3bt4s8XFRNrVq1Ur1799aH6vjx48KTM2dOq61r167ShhtDU7Zs2VS6dOn0oaMtFALrK1iwoHr16pX0uX37trRB6dkCStu9e/ekrWfPnhYPW3nFsZXfOLVr1xY+9iDSzu+DyjFuWE24YXwVzsl63Va4iF0qzD6yGaTbmpo2bUrv3r0Tt6jbsPVNHuB22XLRhw8fCK4LL8y2aNGCENzCjSCFB+E9WRArmmwbNmwobhVuLByC60B2dufOHSpatChVq1ZNusO1gTAPuPMMGTKIO0Ub+PniE2Ker1+/yrgpU6bEKb8yT4kSJSg6OprYMlPy5MnlvN0X1gzXljt3bsqUKZOwYyzETHCRGTNmlLYCBQrIuAgpQHDjmIuevzTyF9w+3gOFu+/RowctXrxYZGOtmhBCgDCu0/Xqvm5tI1a4K1euyFw0QHpiiItCkQ5G+e4TNnYRhBipcuXKxC5G4jPES5p0oBvbBATxJBSZLRuNGjVK0n3I1vHZzZs3ZSjUnYYPH66HdbSN7ZxiEh6TwgKrHz9+CDtbQdlqPLSMmjVryi7WgViZrSbpNs2jYzdsI1mvlhebbcRZKhQFhDsvkGDlnNLmzZvl4vfp04eWLl3qZ0GcygjFN3bsWGIXQtgOGDCAhg4dShz7WGl/qlSppPvp06eDxHz+/JmePXsW1J4QDVmzZpVhOSb1Gx6WEoQk7Pv377LP4YlYXTkI+Eqo9UascKVLl5alIIOEa9WEJwyB/yuhz8W0nTlzprgwnWl9+vRJ2LQFjKlPOG3I2HDXw+UMGjRIMjeMqe96uDNYZfzNBSrsmn7//i1Z5MOHD3VTnG7t1lu1alUZP/BvFPDXCiBOhiRrhpvGmn0Vk+NX4cGaEmq9ESscCoOIOS5dukQAAy4JpYwuXbrQpEmTZIE6DtNbNOJRi+9Wu1goKawdygAglEjgBhGDgTizkm24XyNHjqSLFy/ShQsXRNmgWLt37xYlhCxY6sGDB4vS16lThzgrkz/1QfGTA3tx9eB7+vQpNhKjyg5/6fgKCu2UYDGhXL5eAAoCa+qLE248tKN8AkJ8jPAAOODm0bRnzx7KkiWLWG20wXqDOAmiaI4vcX1QhgLBG0Fhnaz3yZMn0gdu2hXiRftRuFkqOnPtTXFAKhkQT0px4C9tOMcuUjJWtIPn1q1bkmGhBKLbeFGK61kKmSvKHUinka1ywKw4SFfIDFEmAD8yzbZt2yoGAOIdE0od6B/44SRBsTUWOeyKJEvVc2PrJ6UJVgI5zxZR5gMZKMlwTCjntUyUg16+fGk7J46flM7cMRYyZY6FrVIJ5CGDvHr1qmCh5XNNUGEuKHN07NhR5oI2rtsplJzArwnZOMeqUj5Bf2TSbAQUJysKGS8rnLJbL6oMKBuhP3DicESLt93+qSwShZ4s0CLc+Sg+BjRb50PtwArBXPsWc0PxB55DYIyPji/gAnCsM8NA/nCOuc4lGSQyOMwTlgQhANfYCHHR6tWrLXFoR3AOt4Ns8r9KsKiXL1+mfPnyiRWOaZ6I5+BNYKWxZoQQGl/NHxfr1X9mgyK/LyXxPYh0Xwe0sZWDajg+muBmtavVbb5bmHk8bbAjlGPwhAOKBreDpwyakF0HgoJYDplyuOR0PnDveJoQKXFRmOD+QxFCBTzVAaVOnTpG1tiuN0ZhNo2uKpzNWK6f5sKxFeuFEo64DYS7rmXLlvKIBzEQyiR4FLd+/fpQ3R2fczqfSpUqOZb5f2P0tMKhZsWPc2yvCQrKqJVx1VzKILCaxYsXl+xz9uzZf7zzbQUHMDidT0C3v+rQ0wrn9EpBEZAx44MqPY51OcSpDMPnDgJ/hcL5QoVfmxhKOAQirsMl3NTNyF5EwCicF6+ah+dsFM7DF8+LUzcK58Wr5uE5G4Xz8MXz4tSNwnnxqnl4zkbhPHzxvDh1o3BevGoenrNROA9fPC9O3SicF6+ah+dsFM7DF8+LU//js1T8CNOQQSC2COAdiyJFigR1D1I4vJ/g5Cc/QZJMg0HABwH85k//fYZPMwX9xNz3pNk3CLiNgInh3EbUyAuJgFG4kPCYk24jYBTObUSNvJAIGIULCY856TYC/wAMaiQ4Xe0THQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CharGenModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, num_timesteps, embedding_dim, rnn_output_dim, **kwargs):\n",
    "        super(CharGenModel, self).__init__(**kwargs)\n",
    "        self.embedding_layer = tf.keras.layers.Embedding(\n",
    "            vocab_size,\n",
    "            embedding_dim)\n",
    "        self.rnn_layer = tf.keras.layers.GRU(\n",
    "            num_timesteps,\n",
    "            recurrent_initializer='glorot_uniform',\n",
    "            recurrent_activation='sigmoid',\n",
    "            stateful=True,\n",
    "            return_sequences=True)\n",
    "        self.dense_layer = tf.keras.layers.Dense(vocab_size)\n",
    "    def call(self, x):\n",
    "        x = self.embedding_layer(x)\n",
    "        x = self.rnn_layer(x)\n",
    "        x = self.dense_layer(x)\n",
    "        return x\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_output_dim = 1024\n",
    "\n",
    "model = CharGenModel(vocab_size, seq_length, embedding_dim, rnn_output_dim)\n",
    "model.build(input_shape=(batch_size, seq_length))\n",
    "model.summary()\n",
    "tf.keras.utils.plot_model(model, 'model/RNN(1:n,GRU)/GRU_gen.png', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다음으로 손실함수를 정의하고 모델을 컴파일한다.\n",
    "- 여기서는 희소 범주형 교차 엔트로피(sparse_categorical_crossentropy)를 손실 함수로 사용하는데, 입력과 출력이 정수 시퀀스일 때 사용되는 표준 손실 함수이기 때문이다.\n",
    "- 최적화기는 Adam을 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, predictions):\n",
    "    return tf.losses.sparse_categorical_crossentropy(\n",
    "        labels,\n",
    "        predictions,\n",
    "        from_logits=True)\n",
    "\n",
    "model.compile(optimizer=tf.optimizers.Adam(), loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, prefix_string, char2idx, idx2char, num_chars_to_generate=10000, temperature=1.0):\n",
    "    input = [char2idx[s] for s in prefix_string]\n",
    "    input = tf.expand_dims(input, 0)\n",
    "    text_generated = []\n",
    "    model.reset_states()\n",
    "    for i in range(num_chars_to_generate):\n",
    "        preds = model(input)\n",
    "        preds = tf.squeeze(preds, 0) / temperature\n",
    "        # 모델이 반환한 문자 예측\n",
    "        pred_id = tf.random.categorical(\n",
    "            preds, num_samples=1)[-1, 0].numpy()\n",
    "        text_generated.append(idx2char[pred_id])\n",
    "        # 예측을 모델의 다음 입력으로 전달\n",
    "        input = tf.expand_dims([pred_id], 0)\n",
    "    \n",
    "    return prefix_string + \"\".join(text_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ht like little gabled, ‘Now.\"\"Noor, could it, and \"It wouldons be a fattle down and quite it less, as you go and come under on, persaised Alice speak.Hown’queter’me, with the had thing-prible for fetcrosempatfinged off again. Fover to goodroly, I began.\"Th it melunted too attencess, I'm just to see away yoptring.‘All going orcourned down ne work way withthings time!\"\"You much at here voice.\"N't,\" shouting her. If wis--and, that with which, with it whether shall thisK--this hireday or two this jock not voice--in the more time. And you minethatfully.‘To say--ho, \"Which said to,” if of my agreeme in find a word man’t?’ the Queen: And.‘It’s u surewhat it's hand, and a little strinking like any buy not are go join was not white take the fay and round a-- wondrogwinto mose,\" home rounch had in her much permaythe (did get so, when she could her best grave Archive Fryks, if too liked the Project Gutenberghis westributieftles and up to ME all hall!’ she a nile, thoese!’ she said to anyittion by the full handsile too meand sorr.* * * * Timough afrean:‘In't the undered have Ne VOULLINDEVE it do a little is sorty.\"\"You dishaction, so her, both un,\" said Alice is intent would you read in a find it had from out--if you incis and caller, and remay\"she said and from alking and the sunting, but that?’shouldn’t get that she had down and they don its teccried round on the tail quitenbarks to herster in a mustreepinning. Andthe sean, as she asked and remiper, set it twis long fromsbeed tumble, I come on faused to you, and arm,\" the can, if in at all she woodly, she with Kingreatment. In't have costrict sudden a 16-xitced distriect: shouldor: the lived the course her. ‘I then agree lisner,’ she tribernation]The Rabbit off tall! Alice and flow the horse very come: as you know,’ said the Whree ROU can itseed the dodig twell of a whore a prouble!\" and part to an allbe all with a little seeform abied, you know,’ The nowed Project Gutenberg-tm elis_ProjebArgurion in what she was acrinnyes le, anytame.\"It ser! Who where she gander dotay the Red Queen, bested Alice. ‘I to and shav't more?’ Alice said no bount would ntation in anying her vertulls vaict, ‘on the grow.’‘Why, but it's guess?’ so crozdes roore?\"[Illust higf in cried, IS pleas botmbef--’ And wat were Project Gutenberg and roudonsting pay. It was screa-mark as the vouldden over and had been Licensefurth of the boor of the to be goo, if you like all their two fall my why, when them agviduse of Jabjule as he shory ulaught ad it turplied sight of it Would one off said wish an other asking Alicum, \"Alice couldn’t comforted at elled on like at hiss wouldwonsecupping!’‘But he was come great you know these, she suind over it, plycosh, _Tunte as with the For cook you for drewa gament out rojocomether?’ Allow.\" compling think your whisnes she, and there going themow and pause Ise withatchousionpidicessed your apports. \"You're first that, but the glocat the could all the other.‘Whe wichliked itteren it digh, and the Queen conle I'll suplangle 'SMooking an enon of haxisthoven and to keze they happen, went the works firs cleug out to your e-dery. She, you scouttenbes--she carplance.\"\"Just he talking down and leances.\"\"Mook entacllengry and repeated win worh peffing of all the same and other?\" Alice said with!\" she said sone hold and permonter turned up?’ Alice clew,\" and found manyme down before, right the bounded out fay to turticar ream make’t get,’ said Stitting she hadfullagesthon on the airy.cup, and she spick: ‘Thisbillenst.Sen sinto this two whis down for a copeminate your-mause we continnow are, and a very being he with nex Queen’s hand,bit--o\" could not do agroon and noch voice.\"Abous-buing it went on froght sare them,\" shook it's a little tones, a mudemorup and use spleak a thinn, talough turn, and the chook, what you tho mis for a edecipt Alice tablear it round rowing e into her sure’she was seess at the eezling in a tame, and the paudenly, and sforefully: you know. UU First,\" said the Knige or very said.‘She had provided this againtering, at going Alice agmems to remapker it gown it wition of notamesed of a never souned to side about you door like the wrange by the tas the Red Queen pacts, and bless. Whattait dayshim, andow!’ she said air of courss again came! Which cave pecandeaprasidered the headd or again, or pabilits of two, \"I this ProjectGulesite him waited lars here isn’rmpleaded intere).Penasity to your toking-fin this court.’As like away, andthe p_orh arin away itsay.‘I'm go the missive thechim queer. A ventule at once of his if it was nothtly. ‘Ifit greatingshades: riffice little whiten straid, for of the to sho!’ the Lienchebly, is it haves after silbering: and be look the she oldny had curiousafter down they remarken!\"\"But down to must that to besise much a litter,’ she chouse while well be danct: \" anghersunten fereamaping! Noo, DI_MBoez. Is not on, both!\" she had coscould sat ittall clied adre one! Of could scopy be quetest we the botting effo mad to be repeict.But to see the Disten and,” yed her shall herlowd us the white Queen ax these begin so atement: Bit about them said HERADASTY Kink of themortreat, but as say,’ thought it wold who this ran to the certect the room a a shoom umble.\"Yes,’ the remeltly beginter sich and sceran?\" copy.For it jursqueer frying wouldn't backsond it them, was a door plasters aquent disht back her go puzzled up it?\" said the Dodment like to putting trying and refum aboute, andhe onever \"And here paye Restithoh it, trwer that the Robit las?’ the Queen, refering--. So they welpuce dlents the ting of-hese at here, though on great hearthement again it in a litexely, \"squeArmbory up and SUNiziw them. Hower \"heard asksonaboring them as she said now,\"The full with Romusisay for the red revery fild into the hadniffige usion that, what not,\" said the Spodderid took and getting tocle-rxticllice begin. Oh, she dree, Which way.’‘Dome igen, as she said to me!\"\"Nown shemecongitioush, fur on the beint do yourself by what are many down it, and one of any exactly tereed notjous not come of all usutioncould. \"Em mound it round the mean again, and in the Went on it all net movery much last floke in a timaws this this vouldes priedly dimes aiving times diffelantation: Fousn't you a fish earce.\"No dow?\"\"I dark outing-DORE a fight, \"I damown by him surections and getting be littlegul--That’s a bothort,’ said Alice. ‘felles as old how her use the kister aver fan whore this timewhich anvent off, as?’ Sheurthat legans than you gropsion alin pass eversum tried, he went on, “Then, she said into the sceat, the words out its retly,\" said the Faugh, screese dance or very allowed afrempeivide a dister come works what me it comaning.‘Wouknother held enlythe same and nice ittenage, IG got a \n",
      "---\n",
      "Epoch 1/10\n",
      "53/53 [==============================] - 4s 73ms/step - loss: 1.5161\n",
      "Epoch 2/10\n",
      "53/53 [==============================] - 4s 73ms/step - loss: 1.5111\n",
      "Epoch 3/10\n",
      "53/53 [==============================] - 4s 74ms/step - loss: 1.5057\n",
      "Epoch 4/10\n",
      "53/53 [==============================] - 4s 79ms/step - loss: 1.5037\n",
      "Epoch 5/10\n",
      "53/53 [==============================] - 4s 79ms/step - loss: 1.4964\n",
      "Epoch 6/10\n",
      "53/53 [==============================] - 4s 76ms/step - loss: 1.4948\n",
      "Epoch 7/10\n",
      "53/53 [==============================] - 4s 72ms/step - loss: 1.4895\n",
      "Epoch 8/10\n",
      "53/53 [==============================] - 4s 72ms/step - loss: 1.4850\n",
      "Epoch 9/10\n",
      "53/53 [==============================] - 4s 79ms/step - loss: 1.4838\n",
      "Epoch 10/10\n",
      "53/53 [==============================] - 4s 81ms/step - loss: 1.4785\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).embedding_layer.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).rnn_layer.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).embedding_layer.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).dense_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).rnn_layer.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "after epoch: 5after epoch: 5after epoch: 5after epoch: 5after epoch: 5after epoch: 5after epoch: 5after epoch: 5after epoch: 5after epoch: 5\n",
      "Alice of the nisterely well feetared, everybodyledorkwards?\" armouse to kettled mound hed speaking, largentely cook!\" said theFU.THE court oft things kecket your bards or a vo\"1.E.9. angery,\" the Queen are the full it as shewappened twick with first, ‘this would head bet, as its spreaking with it by redlyersclay,to sayintle about Alice, looking of theKing and then the Kniggless in a carpance, (ait kitting, only so say, \"ProjectGutenberg-daysle! Comply,’ he say they, with an head, won.‘Whically eached one Lion and say?’ she place, ‘I’ll, got your knopenjust the reas? I don't make very ser to no romation, couldveny coher for shaped bet!\"\"That’s a large on?’ Meswere my hands,’ she would, that would thene sevenly electly;--and it cried ald-going rule what I'll, grom cook of thetherow,because incope oneven the spoke! And his Lobs: I when swalker.\"Dever turf.\"\"IME, just be?’Tfiedly; she felt away about King was a.Legite eak, and around tiers as lastand it,\" said Break, and thanf sto excried to besterpanty course a precelful real Project Gutenberg-plyitting, another sure. \"I gaxestrong the copfich out just begunned botherself stasthin.\"\"Of course! There was nothingsitted 3. \"There's coll all tarat it would get been can it with it out Dieves are. Then er beg somether, my beg a romitior ups both!’‘Houratest would behind Alice, at the way!’ and she costaringoronen it if you can, and nearefully access to nearly, and master,with away hur, could a minution looking is a take I havin way to be a Lied down on the two and see retusiture or otherpsimtertly with that she are _thi“PLHIt’re cried wheealing itsaught, whether out on a life,’ said the White Know mound yreceling. ‘I’m clast. There’s on the lessond belan: she had nawershereep, oroor sense--’‘What _ish the rather must called _The Wale.Alice riddle: ‘to terms, so she eyindcould say?\" Alice thism loass, welld as what head to loster; and which voiceplade and up the aird: what _you.The ProjectGutenberg-tm is sightenbence, and me some lifed hooking in aftol!\" The sermmily. he sander her sound as she felt you was,\" said the withal of them. croand,\" the Rouse the roy.\"Well, then your evening, right forthin, and gomether butter, and roomed to under with hiredicledum, laugh) about eaching it very inviseep to begin time of manuse would you_ me,’ said: \"As one arried outing to see ittooding you asee so all prestan: I'll brokning (‘Disk at her my ever meadf you look! And she had so much growing as fearthing.\"You meliked you never.‘Ever _I.’‘Remover tothone!’ Alice ratainise upon really wasn’t puzzled agreems then goulscration?’ Tweedledum, in a sumpe to all a little Miches ready, the long three thinking fuctly breams time this toAuddnbuding there’ve been all, and full elick it deeation of all her. The Fauted again, chie. In't could be briviscrambling at the things-chast non’t they again probotable aprigh, as it WOUSADONSECh, \"or the top mim \"MIs!’ cried, and they're was almostion going down to know. The treat.CHAPTE CER AND NI’TOWISHIT WHING FROM RACF, Ifeat round his work inve--that my hoired.The Looking after finis; instind, but of the was out about un and ready rubly kid no set not she fact. Plolled: _this_--\"she seently stasted into the happing at alming voice. ‘I’mbling not no stay Ifasely. \"Wo, and it would be onef the weit and thouldn’t be certain--butto.FE. \"Of ccroveswould not couldn't know into the Red court while you alarch ture.Thengrield, behang) must be very dete! Ah, \"She age gring to do me the obrus, anbty?’She lail an of me name. The Durme0tubution and the bett?’ she said \"Dumpty Dinsing-rimalall tof afraid, looked with electro.\"\"I'm vent say bus like munter in’should were again--how were Sheep:[Illustript was as riburion with him, to you a heartreement, in the endedagaining,\" as I went putch flunud?’ Alice dind receive them afread it's befimen, withoutter insairs heard of cace me?\"\"I he give, you see one trasted.\"Not a loudlythe kittenly, preplied in when through to atwock into her, I think, she as she me’s she said, after thanself YOU don’t said Here is treme--bed seem, and tramee is the jury denight to sisployed ewn’t talk, and high! Thought a rattle into the Queen, ‘a never got of it, at looking is one’s the kittle of the Project Gutenberghickles. When I come other frie donatclove to getting the Unicin pactuaust hold outsened againing at your differenth, you should nicelyou truedabl!\" the Mock beher of reas!’ Alice can roundstone!\" slown,’ the King for at oning the topsing to her anviles when I’m we Queen SNo).\"Don't do Alice's hand, if you next largerys, theer, back dumpy out.’ And weAlice dis be cone follw. Alice had just in the tremorushougiment, and storts don’t if Alice.\"What neezich of the tried, fermborumwhener, or used in all the troublat work.‘By it impoboline thought mind any evire.\"If ender moment of do you had not going than three didn't MUS. twenred fbred.’Here impectid, who knownly.\"Dinenor a horpolning to your seetrmay that the arrs the Unicord--and Alice.\"Ah, and think it.\"\"Oh, but you fall my dartpented again, and cut. Sing wook the while a made theShe came Coon tell opened beand to much screat she managet, and she hore? Near, you know-Sould half at the work on it so said to Alice a little of the pattes threeld,bet, if you don't be goes one eithen his place, and stastine, out 1E.F5 their lone get very again, the of the shaves only comn in his little topp!\" Take?\" said Alice, mark in all!\"\"I houkness, in the boor beginning well dear she carrog, race inge!Haxtick, and she way beingsueded; and the too came once.’‘WhatI _snive dreaming bringlouses quite if its preatle, but he quested lengre over in the little,\" she said: \"but the house, ‘oh! it twill my could having herself better,’ Alice added it, You're was imbling to Alice talk ad tell me her wask,’ the Queen smiliong the Docried all SWeetryand a little just ond!\" and you were OF then took hay wereded here, belan away it could nothee fest notchation and bectiffullyperiep.She spead to the veryes as I seem that trouble, with when it at noir in imporcet up of a quickly?\" Manager without?\" Is eyint and brate, the door,invenyshe could hards, ‘OUts could copy room: you’re quite the accedself, your poor sort of roelt only round bir siddy, snoricioned grave the other. ‘Now went on en porus it dress, as sleepan now.\"All or help as my mays round been eithered it a little round rate and dear the confeecon easianch, would compostanty of them aboused begin to come and tryingershe people rang all at thought she shoe the perch thiluddan!\"_I don't goand hillohe out of the other knigh.And other.\" Andthen shoudding on the little think Inversem, and had never exprise’s have orrearafter up as saumber my tell to get-- Oh they used, \"said they knose, But LITERS Loop?\"\"When began. ‘I can shoke for the other a chast to hers, op of trying,\" the Hattledcerorthing bet. Uf shepote and dst. \"I feeling-y.‘Plew I don't like a braight out again, for I'm as than the Wile over thin.\"I do,’ the Mouse alarch, andork again off fire, one Cater,but was fowdy again. \"Hevern’t bess apailfulfieve! Rudt and sphere how Sout he looked at her more! There’s as you know writed; home wine, he mewise dispeadablas dear a tream hive have dic of littleto ear of itsille owherrooded; Yes, they're--theney would be a dishess face! wwo an hute: thoumpenessiffired to a phytalling from long for so Alice. \"No, the Catchem. The Cater Lutten. \"I down she’s defelw you our or is here-couldn’t bet the endifiething be reply nearer to put wen acranfieting sentribcee thingsat the Queen. ME the sighed:‘It’s her hands: \"The Project Gutenberg-license'S day. Non’t dole tape, trissicaling Arn was go on. \"Her have for the ploy.Set her gettleddabout elfered distributing to Ald in a regued: a fee is so think off towork nothide YOU. To tume is. He sha, I’ll more up to see, and the grat no and then the DEed to her isward---it unchedsams of not?\" said Alice. \"I _heriver like a Deind fightssing for kink took,\"So far himsilled for couldn’tryobots. And ham round ever.CO voice.‘Would you a sinter the reads soonerasfiers! The Hatter--on,\" And words, and flatch and said after,\" said Alice, ‘You hearding ran inseefrovellore. After--down to make thearin means took time (and said!’ the Caterply,\" and not LOTTHERY everget that Alice was as she child of the ProJaje pluss herse: They had put it.\"You egrill miving becorner,\" she could it and then came breveraring at latefully: but now, and out of than“Gryphon!)the Knight statuse is, shemogh, timidumet to see was the thrumection of theme inquiat helory, called ‘wrone again.\"Bill crow.\"\"You part, or your work ould sudding tell a grearat the distrang her right him helit you came very get underrifisionaby!’‘OL and she kettyall suddand, ulls up done--Never you_.\"By whink to heads.’‘In anyssosifiedn, and that Alice,’ said Alice: ‘be!\" said her she came a liams a things!\" Only you, welling sharm old made a works all said. I wish again. And but the White Queen, till thoug” in here, said awow do Alice was to, distrare I mishere the took to the other soon, and the Pien,\" said \"I go to, that’s the choefedfine. She were tile--ce dear in she siggon the with no ctasted or dread8. Inlitharchy excired and put its ihe of that she moment some you deelinsted and she said, and my wat if you shan’t keeply some charped how considers, was soles and no Knight stop groves. How _were,’ said garked when here read to _youht he win oed on. And the mouse The King, shethe neeble--of them! And to hearhs; table,\" the Frien of chore, 92: Hever a eance rather in the very ways the Hatter: he walked here frunitwappen, please _voice.\"Roin, andon’t keeple-thing boor. Then issarm execut to her more grown. ‘Then he a pavedy therochous_,’ said Alice age to,\" said the Queen provoke.‘I do it another picouted no_s addidesaip tomoups agree of the expense. ‘I canall to can't copyprovospirely off to get up, we keep of great fould #6560[alive ands works aftig sure with the dreat verybwold \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "for i in range(num_epochs // 10):\n",
    "    model.fit(\n",
    "        dataset.repeat(),\n",
    "        epochs=10,\n",
    "        steps_per_epoch=steps_per_epoch\n",
    "        # callbacks=[checkpoint_callback, tensorboard_callback]\n",
    "    )\n",
    "\n",
    "    checkpoint_file = os.path.join(\n",
    "        CHECKPOINT_DIR, \"model_epoch_{:d}\".format(i+1))\n",
    "    model.save_weights(checkpoint_file)\n",
    "\n",
    "    # 지금까지(10에폭마다) 훈련된 모델을 사용해 생성 모델 재구축\n",
    "    gen_model = CharGenModel(vocab_size, seq_length, embedding_dim, rnn_output_dim)\n",
    "    gen_model.load_weights(checkpoint_file)\n",
    "    gen_model.build(input_shape=(1, seq_length))\n",
    "\n",
    "    print(\"after epoch: {:d}\".format(i+1)*10)\n",
    "    print(generate_text(gen_model, \"Alice \", char2idx, idx2char))\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다대일 감정 분석 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이 예제에서는 문장을 입력, 해당 감정을 양수나 음수로 예측해본다.\n",
    "- 데이터셋은 UCI 머신러닝 저장소에 있는 레이블된 문장집합(Amazon,IMDb,Yelp 등에서 구한 3000여개의 문장 집합으로서 부정적인 감정을 표현할 경우 0으로 레이블되고 긍정적인 감정을 표현할 경우에는 1로 레이블된다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_and_read(url):\n",
    "    local_file = url.split('/')[-1]\n",
    "    local_file = local_file.replace('%20', ' ')\n",
    "    p = tf.keras.utils.get_file(local_file, url, extract=True, cache_dir='.')\n",
    "    local_folder = os.path.join(\"./datasets\", local_file.split('.')[0])\n",
    "    labeled_sentences = []\n",
    "    for labeled_filename in os.listdir(local_folder):\n",
    "        if labeled_filename.endswith('_labelled.txt'):\n",
    "            with open(os.path.join(\n",
    "                    local_folder, labeled_filename), 'r') as f:\n",
    "                for line in f:\n",
    "                    sentence, label = line.strip().split('\\t')\n",
    "                    labeled_sentences.append((sentence, label))\n",
    "    return labeled_sentences\n",
    "\n",
    "labeled_sentences = download_and_read(\n",
    "    'https://archive.ics.uci.edu/ml/machine-learning-databases/' +\n",
    "    '00331/sentiment%20labelled%20sentences.zip')\n",
    "sentences = [s for (s, l) in labeled_sentences]\n",
    "labels = [int(l) for (s, l) in labeled_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3000,),\n",
       " 'So there is no way for me to plug it in here in the US unless I go by a converter.',\n",
       " list)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(sentences), sentences[0], type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['So there is no way for me to plug it in here in the US unless I go by a converter.',\n",
       " 'Good case, Excellent value.',\n",
       " 'Great for the jawbone.',\n",
       " 'Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!',\n",
       " 'The mic is great.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 여기서의 목표는 입력된 문장에 대해 제공된 레이블에 있는 해당 감정을 예측하는 방법을 학습하도록 모델을 훈련시키는 것이다.\n",
    "- 각 문장은 단어의 시퀀스이다. 그러나 모델에 입력하려면 정수 시퀀스로 변환해야 한다.\n",
    "- 시퀀스의 각 정수는 단어를 가리킨다.\n",
    "- 말뭉치의 단어를 정수로 매핑한 것을 어휘라고 한다.\n",
    "- 따라서 문장을 토큰화하고 어휘를 만들어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 5271\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "vocab_size = len(tokenizer.word_counts)\n",
    "print('vocabulary size: {:d}'.format(vocab_size))\n",
    "\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v:k for (k, v) in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(75, 16.0), (80, 18.0), (90, 22.0), (95, 26.0), (99, 36.0), (100, 71.0)]\n"
     ]
    }
   ],
   "source": [
    "seq_lengths = np.array([len(s.split()) for s in sentences])\n",
    "print([(p, np.percentile(seq_lengths, p)) for p in [75,80,90,95,99,100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결과에서 보듯 최대 문장 길이는 71단어지만 문장의 99%는 36단어 이하이다. 예를 들어 64라는 값을 선택한다면 대부분의 문장을 자를 필요 없다.\n",
    "- 다음 단계는 모델이 사용할 수 있는 데이터셋을 만든다.\n",
    "- 먼저 훈련된 토크나이저를 사용해 각 문장을 단어 시퀀스에서 정수 시퀀스로 변환한다. 여기서 각 해당 정수는 tokenizer.word_index에 있는 단어의 인덱스다. 그런 다음 잘리고 0으로 채워짐\n",
    "- 레이블은 넘파이 배열 labels_as_ints로 변환되고, 마지막으로 텐서 sentences_as_ints와 labels_as_ints를 결합해 텐서플로 데이터셋을 형성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seqlen = 64\n",
    "\n",
    "# 데이터셋 생성\n",
    "sentences_as_ints = tokenizer.texts_to_sequences(sentences)\n",
    "sentences_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    sentences_as_ints, maxlen=max_seqlen)\n",
    "labels_as_ints = np.array(labels)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((sentences_as_ints, labels_as_ints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3000, 64),\n",
       " array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,   27,\n",
       "          48,    5,   56,  116,   12,   70,    7,  370,    6,   11,   64,\n",
       "          11,    1,  186,  578,    3,   75,   60,    4, 2267], dtype=int32))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(sentences_as_ints), sentences_as_ints[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터셋의 1/3은 평가용으로 남겨두려 한다. 나머지 데이터 2/3중 10%는 모델이 훈련 중 자체 진행상황을 측정하는데 사용할 인라인 검증 데이터셋으로 사용하고 90%는 훈련 데이터셋으로 사용한다.\n",
    "- 마지막으로 각 데이터셋에 대해 64문장의 배치를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(10000)\n",
    "test_size = len(sentences) // 3\n",
    "val_size = (len(sentences) - test_size) // 10\n",
    "test_dataset = dataset.take(test_size)\n",
    "val_dataset = dataset.skip(test_size).take(val_size)\n",
    "train_dataset = dataset.skip(test_size + val_size)\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 각 입력 문장은 max_seqlen(64) 크기의 정수 시퀀스다.\n",
    "- 이는 임베딩 계층에 입력돼 각 단어를 어휘+1 크기의 벡터로 변환함\n",
    "- 그다음 양방향 LSTM 계층으로 공급되고, 이는 각 단어를 크기 64의 벡터로 변환함\n",
    "- 각 타임스텝에서 LSTM의 출력은 밀집계층으로 공금되고 이는 ReLU 활성화와 함께 크기 64인 벡터를 생성함\n",
    "- 이 밀집계층의 출력은 다른 밀집 계층으로 공급되고, 이는 각 타임스텝에서 (1,)형태의 벡터를 출력하고 sigmoid 활성화를 통해 변조됨\n",
    "- 모델은 이진 교차 엔트로피(binary_crossentropy)손실 함수와 Adam최적화기를 사용해 컴파일되고 10에폭 동안 훈련한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sentiment_analysis_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      multiple                  337408    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection multiple                  66048     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              multiple                  8256      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              multiple                  65        \n",
      "=================================================================\n",
      "Total params: 411,777\n",
      "Trainable params: 411,777\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class SentimentAnalysisModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, max_seqlen, **kwargs):\n",
    "        super(SentimentAnalysisModel, self).__init__(**kwargs)\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size, max_seqlen)\n",
    "        self.bilstm = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(max_seqlen))\n",
    "        self.dense = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.out = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.bilstm(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "model = SentimentAnalysisModel(vocab_size+1, max_seqlen)\n",
    "model.build(input_shape=(batch_size, max_seqlen))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컴파일\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 2/29 [=>............................] - ETA: 3s - loss: 0.6942 - accuracy: 0.5156WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.101762). Check your callbacks.\n",
      "29/29 [==============================] - 2s 85ms/step - loss: 0.6930 - accuracy: 0.5239 - val_loss: 0.6865 - val_accuracy: 0.7250\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 1s 46ms/step - loss: 0.6435 - accuracy: 0.6633 - val_loss: 0.5277 - val_accuracy: 0.8200\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 1s 47ms/step - loss: 0.4094 - accuracy: 0.8394 - val_loss: 0.3169 - val_accuracy: 0.9050\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 1s 45ms/step - loss: 0.2522 - accuracy: 0.9133 - val_loss: 0.2109 - val_accuracy: 0.9150\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 1s 44ms/step - loss: 0.1736 - accuracy: 0.9389 - val_loss: 0.0873 - val_accuracy: 0.9650\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 1s 42ms/step - loss: 0.1281 - accuracy: 0.9578 - val_loss: 0.1077 - val_accuracy: 0.9650\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 1s 41ms/step - loss: 0.0859 - accuracy: 0.9728 - val_loss: 0.0807 - val_accuracy: 0.9750\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 1s 43ms/step - loss: 0.0666 - accuracy: 0.9844 - val_loss: 0.0219 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 1s 44ms/step - loss: 0.0620 - accuracy: 0.9839 - val_loss: 0.0334 - val_accuracy: 0.9900\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 1s 45ms/step - loss: 0.0489 - accuracy: 0.9878 - val_loss: 0.0516 - val_accuracy: 0.9950\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-45915623e5cf62a0\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-45915623e5cf62a0\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 훈련\n",
    "data_dir = './model/RNN_n_1_LSTM'\n",
    "logs_dir = './logs/RNN_n_1_LSTM'\n",
    "best_model_file = os.path.join(data_dir, 'best_model.h5')\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(best_model_file,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True)\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=logs_dir)\n",
    "num_epochs = 10\n",
    "history = model.fit(train_dataset, epochs=num_epochs,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[checkpoint, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 50841), started 0:01:21 ago. (Use '!kill 50841' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/RNN_n_1_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 14ms/step - loss: 0.0368 - accuracy: 0.9910\n",
      "test_loss: 0.037, test accuracy: 0.991\n"
     ]
    }
   ],
   "source": [
    "best_model = SentimentAnalysisModel(vocab_size+1, max_seqlen)\n",
    "best_model.build(input_shape=(batch_size, max_seqlen))\n",
    "best_model.load_weights(best_model_file)\n",
    "best_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 데이터셋에 대해 모델을 평가하는 가장 쉬운 고급 방법은 model.evaluate() 호출을 사용하는 것이다.\n",
    "test_loss, test_acc = best_model.evaluate(test_dataset)\n",
    "print('test_loss: {:.3f}, test accuracy: {:.3f}'.format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 또한 model.predict()를 사용해 예측하고, 이를 개별적으로 레이블과 비교하고 외부 도구(예를들어 scikit-learn)를 사용해 결과를 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(40, 64), dtype=int32, numpy=\n",
       "array([[   0,    0,    0, ...,  215,   17, 1672],\n",
       "       [   0,    0,    0, ..., 4035,    1, 1431],\n",
       "       [   0,    0,    0, ...,    9,    1,   29],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  292,  118,  246],\n",
       "       [   0,    0,    0, ...,    4,   18, 2705],\n",
       "       [   0,    0,    0, ...,   33,  243,  654]], dtype=int32)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(40,), dtype=int64, numpy=\n",
       "array([1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0])>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "LABEL\tPRED\tSENTENCE\n",
      "------------------------\n",
      "0\t0\tall three broke within two months of use\n",
      "0\t0\tlewis black's considerable talent is wasted here too as he is at his most incendiary when he is unrestrained which the pg 13 rating certainly won't allow\n",
      "1\t1\ti'd advise anyone to go and see it\n",
      "0\t0\tthe holster that arrived did not match the photo in the ad\n",
      "1\t1\tgood price\n",
      "0\t0\tspend your money and time some place else\n",
      "1\t1\toverall i rate this movie a 10 out of a 1 10 scale\n",
      "1\t1\tso far it has worked like a charm\n",
      "0\t0\ti'm so sorry but i really can't recommend it to anyone\n",
      "1\t1\tis pretty funny\n",
      "1\t1\tgreat food for the price which is very high quality and house made\n",
      "0\t0\ti does not maintain a connection with the computer while it is on my lap\n",
      "0\t0\tthis film and i use that word loosely is an insult to the movie going public\n",
      "0\t0\tthe ambiance isn't much better\n",
      "1\t0\ti don't think you will be disappointed\n",
      "0\t0\tno additional ear gels provided and no instructions whatsoever\n",
      "1\t1\ti will continue to come here on ladies night andddd date night highly recommend this place to anyone who is in the area\n",
      "1\t1\tlightweight and works well\n",
      "1\t1\tvoice recognition is tremendous\n",
      "0\t0\tsay bye bye to your tip lady\n",
      "1\t1\tbut despite these few flaws this case is of exceptional quality and well worth the additional costs of owning an official oem product\n",
      "0\t0\tthe dialogue sucked\n",
      "1\t1\tthe movie i received was a great quality film for it's age\n",
      "0\t0\tthe plug did not work very well\n",
      "1\t1\tit was absolutely amazing\n",
      "0\t0\ti would advise to not purchase this item it never worked very well\n",
      "1\t1\tursula burton's portrayal of the nun is both touching and funny at the same time with out making fun of nuns or the church\n",
      "0\t0\ti kept catching the cable on the seat and i had to pull the phone out to turn it on an off\n",
      "0\t0\tthis is the first phone i've had that has been so cheaply made\n",
      "0\t0\tbut other than that the movie seemed to drag and the heroes didn't really work for their freedom\n",
      "1\t1\tthe waitress was friendly and happy to accomodate for vegan veggie options\n",
      "1\t1\tevery time i eat here i see caring teamwork to a professional degree\n",
      "0\t0\tthat's how i'd describe this painfully dreary time waster of a film\n",
      "1\t1\tthe roast beef sandwich tasted really good\n",
      "0\t0\tbad reception\n",
      "1\t1\tthe cinematography is simply stunning to say the least and the fx are nothing if not state of the art\n",
      "1\t1\tif you love death and decay and shakespears lyrics this is the one\n",
      "0\t0\ti'll take my business dinner dollars elsewhere\n",
      "0\t0\tdefinitely not worth the 3 i paid\n",
      "1\t1\tdelicious\n",
      "0\t0\tany grandmother can make a roasted chicken better than this one\n",
      "0\t0\tas many people complained i found this headset's microphone was very weak\n",
      "1\t1\tsimply beautiful\n",
      "0\t0\tthe story starts too fast with absolutely no suspense or build up in the slightest\n",
      "1\t1\ti have only had it for a few weeks but so far so good\n",
      "0\t0\tcharacters are one dimensional even the good guys and especially the bad guys\n",
      "1\t1\teverything about it is fine and reasonable for the price i e\n",
      "0\t0\tand the beans and rice were mediocre at best\n",
      "0\t0\twe waited for forty five minutes in vain\n",
      "1\t1\tpretty good beer selection too\n",
      "1\t1\tthe rest of the cast also play well\n",
      "0\t0\twe watched our waiter pay a lot more attention to other tables and ignore us\n",
      "0\t0\tthis is infuriating\n",
      "1\t1\ti love the owner chef his one authentic japanese cool dude\n",
      "0\t0\tit didn't make me scared horrified or make me sympathetic towards the characters it was simply annoying\n",
      "1\t1\tif you want a real scare rent this one\n",
      "0\t0\ti consider this theft\n",
      "0\t0\twhat should have been a hilarious yummy christmas eve dinner to remember was the biggest fail of the entire trip for us\n",
      "1\t1\tthe fact is this film is a wonderful heartwarming tale about two people chasing their dreams\n",
      "0\t0\ti hate to disagree with my fellow yelpers but my husband and i were so disappointed with this place\n",
      "1\t1\tthis wonderful experience made this place a must stop whenever we are in town again\n",
      "0\t0\tmy only complaint is the standard sound volume is a little low even when turned up to 5 of 5\n",
      "1\t1\tmark my words this is one of those cult films like evil dead 2 or phantasm that people will still be discovering and falling in love with 20 30 40 years down the line\n",
      "0\t0\tplug was the wrong size\n",
      "\n",
      "accuracy score: 0.991\n",
      "\n",
      "confusion matrix\n",
      "[[488   5]\n",
      " [  4 503]]\n"
     ]
    }
   ],
   "source": [
    "labels, predictions = [], []\n",
    "idx2word[0] = \"PAD\"\n",
    "is_first_batch = True\n",
    "for test_batch in test_dataset:\n",
    "    inputs_b, labels_b = test_batch\n",
    "    pred_batch = best_model.predict(inputs_b)\n",
    "    predictions.extend([(1 if p > 0.5 else 0) for p in pred_batch])\n",
    "    labels.extend([l for l in labels_b])\n",
    "    if is_first_batch:\n",
    "        # 레이블, 예측, 문장 배치를 프린트\n",
    "        print(\"------------------------\\n\" + \n",
    "        \"LABEL\\tPRED\\tSENTENCE\" + \n",
    "        \"\\n------------------------\")\n",
    "        for rid in range(inputs_b.shape[0]):\n",
    "            words = [idx2word[idx] for idx in inputs_b[rid].numpy()]\n",
    "            words = [w for w in words if w != \"PAD\"]\n",
    "            sentence = \" \".join(words)\n",
    "            print(\"{:d}\\t{:d}\\t{:s}\".format(labels[rid], predictions[rid], sentence))\n",
    "        is_first_batch = False\n",
    "\n",
    "print(\"\\naccuracy score: {:.3f}\".format(accuracy_score(labels, predictions)))\n",
    "print(\"\\nconfusion matrix\")\n",
    "print(confusion_matrix(labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다대다 POS(품사) 태깅 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/HumanRevolution/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/treebank.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of records: 3914\n"
     ]
    }
   ],
   "source": [
    "def download_and_read(dataset_dir, num_pairs=None):\n",
    "    sent_filename = os.path.join(dataset_dir, \"treebank-sents.txt\")\n",
    "    poss_filename = os.path.join(dataset_dir, \"treebank-poss.txt\")\n",
    "    if not(os.path.exists(sent_filename) and os.path.exists(poss_filename)):\n",
    "        import nltk\n",
    "\n",
    "        if not os.path.exists(dataset_dir):\n",
    "            os.makedirs(dataset_dir)\n",
    "        fsents = open(sent_filename, 'w')\n",
    "        fposs = open(poss_filename, 'w')\n",
    "        sentences = nltk.corpus.treebank.tagged_sents()\n",
    "        for sent in sentences:\n",
    "            fsents.write(\" \".join([w for w, p in sent]) + '\\n')\n",
    "            fposs.write(\" \".join([p for w, p in sent]) + '\\n')\n",
    "\n",
    "        fsents.close()\n",
    "        fposs.close()\n",
    "    sents, poss = [],[]\n",
    "    with open(sent_filename, 'r') as fsent:\n",
    "        for idx, line in enumerate(fsent):\n",
    "            sents.append(line.strip())\n",
    "            if num_pairs is not None and idx >= num_pairs:\n",
    "                break\n",
    "\n",
    "    with open(poss_filename, 'r') as fposs:\n",
    "        for idx, line in enumerate(fposs):\n",
    "            poss.append(line.strip())\n",
    "            if num_pairs is not None and idx >= num_pairs:\n",
    "                break\n",
    "    return sents, poss\n",
    "\n",
    "sents, poss = download_and_read(\"./datasets/RNN_n_n_GRU\")\n",
    "assert(len(sents) == len(poss))\n",
    "print('# of records: {:d}'.format(len(sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab sizes (source): 9001, (target): 39\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_build_vocab(texts, vocab_size=None, lower=True):\n",
    "    if vocab_size is None:\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer(lower=lower)\n",
    "    else:\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "            num_words=vocab_size+1, oov_token='UNK', lower=lower)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    if vocab_size is not None:\n",
    "        # 추가적 우회 방법 이슈 8092를 참고\n",
    "        # https://github.com/keras-team/keras/issues/8092\n",
    "        tokenizer.word_index = {e:i for e, i in tokenizer.word_index.items() if i <= vocab_size+1}\n",
    "    word2idx = tokenizer.word_index\n",
    "    idx2word = {v:k for k, v in word2idx.items()}\n",
    "    return word2idx, idx2word, tokenizer\n",
    "\n",
    "word2idx_s, idx2word_s, tokenizer_s = tokenize_and_build_vocab(\n",
    "    sents, vocab_size=9000)\n",
    "word2idx_t, idx2word_t, tokenizer_t = tokenize_and_build_vocab(\n",
    "    poss, vocab_size=38, lower=False)\n",
    "source_vocab_size = len(word2idx_s)\n",
    "target_vocab_size = len(word2idx_t)\n",
    "print('vocab sizes (source): {:d}, (target): {:d}'.format(\n",
    "    source_vocab_size, target_vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문장의 토큰수와 해당 POS태그 순서는 동일하지만 문장의 길이는 달라진다.\n",
    "- 신경망은 동일한 입력길이를 넣어줘야 하므로 문장길이를 지정해줘야 한다.\n",
    "- 다음의 코드는 다양한 백분위수를 계산하고 그에 해당하는 문장길이를 출력해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(75, 33.0), (80, 35.0), (90, 41.0), (95, 47.0), (99, 58.0), (100, 271.0)]\n"
     ]
    }
   ],
   "source": [
    "sequence_lengths = np.array([len(s.split()) for s in sents])\n",
    "print([(p, np.percentile(sequence_lengths, p)) for p in [75,80,90,95,99,100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문장 길이를 100정도로 설정한다면 대부분 문제없이 지나가고 그중 몇 개는 잘려나갈 것이다. 선택한 길이보다 짧은 문장은 끝이 채워진다. 그러나 여기서는 데이터셋이 작기 때문에 최대한 많이 사용하는 것이 더 선호되므로 최대 길이로 선택한다.\n",
    "- 다음 단계는 입력에서 데이터셋을 만드는 것이다.\n",
    "- 첫째, 입출력 시퀀스의 토큰시퀀스와 POS태그를 정수 시퀀스로 변환\n",
    "- 둘째, 최대 문장길이보다 더 짧은 시퀀스는 최대길이인 271로 채운다.\n",
    "- #### POS 태그 시퀀스를 정수 시퀀스로 변환후 그대로 유지하지 않고 패딩 후 추가적인 연산을 수행하는데 to_categorical() 함수를 사용해 원핫 인코딩 시퀀스로 변환한다는 것에 주목\n",
    "- 셋째, from_tensor_slices() 함수를 사용해 데이터셋을 생성하고 '섞은 후' 훈련,검증,테스트 집합으로 분할한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seqlen = 271\n",
    "sents_as_ints = tokenizer_s.texts_to_sequences(sents)\n",
    "sents_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    sents_as_ints, maxlen=max_seqlen, padding='post')\n",
    "poss_as_ints = tokenizer_t.texts_to_sequences(poss)\n",
    "poss_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    poss_as_ints, maxlen=max_seqlen, padding='post')\n",
    "poss_as_catints = []\n",
    "for p in poss_as_ints:\n",
    "    poss_as_catints.append(tf.keras.utils.to_categorical(\n",
    "        p, num_classes=target_vocab_size, dtype='int32'))\n",
    "poss_as_catints = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    poss_as_catints, maxlen=max_seqlen)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((sents_as_ints, poss_as_catints))\n",
    "\n",
    "idx2word_s[0], idx2word_t[0] = 'PAD', 'PAD'\n",
    "\n",
    "# 훈련, 검증, 테스트 데이터셋으로 분할\n",
    "dataset = dataset.shuffle(10000)\n",
    "test_size = len(sents) // 3\n",
    "val_size = (len(sents) - test_size) // 10\n",
    "test_dataset = dataset.take(test_size)\n",
    "val_dataset = dataset.skip(test_size).take(val_size)\n",
    "train_dataset = dataset.skip(test_size + val_size)\n",
    "\n",
    "# 배치 생성\n",
    "batch_size = 128\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<BatchDataset shapes: ((None, 271), (None, 271, 39)), types: (tf.int32, tf.int32)>,\n",
       " <BatchDataset shapes: ((None, 271), (None, 271, 39)), types: (tf.int32, tf.int32)>,\n",
       " <BatchDataset shapes: ((None, 271), (None, 271, 39)), types: (tf.int32, tf.int32)>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset,val_dataset,test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다음은 모델을 정의하고 인스턴스화 한다.\n",
    "- 이 모델은 임베딩계층, 드롭아웃계층, 양방향 GRU계층, 밀집계층, 소프트맥스 활성화 계층으로 구성된 순차 모델이다.\n",
    "- 입력은 형태가 (batch_size, max_seqlen)인 정수 시퀀스 배치, 임베딩 계층을 통과 후 출력은 형태가 (batch_size, max_seqlen, embedding_dim), 출력 차원이 256인 양방향 GRU의 해당 타임스텝으로 전달, GRU는 양방향이므로 하나의 GRU를 다른 GRU위에 쌓는 것과 같다. 따라서 나오는 텐서는 차원이 (batch_size, max_seqlen, 2 x rnn_output_dimension), 형태가 (batch_size, 1, 2 x rnn_output_dimension)인 각 타임스텝의 텐서는 밀집계층으로 공급, 각 타입스텝을 대상어휘와 같은 크기의 벡터, 즉 (batch_size, number_of_timesteps, output_vocab_size)로 변환, 최종 소프트맥스 계층은 각 타임스텝에 적용돼 출력 POS 토큰의 시퀀스를 반환, 마지막으로 일부 매개변수를 선언, Adam최적화기, 범주형 교차 엔트로피 손실함수, 척도-정확도로 설정하고 컴파일한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"pos_tagging_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     multiple                  1152128   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial multiple                  0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection multiple                  592896    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist multiple                  20007     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    multiple                  0         \n",
      "=================================================================\n",
      "Total params: 1,765,031\n",
      "Trainable params: 1,765,031\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class POSTaggingModel(tf.keras.Model):\n",
    "    def __init__(self, source_vocab_size, taeget_vocab_size, embedding_dim, max_seqlen, rnn_output_dim, **kwargs):\n",
    "        super(POSTaggingModel, self).__init__(**kwargs)\n",
    "        self.embed = tf.keras.layers.Embedding(\n",
    "            source_vocab_size, embedding_dim, input_length=max_seqlen)\n",
    "        self.dropout = tf.keras.layers.SpatialDropout1D(0.2)\n",
    "        self.rnn = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.GRU(rnn_output_dim, return_sequences=True))\n",
    "        self.dense = tf.keras.layers.TimeDistributed(\n",
    "            tf.keras.layers.Dense(target_vocab_size))\n",
    "        self.activation = tf.keras.layers.Activation('softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.rnn(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "def masked_accuracy():\n",
    "    def masked_accuracy_fn(ytrue, ypred):\n",
    "        ytrue = tf.keras.backend.argmax(ytrue, axis=-1)\n",
    "        ypred = tf.keras.backend.argmax(ypred, axis=-1)\n",
    "        mask = tf.keras.backend.cast(\n",
    "            tf.keras.backend.not_equal(ypred, 0), tf.int32)\n",
    "        matches = tf.keras.backend.cast(\n",
    "            tf.keras.backend.equal(ytrue, ypred), tf.int32) * mask\n",
    "        numer = tf.keras.backend.sum(matches)\n",
    "        denom = tf.keras.backend.maximum(tf.keras.backend.sum(mask), 1)\n",
    "        accuracy = numer / denom\n",
    "        return accuracy\n",
    "    return masked_accuracy_fn\n",
    "\n",
    "embedding_dim = 128\n",
    "rnn_output_dim = 256\n",
    "\n",
    "model = POSTaggingModel(source_vocab_size, target_vocab_size, embedding_dim, max_seqlen, rnn_output_dim)\n",
    "model.build(input_shape=(batch_size, max_seqlen))\n",
    "model.summary()\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy', masked_accuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.4451 - accuracy: 0.8647 - masked_accuracy_fn: 0.0022 - val_loss: 0.3254 - val_accuracy: 0.9126 - val_masked_accuracy_fn: 0.0667\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 0.3265 - accuracy: 0.9175 - masked_accuracy_fn: 0.0916 - val_loss: 0.3109 - val_accuracy: 0.9240 - val_masked_accuracy_fn: 0.1059\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 32s 2s/step - loss: 0.3168 - accuracy: 0.9196 - masked_accuracy_fn: 0.1442 - val_loss: 0.3124 - val_accuracy: 0.9138 - val_masked_accuracy_fn: 0.1374\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 29s 2s/step - loss: 0.3017 - accuracy: 0.9149 - masked_accuracy_fn: 0.0834 - val_loss: 0.2962 - val_accuracy: 0.9137 - val_masked_accuracy_fn: 0.0535\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 30s 2s/step - loss: 0.2742 - accuracy: 0.9208 - masked_accuracy_fn: 0.1109 - val_loss: 0.2683 - val_accuracy: 0.9233 - val_masked_accuracy_fn: 0.1281\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 31s 2s/step - loss: 0.2575 - accuracy: 0.9268 - masked_accuracy_fn: 0.1568 - val_loss: 0.2630 - val_accuracy: 0.9218 - val_masked_accuracy_fn: 0.1254\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 29s 2s/step - loss: 0.2489 - accuracy: 0.9261 - masked_accuracy_fn: 0.1397 - val_loss: 0.2421 - val_accuracy: 0.9277 - val_masked_accuracy_fn: 0.1312\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 30s 2s/step - loss: 0.2450 - accuracy: 0.9280 - masked_accuracy_fn: 0.1648 - val_loss: 0.2533 - val_accuracy: 0.9266 - val_masked_accuracy_fn: 0.1781\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 31s 2s/step - loss: 0.2387 - accuracy: 0.9327 - masked_accuracy_fn: 0.2138 - val_loss: 0.2379 - val_accuracy: 0.9341 - val_masked_accuracy_fn: 0.2566\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 31s 2s/step - loss: 0.2352 - accuracy: 0.9356 - masked_accuracy_fn: 0.2636 - val_loss: 0.2227 - val_accuracy: 0.9403 - val_masked_accuracy_fn: 0.3217\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 31s 2s/step - loss: 0.2240 - accuracy: 0.9396 - masked_accuracy_fn: 0.3065 - val_loss: 0.2375 - val_accuracy: 0.9371 - val_masked_accuracy_fn: 0.3910\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 31s 2s/step - loss: 0.2146 - accuracy: 0.9422 - masked_accuracy_fn: 0.3408 - val_loss: 0.1938 - val_accuracy: 0.9480 - val_masked_accuracy_fn: 0.3677\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 30s 2s/step - loss: 0.2034 - accuracy: 0.9443 - masked_accuracy_fn: 0.3664 - val_loss: 0.1980 - val_accuracy: 0.9465 - val_masked_accuracy_fn: 0.4034\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 29s 2s/step - loss: 0.1903 - accuracy: 0.9482 - masked_accuracy_fn: 0.4028 - val_loss: 0.1828 - val_accuracy: 0.9510 - val_masked_accuracy_fn: 0.3746\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 30s 2s/step - loss: 0.1855 - accuracy: 0.9502 - masked_accuracy_fn: 0.4375 - val_loss: 0.1799 - val_accuracy: 0.9525 - val_masked_accuracy_fn: 0.5396\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 30s 2s/step - loss: 0.1737 - accuracy: 0.9537 - masked_accuracy_fn: 0.4714 - val_loss: 0.1586 - val_accuracy: 0.9577 - val_masked_accuracy_fn: 0.4659\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 30s 2s/step - loss: 0.1663 - accuracy: 0.9562 - masked_accuracy_fn: 0.4958 - val_loss: 0.1599 - val_accuracy: 0.9568 - val_masked_accuracy_fn: 0.4653\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 30s 2s/step - loss: 0.1604 - accuracy: 0.9576 - masked_accuracy_fn: 0.5212 - val_loss: 0.1625 - val_accuracy: 0.9569 - val_masked_accuracy_fn: 0.5540\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 28s 1s/step - loss: 0.1510 - accuracy: 0.9602 - masked_accuracy_fn: 0.5402 - val_loss: 0.1549 - val_accuracy: 0.9594 - val_masked_accuracy_fn: 0.5654\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 29s 2s/step - loss: 0.1492 - accuracy: 0.9606 - masked_accuracy_fn: 0.5565 - val_loss: 0.1393 - val_accuracy: 0.9632 - val_masked_accuracy_fn: 0.5888\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 29s 2s/step - loss: 0.1429 - accuracy: 0.9623 - masked_accuracy_fn: 0.5743 - val_loss: 0.1401 - val_accuracy: 0.9633 - val_masked_accuracy_fn: 0.6249\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 29s 2s/step - loss: 0.1409 - accuracy: 0.9623 - masked_accuracy_fn: 0.5716 - val_loss: 0.1381 - val_accuracy: 0.9626 - val_masked_accuracy_fn: 0.5723\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 29s 2s/step - loss: 0.1365 - accuracy: 0.9629 - masked_accuracy_fn: 0.5820 - val_loss: 0.1296 - val_accuracy: 0.9641 - val_masked_accuracy_fn: 0.5501\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 30s 2s/step - loss: 0.1303 - accuracy: 0.9643 - masked_accuracy_fn: 0.5948 - val_loss: 0.1194 - val_accuracy: 0.9670 - val_masked_accuracy_fn: 0.6149\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 29s 2s/step - loss: 0.1261 - accuracy: 0.9652 - masked_accuracy_fn: 0.6081 - val_loss: 0.1242 - val_accuracy: 0.9650 - val_masked_accuracy_fn: 0.5616\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 30s 2s/step - loss: 0.1230 - accuracy: 0.9657 - masked_accuracy_fn: 0.6132 - val_loss: 0.1157 - val_accuracy: 0.9671 - val_masked_accuracy_fn: 0.5643\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 29s 2s/step - loss: 0.1168 - accuracy: 0.9672 - masked_accuracy_fn: 0.6247 - val_loss: 0.1198 - val_accuracy: 0.9657 - val_masked_accuracy_fn: 0.6100\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 29s 2s/step - loss: 0.1145 - accuracy: 0.9675 - masked_accuracy_fn: 0.6273 - val_loss: 0.1155 - val_accuracy: 0.9663 - val_masked_accuracy_fn: 0.6527\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 29s 2s/step - loss: 0.1114 - accuracy: 0.9680 - masked_accuracy_fn: 0.6342 - val_loss: 0.0981 - val_accuracy: 0.9719 - val_masked_accuracy_fn: 0.7143\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 29s 2s/step - loss: 0.1061 - accuracy: 0.9697 - masked_accuracy_fn: 0.6536 - val_loss: 0.1055 - val_accuracy: 0.9690 - val_masked_accuracy_fn: 0.6714\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 29s 2s/step - loss: 0.1078 - accuracy: 0.9687 - masked_accuracy_fn: 0.6454 - val_loss: 0.0954 - val_accuracy: 0.9721 - val_masked_accuracy_fn: 0.6799\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 29s 2s/step - loss: 0.1040 - accuracy: 0.9699 - masked_accuracy_fn: 0.6601 - val_loss: 0.0919 - val_accuracy: 0.9737 - val_masked_accuracy_fn: 0.7121\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 29s 2s/step - loss: 0.1002 - accuracy: 0.9707 - masked_accuracy_fn: 0.6715 - val_loss: 0.0934 - val_accuracy: 0.9728 - val_masked_accuracy_fn: 0.7285\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 30s 2s/step - loss: 0.0968 - accuracy: 0.9717 - masked_accuracy_fn: 0.6765 - val_loss: 0.0946 - val_accuracy: 0.9727 - val_masked_accuracy_fn: 0.6918\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 30s 2s/step - loss: 0.0959 - accuracy: 0.9721 - masked_accuracy_fn: 0.6868 - val_loss: 0.0777 - val_accuracy: 0.9770 - val_masked_accuracy_fn: 0.6755\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 30s 2s/step - loss: 0.0919 - accuracy: 0.9733 - masked_accuracy_fn: 0.6960 - val_loss: 0.0935 - val_accuracy: 0.9724 - val_masked_accuracy_fn: 0.6743\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 29s 2s/step - loss: 0.0909 - accuracy: 0.9733 - masked_accuracy_fn: 0.6975 - val_loss: 0.0962 - val_accuracy: 0.9718 - val_masked_accuracy_fn: 0.6935\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 30s 2s/step - loss: 0.0890 - accuracy: 0.9741 - masked_accuracy_fn: 0.7071 - val_loss: 0.0783 - val_accuracy: 0.9774 - val_masked_accuracy_fn: 0.8209\n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 30s 2s/step - loss: 0.0878 - accuracy: 0.9745 - masked_accuracy_fn: 0.7080 - val_loss: 0.0771 - val_accuracy: 0.9776 - val_masked_accuracy_fn: 0.7654\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 29s 2s/step - loss: 0.0868 - accuracy: 0.9747 - masked_accuracy_fn: 0.7146 - val_loss: 0.0733 - val_accuracy: 0.9787 - val_masked_accuracy_fn: 0.7800\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 29s 2s/step - loss: 0.0833 - accuracy: 0.9756 - masked_accuracy_fn: 0.7236 - val_loss: 0.0779 - val_accuracy: 0.9771 - val_masked_accuracy_fn: 0.7911\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 30s 2s/step - loss: 0.0826 - accuracy: 0.9758 - masked_accuracy_fn: 0.7271 - val_loss: 0.0773 - val_accuracy: 0.9774 - val_masked_accuracy_fn: 0.7528\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 29s 2s/step - loss: 0.0787 - accuracy: 0.9770 - masked_accuracy_fn: 0.7369 - val_loss: 0.0786 - val_accuracy: 0.9762 - val_masked_accuracy_fn: 0.7412\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 30s 2s/step - loss: 0.0783 - accuracy: 0.9769 - masked_accuracy_fn: 0.7393 - val_loss: 0.0747 - val_accuracy: 0.9786 - val_masked_accuracy_fn: 0.8148\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 30s 2s/step - loss: 0.0761 - accuracy: 0.9775 - masked_accuracy_fn: 0.7435 - val_loss: 0.0713 - val_accuracy: 0.9791 - val_masked_accuracy_fn: 0.7681\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 29s 2s/step - loss: 0.0760 - accuracy: 0.9776 - masked_accuracy_fn: 0.7474 - val_loss: 0.0767 - val_accuracy: 0.9777 - val_masked_accuracy_fn: 0.7604\n",
      "Epoch 47/50\n",
      "19/19 [==============================] - 30s 2s/step - loss: 0.0752 - accuracy: 0.9778 - masked_accuracy_fn: 0.7511 - val_loss: 0.0630 - val_accuracy: 0.9811 - val_masked_accuracy_fn: 0.8236\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 30s 2s/step - loss: 0.0722 - accuracy: 0.9785 - masked_accuracy_fn: 0.7561 - val_loss: 0.0658 - val_accuracy: 0.9807 - val_masked_accuracy_fn: 0.7879\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 30s 2s/step - loss: 0.0688 - accuracy: 0.9798 - masked_accuracy_fn: 0.7700 - val_loss: 0.0596 - val_accuracy: 0.9829 - val_masked_accuracy_fn: 0.7808\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 30s 2s/step - loss: 0.0692 - accuracy: 0.9796 - masked_accuracy_fn: 0.7690 - val_loss: 0.0611 - val_accuracy: 0.9820 - val_masked_accuracy_fn: 0.7409\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "data_dir = './model/RNN_n_n_GRU'\n",
    "logs_dir = './logs/RNN_n_n_GRU'\n",
    "\n",
    "best_model_file = os.path.join(data_dir, 'best_model.h5')\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    best_model_file,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True)\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=logs_dir)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[checkpoint, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 4s 382ms/step - loss: 0.0637 - accuracy: 0.9811 - masked_accuracy_fn: 0.7906\n",
      "test loss: 0.064, test accuracy: 0.981, masked test accuracy: 0.791\n"
     ]
    }
   ],
   "source": [
    "# evaluate with test set\n",
    "best_model = POSTaggingModel(source_vocab_size, target_vocab_size,\n",
    "    embedding_dim, max_seqlen, rnn_output_dim)\n",
    "best_model.build(input_shape=(batch_size, max_seqlen))\n",
    "best_model.load_weights(best_model_file)\n",
    "best_model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=\"adam\", \n",
    "    metrics=[\"accuracy\", masked_accuracy()])\n",
    "\n",
    "test_loss, test_acc, test_masked_acc = best_model.evaluate(test_dataset)\n",
    "print(\"test loss: {:.3f}, test accuracy: {:.3f}, masked test accuracy: {:.3f}\".format(\n",
    "    test_loss, test_acc, test_masked_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try/NN a/DT form/NN asking/VBG them/PRP 2/NONE to/TO check/VB one/CD of/IN three/CD answers/NNS 1/LS rrb/RRB no/RB the/DT wine/NN is/VBZ too/RB high/JJ lrb/LRB 2/CD responses/NNS rrb/RRB 2/LS rrb/RRB yes/RB it/PRP 's/VBZ high/JJ but/CC i/PRP 'll/MD take/VB it/PRP lrb/LRB 2/CD responses/NNS rrb/RRB 3/LS rrb/RRB i/PRP 'll/MD take/VB all/DT 0/NONE i/PRP can/MD get/VB t/NONE 3/LRB lrb/CD 58/NNS responses/RRB\n",
      "predicted: 1/NONE offering/VBG the/DT wine/NN at/IN roughly/IN 65/CD u/NONE a/DT bottle/NN wholesale/NN lrb/LRB 100/CD u/NONE retail/NN rrb/CC he/PRP sent/VBD merchants/NNS around/IN the/DT country/NN a/DT form/NN asking/VBG them/PRP 2/NONE to/TO check/VB one/CD of/IN three/CD answers/NNS 1/NONE rrb/VBP no/DT the/DT wine/NN is/VBZ too/RB high/JJ lrb/LRB 2/CD responses/NNS rrb/RRB 2/WP rrb/RB yes/RB it/PRP 's/POS high/JJ but/CC i/PRP 'll/MD take/VB it/PRP lrb/LRB 2/CD responses/NNS rrb/WP 3/WP rrb/RRB i/PRP 'll/MD take/VB all/VB 0/NONE i/PRP can/MD get/VB t/NONE 3/CD lrb/CD 58/CD responses/NNS\n",
      " \n",
      "labeled  : each/DT side/NN has/VBZ a/DT litany/NN of/IN recommendations/NNS for/IN the/DT other/JJ\n",
      "predicted: each/DT side/NN has/VBZ a/DT litany/NN of/IN recommendations/NNS for/IN the/DT other/JJ\n",
      " \n",
      "labeled  : mr/NNP ross/NNP said/VBD 0/NONE he/PRP met/VBD with/IN officials/NNS of/IN the/DT irs/NNP and/CC the/DT justice/NNP department/NNP which/WDT t/NONE 133/MD would/VB bring/DT any/NN enforcement/NNS actions/IN against/NNS taxpayers/NONE 1/TO to/VB discuss/DT the/NN issue/JJ last/NNP\n",
      "predicted: mr/NNP ross/NNP said/VBD 0/NONE he/PRP met/VBD with/IN officials/NNS of/IN the/DT irs/NNP and/CC the/DT justice/NNP department/NNP which/WDT t/NONE 133/MD would/VB bring/DT any/NN enforcement/NNS actions/IN against/NNS taxpayers/NONE 1/TO to/VB discuss/DT the/NN issue/NN last/NN may/VB\n",
      " \n",
      "labeled  : the/DT proposal/NN comes/VBZ as/IN a/DT surprise/NN even/RB to/TO administration/NN officials/NNS and/CC temporarily/RB throws/VBZ into/IN UNK/NN the/DT house/NNP 's/POS work/NN on/IN clean/JJ air/NN\n",
      "predicted: the/DT proposal/NN comes/VBZ as/IN a/DT surprise/NN even/RB to/TO administration/NN officials/NNS and/CC temporarily/RB throws/VBZ into/IN UNK/NN the/DT house/NNP 's/POS work/NN on/IN clean/JJ air/NN legislation/NN\n",
      " \n",
      "labeled  : it/PRP exp/NONE 1/VBZ is/JJ clear/IN that/JJS most/RB mentally/JJ ill/NNS people/CC and/JJS most/NNS alcoholics/VBP do/RB not/VB become/JJ\n",
      "predicted: it/PRP exp/NONE 1/VBZ is/JJ clear/RB that/RB most/RB mentally/JJ ill/NNS people/CC and/NNS most/NNS alcoholics/VBP do/RB not/RB become/JJ\n",
      " \n",
      "labeled  : also/RB a/DT communist/NNP official/NN for/IN the/DT first/JJ time/NN said/VBD 0/NONE the/DT future/NN of/IN the/DT berlin/NNP wall/NNP could/MD be/VB open/JJ to/TO discussion/NN\n",
      "predicted: also/RB a/DT communist/NNP official/NN for/IN the/DT first/JJ time/NN said/VBD 0/NONE the/DT future/NN of/IN the/DT berlin/NNP wall/NNP could/MD be/VB open/VB to/TO discussion/NN\n",
      " \n",
      "labeled  : what/WP if/IN i/PRP tune/VBP in/RP my/PRP short/JJ wave/NN radio/VBP transcribe/DT an/NN editorial/CC or/NN program/CC and/VBP print/PRP it/IN in/PRP my/NN\n",
      "predicted: what/WP if/IN i/PRP tune/VBP in/PRP my/PRP short/JJ wave/NN radio/VBP transcribe/DT an/NN editorial/CC or/CC program/NN and/NN print/PRP it/PRP in/PRP my/NN\n",
      " \n",
      "labeled  : earlier/RBR this/DT year/NN shaw/NNP publishing/NNP inc/NNP charlotte/NNP acquired/VBD 30/CD of/NN american/IN city/NNP t/NNP 1/NONE and/CC has/VBZ an/DT agreement/NN to/NONE acquire/TO a/VB further/DT 25/JJ from/CD e/NN w/IN UNK/NNP co/NNP next/NNP year/JJ PAD/NN\n",
      "predicted: earlier/JJR this/DT year/NN shaw/NNP publishing/NNP inc/NNP charlotte/NNP acquired/VBD 30/CD of/IN american/NNP city/NNP t/NNP 1/CC and/VBZ has/DT an/NN agreement/NN to/TO acquire/VB a/VB further/JJ 25/NN from/IN e/NNP w/NNP UNK/NNP co/CC next/JJ year/NN\n",
      " \n",
      "labeled  : many/JJ auto/NN dealers/NNS now/RB let/VBP car/NN buyers/NNS charge/VB part/NN or/CC all/DT of/IN their/PRP purchase/NN on/IN the/DT american/NNP express/NNP card/NN but/CC few/JJ card/NN holders/NNS realize/VBP this/DT mr/NNP riese/NNP says/VBZ 0/NONE t/NONE\n",
      "predicted: many/JJ auto/NN dealers/NNS now/RB let/VBP car/NN buyers/NNS charge/NN part/NN or/CC all/DT of/IN their/PRP purchase/NN on/IN the/DT american/NNP express/NNP card/NN but/CC few/JJ card/NN holders/NNS realize/VBP this/DT mr/NNP riese/NNP says/VBZ 0/NONE t/NONE\n",
      " \n",
      "labeled  : among/IN other/JJ things/NNS the/DT survey/NN found/VBD that/IN manufacturing/NN activity/NN UNK/VBD considerably/RB across/IN districts/NNS and/CC among/IN industries/NNS\n",
      "predicted: among/IN other/JJ things/NNS the/DT survey/NN found/VBD that/IN manufacturing/NN activity/NN UNK/VBD considerably/RB across/IN districts/NNS and/CC among/IN industries/NNS\n",
      " \n",
      "labeled  : at/IN st/NNP louis/NNP the/DT water/NN level/NN of/IN the/DT mississippi/NNP river/NNP is/VBZ already/RB 6/CD 5/NNS feet/IN below/JJ normal/CC and/PRP it/MD could/VB drop/DT an/JJ additional/CD 2/NNS 5/WRB feet/DT when/NN the/IN flow/DT of/NNP the/NNP missouri/VBZ river/VBN is/NONE UNK/NONE 1/DT t/NNP 2/NNP an/NN army/VBD corps/NONE spokesman/NONE\n",
      "predicted: at/IN st/NNP louis/NNP the/DT water/NN level/NN of/IN the/DT mississippi/NNP river/NNP is/VBZ already/RB 6/CD 5/CD feet/IN below/NN normal/CC and/CC it/MD could/VB drop/DT an/JJ additional/JJ 2/CD 5/NNS feet/DT when/DT the/NN flow/IN of/DT the/NNP missouri/NNP river/VBZ is/NONE UNK/NONE 1/NONE t/DT 2/DT an/NNP army/VBD corps/VBD spokesman/NONE said/NONE\n",
      " \n",
      "labeled  : the/DT purpose/NN of/IN the/DT bill/NN is/VBZ to/NONE put/TO the/VB brakes/DT on/NNS airline/IN acquisitions/NN that/NNS t/WDT 2/NONE would/MD so/RB load/VB a/DT carrier/NN up/RP with/IN debt/NN that/IN it/PRP would/MD impede/VB safety/NN or/CC a/DT carrier/NN 's/POS ability/NN to/NONE compete/TO ''/VB rep/'' john/NNP paul/NNP hammerschmidt/NNP lrb/NNP r/LRB ark/NNP rrb/NNP said/RRB t/VBD 1/NONE\n",
      "predicted: the/DT purpose/NN of/IN the/DT bill/NN is/VBZ to/NONE put/TO the/VB brakes/DT on/IN airline/IN acquisitions/NNS that/WDT t/NONE 2/NONE would/MD so/RB load/VB a/DT carrier/NN up/IN with/IN debt/NN that/IN it/PRP would/MD impede/VB safety/NN or/CC a/DT carrier/NN 's/POS ability/NN to/NONE compete/TO ''/VB rep/NNP john/NNP paul/NNP hammerschmidt/NNP lrb/LRB r/NNP ark/NNP rrb/NNP said/VBD t/NONE\n",
      " \n",
      "labeled  : the/DT herald/NNP 's/POS sports/NNS coverage/NN and/CC arts/NNS criticism/NN were/VBD also/RB highly/RB regarded/VBN\n",
      "predicted: the/DT herald/NNP 's/POS sports/NNS coverage/NN and/CC arts/NNS criticism/NN were/VBD also/RB highly/RB regarded/VBN\n",
      " \n",
      "labeled  : the/DT average/NN of/IN interbank/NN offered/VBD rates/NNS for/IN dollar/NN deposits/NNS in/IN the/DT london/NNP market/NN based/VBN on/NONE quotations/IN at/NNS five/IN major/CD banks/JJ PAD/NNS\n",
      "predicted: the/DT average/NN of/IN interbank/NN offered/VBD rates/NNS for/IN dollar/NN deposits/NNS in/IN the/DT london/NNP market/NN based/VBN on/NONE quotations/IN at/IN five/JJ major/NNS\n",
      " \n",
      "labeled  : such/JJ laws/NNS violate/VBP the/DT provision/NN in/IN article/NNP ii/NNP that/WDT t/NONE 1/VBZ requires/DT the/NN president/TO to/VB make/NNS recommendations/TO to/NNP congress/CC but/WDT which/NONE t/VBZ 41/DT gives/NN the/DT president/NN the/NONE discretion/NONE 0/TO to/VB select/DT the/JJ subject/NN matter/IN of/DT those/NNS recommendations/NONE\n",
      "predicted: such/JJ laws/NNS violate/VBP the/DT provision/NN in/IN article/NNP ii/NNP that/WDT t/NONE 1/VBZ requires/DT the/NN president/TO to/TO make/NNS recommendations/TO to/VB congress/CC but/NONE which/NONE t/VBZ 41/VBZ gives/DT the/NN president/NN the/NONE discretion/NONE 0/TO to/VB select/DT the/NN subject/NN matter/IN of/DT those/NNS\n",
      " \n",
      "labeled  : president/NNP bush/NNP should/MD veto/VB appropriations/NNS acts/NNS that/WDT t/NONE 1/VBP contain/DT these/NNS kinds/IN of/JJ unconstitutional/NNS conditions/IN on/DT the/NN president/POS 's/NN ability/NONE to/TO discharge/VB his/PRP duties/NNS and/CC exercise/VB his/PRP prerogatives/NNS\n",
      "predicted: president/NNP bush/NNP should/MD veto/VB appropriations/NNS acts/NNS that/WDT t/NONE 1/NONE contain/DT these/NNS kinds/IN of/JJ unconstitutional/NNS conditions/IN on/DT the/NN president/NN 's/NN ability/NONE to/TO discharge/PRP his/PRP duties/CC and/CC exercise/PRP his/PRP prerogatives/NNS\n",
      " \n",
      "labeled  : that/DT way/NN investors/NNS can/MD essentially/RB buy/VB the/DT funds/NNS without/IN 1/NONE paying/VBG the/DT premium/NN\n",
      "predicted: that/IN way/NN investors/NNS can/MD essentially/RB buy/VB the/DT funds/NNS without/IN 1/NONE paying/VBG the/VBG premium/NN PAD/NN\n",
      " \n",
      "labeled  : that/DT represents/VBZ a/DT very/RB thin/JJ excess/JJ ''/'' return/NN certainly/RB far/RB less/JJR than/IN what/WP most/RBS fundamental/JJ stock/NN pickers/NNS claim/VBP 1/NONE to/TO seek/VB t/NONE 2/IN as/PRP their/NN performance/NN\n",
      "predicted: that/IN represents/VBZ a/DT very/RB thin/JJ excess/JJ ''/'' return/NN certainly/RB far/RB less/JJR than/IN what/WP most/JJS fundamental/JJ stock/NN pickers/NNS claim/VBP 1/NONE to/TO seek/VB t/NONE 2/IN as/PRP their/PRP performance/NN\n",
      " \n",
      "labeled  : at/IN the/DT same/JJ time/NN dealers/NNS said/VBD 0/NONE the/DT u/NNP s/NN unit/VBZ has/VBN been/VBN locked/NONE 1/IN into/DT a/RB relatively/JJ narrow/NN range/IN in/JJ recent/NNS weeks/IN in/NN part/IN because/DT the/JJ hefty/JJ japanese/NN demand/IN for/NNS dollars/VBZ has/VBN been/VBN offset/NONE 2/IN by/DT the/NN mark/POS 's/NN strength/NONE resulting/VBG in/IN a/DT stalemate/NN\n",
      "predicted: at/IN the/DT same/JJ time/NN dealers/NNS said/VBD 0/NONE the/DT u/NNP s/VBZ unit/VBZ has/VBN been/VBN locked/NONE 1/IN into/DT a/RB relatively/JJ narrow/NN range/IN in/JJ recent/NNS weeks/IN in/NN part/IN because/DT the/JJ hefty/JJ japanese/NN demand/IN for/NNS dollars/VBZ has/VBN been/VBN offset/VBN 2/IN by/DT the/NN mark/NN 's/NN strength/NN resulting/IN in/DT a/NN stalemate/NN\n",
      " \n",
      "labeled  : there/EX have/VBP been/VBN only/RB seven/CD other/JJ times/NNS in/NN 1929/CD 1933/CD 1961/CD 1965/CD 1968/CD 1971/CD and/CC 1972/CD when/WRB the/DT yield/NN on/IN the/DT s/NNP p/CD 500/VBD dropped/IN below/CD 3/NN for/IN at/IN least/JJS two/CD consecutive/JJ months/NNS t/NONE 1/NNP mr/NNP perritt/VBD found/NONE 0/NONE\n",
      "predicted: there/EX have/VBP been/VBN only/JJ seven/JJ other/JJ times/NNS in/IN 1929/CD 1933/CD 1961/CD 1965/CD 1968/CD 1971/CD and/CC 1972/CD when/WRB the/DT yield/NN on/IN the/DT s/NNP p/CD 500/CD dropped/IN below/CD 3/IN for/IN at/IN least/CD two/CD consecutive/JJ months/NNS t/NONE 1/NNP mr/NNP perritt/VBD found/NONE 0/NONE\n",
      " \n",
      "labeled  : 2/NONE used/VBN 1/NONE by/IN program/NN traders/NNS and/CC others/NNS to/NONE UNK/TO orders/VB into/NNS the/IN exchange/DT superdot/NN UNK/NNP about/VBZ 80/IN of/CD all/NN orders/IN entered/DT at/NNS the/VBN exchange/NONE PAD/IN PAD/DT PAD/NN\n",
      "predicted: 2/NONE used/VBN 1/NONE by/IN program/NN traders/NNS and/CC others/NNS to/TO UNK/NNS orders/NNS into/IN the/DT exchange/NNP superdot/NN UNK/IN about/IN 80/IN of/DT all/NNS orders/NNS entered/IN at/DT the/NN\n",
      " \n",
      "labeled  : according/VBG to/TO ms/NNP poore/NNP old/NNP house/NNP journal/NNP corp/PRP her/NN publishing/NN company/VBD printed/CC and/VBD sold/DT all/CD 126/NNS 000/IN copies/DT of/NN the/NN\n",
      "predicted: according/VBG to/TO ms/NNP poore/NNP old/NNP house/NNP journal/NNP corp/NNP her/NN publishing/NN company/VBD printed/CC and/VBD sold/DT all/NNS 126/NNS 000/DT copies/DT of/DT the/NN\n",
      " \n",
      "labeled  : komatsu/NNP predicted/VBD that/IN for/IN the/DT fiscal/JJ year/NN ending/VBG march/NNP 31/CD sales/NNS will/MD UNK/VB to/TO 600/CD billion/CD yen/NN from/IN UNK/CD 54/CD billion/NN yen/NN pretax/NN profit/VBD was/VBN forecast/NONE 1/IN at/CD 35/CD billion/NN yen/RB up/IN from/CD 28/CD 53/NN billion/IN yen/JJ in/CD\n",
      "predicted: komatsu/NNP predicted/VBD that/IN for/IN the/DT fiscal/JJ year/NN ending/VBG march/NNP 31/VBD sales/NNS will/MD UNK/VB to/TO 600/CD billion/CD yen/NN from/IN UNK/CD 54/CD billion/CD yen/NN pretax/NN profit/NN was/VBD forecast/NONE 1/NONE at/IN 35/CD billion/CD yen/IN up/IN from/CD 28/CD 53/CD billion/NN yen/IN in/JJ fiscal/CD\n",
      " \n",
      "labeled  : officials/NNS from/IN the/DT various/JJ banks/NNS involved/VBN are/NONE expected/VBP 1/VBN to/NONE meet/TO during/VB the/IN next/DT few/JJ days/JJ 2/NNS to/NONE consider/TO other/VB arrangements/JJ with/NNS local/IN authorities/JJ that/NNS t/WDT 3/NONE could/MD be/VB questionable/JJ\n",
      "predicted: officials/NNS from/IN the/DT various/JJ banks/NNS involved/VBN are/VBP expected/VBN 1/NONE to/TO meet/VB during/IN the/DT next/JJ few/JJ days/NNS 2/NNS to/TO consider/TO other/VB arrangements/JJ with/NNS local/IN authorities/NNS that/WDT t/NONE 3/MD could/MD be/VB questionable/JJ\n",
      " \n",
      "labeled  : named/VBN 2/NONE as/IN defendants/NNS were/VBD t/NONE 1/NNP UNK/NNP UNK/NN president/CC and/NNP peter/NNP UNK/JJ executive/NN vice/NN president/CC and/NN chief/JJ financial/NN officer/RB as/RB well/IN as/DT a/JJ former/NN plant/NN\n",
      "predicted: named/VBN 2/NONE as/IN defendants/NNS were/VBD t/NONE 1/NONE UNK/NNP UNK/NNP president/CC and/NNP peter/NNP UNK/JJ executive/NN vice/NN president/CC and/NN chief/JJ financial/NN officer/RB as/RB well/RB as/DT a/JJ former/NN plant/NN\n",
      " \n",
      "labeled  : in/IN the/DT torrent/NN of/IN replies/NNS that/IN t/NONE 1/VBD followed/CD one/NN woman/NN ringer/IN from/NNP solihull/VBN observed/IN that/DT the/JJ average/JJ male/NN ringer/VBZ leaves/RB quite/DT a/NN lot/NONE 0/NONE t/TO 2/VB to/VBN be/NONE desired/NONE 141/RB 3/VBN badly/VBN dressed/IN decorated/NN with/CC acne/DT and/JJ a/NN large/RB beer/JJ belly/CC frequently/RB unwashed/JJ and/IN unbearably/NNS flatulent/''\n",
      "predicted: in/IN the/DT torrent/NN of/IN replies/NNS that/WDT t/NONE 1/VBD followed/CD one/CD woman/NN ringer/IN from/NNP solihull/VBN observed/IN that/DT the/JJ average/JJ male/NN ringer/VBZ leaves/RB quite/DT a/NN lot/NONE 0/NONE t/NONE 2/TO to/VB be/NONE desired/NONE 141/NONE 3/RB badly/VBN dressed/IN decorated/NN with/NN acne/CC and/DT a/JJ large/RB beer/JJ belly/RB frequently/RB unwashed/JJ and/NNS unbearably/NNS\n",
      " \n",
      "labeled  : all/DT of/IN the/DT changes/NNS require/VBP regulatory/JJ approval/NN which/WDT t/NONE 1/VBZ is/VBN expected/NONE 129/RB\n",
      "predicted: all/DT of/IN the/DT changes/NNS require/VBP regulatory/JJ approval/NN which/WDT t/NONE 1/VBZ is/VBN expected/VBN 129/RB\n",
      " \n",
      "labeled  : western/NNP gas/NNP resources/NNPS inc/NNP initial/JJ offering/NN of/IN 3/CD 250/NNS 000/IN shares/JJ of/NN common/IN stock/WDT of/CD which/NNS 3/NONE UNK/MD 000/VB shares/VBN t/NONE 2/IN will/DT be/NN sold/CC 1/CD by/NNS the/NONE company/IN and/DT 210/NN 000/IN shares/NNP t/NNP 2/NNP by/NNP a/NNP holder/NNP via/NNP prudential/CC bache/NNP capital/CC funding/NNP smith/NNP barney/NNP\n",
      "predicted: western/NNP gas/NNP resources/NNPS inc/NNP initial/JJ offering/NN of/IN 3/CD 250/CD 000/NNS shares/IN of/JJ common/NN stock/IN of/CD which/NONE 3/CD UNK/NNS 000/NNS shares/NONE t/MD 2/MD will/VB be/VBN sold/NONE 1/IN by/DT the/NN company/NN and/NN 210/NNS 000/NNS shares/NONE t/NONE 2/IN by/NNP a/NNP holder/NNP via/NNP prudential/NNP bache/NNP capital/NNP funding/NNP smith/NNP barney/NNP harris/NNP upham/CC co/CC and/NNP\n",
      " \n",
      "labeled  : every/DT day/NN 0/NONE you/PRP delay/VBP t/NONE 1/DT a/NN savings/NN institution/POS 's/NN health/CC and/DT the/JJ federal/NN budget/NN deficit/VBZ grows/JJR\n",
      "predicted: every/DT day/NN 0/NONE you/PRP delay/VBP t/NONE 1/DT a/NN savings/POS institution/POS 's/POS health/NN and/DT the/NN federal/NN budget/NN deficit/NN grows/VBZ\n",
      " \n",
      "labeled  : to/TO that/DT end/NN american/NNP express/NNP has/VBZ been/VBN signing/VBG up/RP gasoline/NN companies/NNS car/NN repair/NN shops/NNS tire/NN companies/NNS and/CC car/NN dealers/NNS 1/NONE to/TO accept/VB the/DT card/NN\n",
      "predicted: to/TO that/IN end/NN american/NNP express/NNP has/VBZ been/VBN signing/VBG up/RP gasoline/NN companies/NNS car/NN repair/NN shops/NNS tire/NN companies/NNS and/CC car/NN dealers/NNS 1/NONE to/TO accept/VB the/DT card/NN\n",
      " \n",
      "labeled  : the/DT training/NN wage/NN covers/VBZ only/JJ workers/NNS who/WP t/NONE 260/VBP are/CD 16/TO to/CD 19/NNS years/JJ\n",
      "predicted: the/DT training/NN wage/NN covers/VBZ only/RB workers/NNS who/WP t/NONE 260/VBP are/VBP 16/TO to/TO 19/CD\n",
      " \n",
      "labeled  : besides/IN the/DT designer/NN 's/POS age/NN other/JJ risk/NN factors/NNS for/IN mr/NNP cray/NNP 's/POS new/JJ company/NN include/VBP the/DT cray/NNP 3/POS 's/JJ tricky/JJ unproven/NN chip/NN\n",
      "predicted: besides/IN the/DT designer/NN 's/POS age/NN other/JJ risk/NN factors/NNS for/IN mr/NNP cray/NNP 's/POS new/JJ company/NN include/VBP the/DT cray/NNP 3/POS 's/JJ tricky/JJ unproven/NN chip/NN\n",
      " \n",
      "labeled  : he/PRP and/CC others/NNS prefer/VBP 1/NONE to/TO install/VB railings/NNS such/JJ as/IN the/DT type/NN f/NN safety/NN shape/NN ''/'' a/DT four/JJ foot/JJ high/NN concrete/IN slab/DT with/NNS\n",
      "predicted: he/PRP and/CC others/NNS prefer/VBP 1/NONE to/TO install/VB railings/NNS such/JJ as/IN the/DT type/NN f/NN safety/NN shape/NN ''/'' a/DT four/JJ foot/JJ high/JJ concrete/IN slab/DT with/DT\n",
      " \n",
      "labeled  : the/DT 30/JJ day/JJ simple/NN yield/VBD fell/TO to/DT an/JJ average/CD 8/NN 19/IN from/CD 8/NN 22/DT the/JJ 30/NN day/NN compound/VBD yield/TO slid/DT to/JJ an/CD average/NN 8/IN 53/CD from/NN\n",
      "predicted: the/DT 30/JJ day/NN simple/NN yield/TO fell/TO to/DT an/JJ average/NN 8/CD 19/CD from/CD 8/CD 22/DT the/JJ 30/JJ day/NN compound/NN yield/TO slid/TO to/DT an/JJ average/NN 8/CD 53/IN from/CD\n",
      " \n",
      "labeled  : similar/JJ levels/NNS UNK/VBP barge/NN shipments/NNS last/JJ year/NN in/IN the/DT wake/NN of/IN the/DT worst/JJS drought/NN in/IN 50/CD years/NNS\n",
      "predicted: similar/JJ levels/NNS UNK/NNS barge/NN shipments/NNS last/JJ year/NN in/IN the/DT wake/NN of/IN the/DT worst/JJS drought/NN in/IN 50/CD years/NNS\n",
      " \n",
      "labeled  : corporate/JJ executives/NNS resent/VBP that/IN their/PRP company/NN 's/POS stock/NN has/VBZ been/VBN transformed/VBN 75/NONE into/IN a/DT nameless/JJ piece/NN of/IN a/DT stock/NN index/NN\n",
      "predicted: corporate/JJ executives/NNS resent/VBP that/IN their/PRP company/NN 's/POS stock/NN has/VBZ been/VBN transformed/VBN 75/NONE into/IN a/DT nameless/JJ piece/NN of/IN a/DT stock/NN index/NN\n",
      " \n",
      "labeled  : but/CC learning/NNP materials/NNPS matched/VBD on/IN 66/CD 5/IN of/CD 69/NNS\n",
      "predicted: but/CC learning/NNP materials/NNPS matched/VBD on/IN 66/CD 5/CD of/NNS 69/NNS\n",
      " \n",
      "labeled  : however/RB five/CD other/JJ countries/NNS china/NNP thailand/NNP india/NNP brazil/NNP and/CC mexico/NNP will/MD remain/VB on/IN that/DT so/JJ called/NN priority/NN watch/NN list/IN as/DT a/NN result/IN of/DT an/JJ interim/NN review/NNP u/NNP s/NNP trade/NNP representative/NNP carla/VBD hills/NONE announced/NONE\n",
      "predicted: however/RB five/JJ other/JJ countries/NNS china/NNP thailand/NNP india/NNP brazil/NNP and/CC mexico/NNP will/MD remain/VB on/IN that/IN so/JJ called/NN priority/NN watch/NN list/IN as/DT a/NN result/IN of/DT an/JJ interim/NN review/NNP u/NNP s/NNP trade/NNP representative/NNP carla/VBD hills/NONE announced/NONE 0/NONE\n",
      " \n",
      "labeled  : yesterday/NN these/DT stocks/NNS rose/VBD by/IN 170/CD UNK/NNS ounces/TO to/DT a/NN record/IN of/CD 226/NNS 570/VBG 380/TO ounces/DT according/NN to/NN\n",
      "predicted: yesterday/NN these/DT stocks/NNS rose/VBD by/IN 170/CD UNK/NNS ounces/TO to/DT a/NN record/IN of/CD 226/NNS 570/NNS 380/TO ounces/TO according/DT to/DT\n",
      " \n",
      "labeled  : western/NNP union/NNP indeed/RB wanted/VBD 1/NONE to/TO get/VB into/IN the/DT telephone/NN business/NN\n",
      "predicted: western/NNP union/NNP indeed/RB wanted/VBD 1/NONE to/TO get/VB into/IN the/DT telephone/NN business/NN\n",
      " \n",
      "labeled  : hudson/NNP general/NNP corp/NNP 's/POS president/NN and/CC chief/JJ executive/NN officer/NN alan/NNP j/NNP stearn/NNP resigned/VBD\n",
      "predicted: hudson/NNP general/NNP corp/NNP 's/POS president/NN and/CC chief/NN executive/NN officer/NN alan/NNP j/NNP stearn/NNP resigned/VBD\n",
      " \n",
      "labeled  : south/NNP carolina/NNP 's/POS reforms/NNS were/VBD designed/VBN 1/NONE for/IN schools/NNS like/IN greenville/NNP high/NNP school/NNP\n",
      "predicted: south/NNP carolina/NNP 's/POS reforms/NNS were/VBD designed/VBN 1/NONE for/IN schools/NNS like/IN greenville/NNP high/NNP school/NNP\n",
      " \n",
      "pos tagging accuracy: 0.980\n"
     ]
    }
   ],
   "source": [
    "# predict on batches\n",
    "labels, predictions = [], []\n",
    "is_first_batch = True\n",
    "accuracies = []\n",
    "\n",
    "for test_batch in test_dataset:\n",
    "    inputs_b, outputs_b = test_batch\n",
    "    preds_b = best_model.predict(inputs_b)\n",
    "    # convert from categorical to list of ints\n",
    "    preds_b = np.argmax(preds_b, axis=-1)\n",
    "    outputs_b = np.argmax(outputs_b.numpy(), axis=-1)\n",
    "    for i, (pred_l, output_l) in enumerate(zip(preds_b, outputs_b)):\n",
    "        assert(len(pred_l) == len(output_l))\n",
    "        pad_len = np.nonzero(output_l)[0][0]\n",
    "        acc = np.count_nonzero(\n",
    "            np.equal(\n",
    "                output_l[pad_len:], pred_l[pad_len:]\n",
    "            )\n",
    "        ) / len(output_l[pad_len:])\n",
    "        accuracies.append(acc)\n",
    "        if is_first_batch:\n",
    "            words = [idx2word_s[x] for x in inputs_b.numpy()[i][pad_len:]]\n",
    "            postags_l = [idx2word_t[x] for x in output_l[pad_len:] if x > 0]\n",
    "            postags_p = [idx2word_t[x] for x in pred_l[pad_len:] if x > 0]\n",
    "            print(\"labeled  : {:s}\".format(\" \".join([\"{:s}/{:s}\".format(w, p) \n",
    "                for (w, p) in zip(words, postags_l)])))\n",
    "            print(\"predicted: {:s}\".format(\" \".join([\"{:s}/{:s}\".format(w, p) \n",
    "                for (w, p) in zip(words, postags_p)])))\n",
    "            print(\" \")\n",
    "    is_first_batch = False\n",
    "\n",
    "accuracy_score = np.mean(np.array(accuracies))\n",
    "print(\"pos tagging accuracy: {:.3f}\".format(accuracy_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기계 변역용 어텐션 없는 seq2seq(인코더-디코너 아키텍쳐) 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, shutil, os, unicodedata\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 상수 정의\n",
    "NUM_SENT_PAIRS = 30000\n",
    "EMBEDDING_DIM = 256\n",
    "ENCODER_DIM, DECODER_DIM = 1024, 1024\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 250\n",
    "\n",
    "# 문자를 아스키화, 인접한 문장에서 특정 문장 부호를 분리,\n",
    "# 알파벳이 아니거나 이러한 특정 문장 부호 이외의 모든 문자를 제거하고,\n",
    "# 마지막으로 문장을 소문자로 변환해주는 함수\n",
    "def preprocess_sentence(sent):\n",
    "    sent = \"\".join([c for c in unicodedata.normalize(\"NFD\", sent) if unicodedata.category(c) != \"Mn\"])\n",
    "    sent = re.sub(r\"([!.?])\", r\" \\1\", sent)\n",
    "    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
    "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
    "    sent = sent.lower()\n",
    "    return sent\n",
    "\n",
    "# 각 문장을 하나의 단어 시퀀스로 변환, 프랑스어 문장은 두 개의 시퀀스로 변환해주는 함수\n",
    "def download_and_read():\n",
    "    en_sents, fr_sents_in, fr_sents_out = [], [], []\n",
    "    local_file = os.path.join(\"datasets/RNN_n_n_seq2seq\", \"fra.txt\")\n",
    "    with open(local_file, \"r\") as fin:\n",
    "        for i, line in enumerate(fin):\n",
    "            #print(line)\n",
    "            #print(line.strip().split('\\t'))\n",
    "            #en_sent = line.strip().split('\\t')[0]\n",
    "            #fr_sent = line.strip().split('\\t')[1]\n",
    "            en_sent, fr_sent = line.strip().split('\\t')[0:2]\n",
    "            en_sent = [w for w in preprocess_sentence(en_sent).split()]\n",
    "            fr_sent = preprocess_sentence(fr_sent)\n",
    "            fr_sent_in = [w for w in (\"BOS \" + fr_sent).split()]\n",
    "            fr_sent_out = [w for w in (fr_sent + \" EOS\").split()]\n",
    "            en_sents.append(en_sent)\n",
    "            fr_sents_in.append(fr_sent_in)\n",
    "            fr_sents_out.append(fr_sent_out)\n",
    "            if i >= NUM_SENT_PAIRS - 1:\n",
    "                break\n",
    "    return en_sents, fr_sents_in, fr_sents_out\n",
    "\n",
    "sents_en, sents_fr_in, sents_fr_out = download_and_read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size (en): 4352, vocab size (fr): 7574\n",
      "seqlen (en): 8, seqlen (fr): 16\n"
     ]
    }
   ],
   "source": [
    "# 영어, 프랑스의 입력시퀀스를 토큰화\n",
    "tokenizer_en = tf.keras.preprocessing.text.Tokenizer(\n",
    "    filters=\"\", lower=False)\n",
    "tokenizer_en.fit_on_texts(sents_en)\n",
    "data_en = tokenizer_en.texts_to_sequences(sents_en)\n",
    "data_en = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    data_en, padding='post')\n",
    "\n",
    "tokenizer_fr = tf.keras.preprocessing.text.Tokenizer(\n",
    "    filters=\"\", lower=False)\n",
    "tokenizer_fr.fit_on_texts(sents_fr_in)\n",
    "tokenizer_fr.fit_on_texts(sents_fr_out)\n",
    "data_fr_in = tokenizer_fr.texts_to_sequences(sents_fr_in)\n",
    "data_fr_in = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    data_fr_in, padding='post')\n",
    "data_fr_out = tokenizer_fr.texts_to_sequences(sents_fr_out)\n",
    "data_fr_out = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    data_fr_out, padding='post')\n",
    "\n",
    "# 영어, 프랑스 어휘 사전만들기\n",
    "vocab_size_en = len(tokenizer_en.word_index)\n",
    "word2idx_en = tokenizer_en.word_index\n",
    "idx2word_en = {v:k for k, v in word2idx_en.items()}\n",
    "\n",
    "vocab_size_fr = len(tokenizer_fr.word_index)\n",
    "word2idx_fr = tokenizer_fr.word_index\n",
    "idx2word_fr = {v:k for k, v in word2idx_fr.items()}\n",
    "\n",
    "print('vocab size (en): {:d}, vocab size (fr): {:d}'.format(\n",
    "    vocab_size_en, vocab_size_fr))\n",
    "\n",
    "maxlen_en = data_en.shape[1]\n",
    "maxlen_fr = data_fr_out.shape[1]\n",
    "print('seqlen (en): {:d}, seqlen (fr): {:d}'.format(\n",
    "    maxlen_en, maxlen_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = BATCH_SIZE\n",
    "\n",
    "# 데이터를 텐서플로 데이터셋으로 변환\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data_en, data_fr_in, data_fr_out))\n",
    "# 데이터를 섞는다.\n",
    "dataset = dataset.shuffle(10000)\n",
    "# 훈련과 테스트 집합으로 분할\n",
    "test_size = NUM_SENT_PAIRS // 4\n",
    "test_dataset = dataset.take(test_size).batch(batch_size, drop_remainder=True)\n",
    "train_dataset = dataset.skip(test_size).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, num_timesteps, embedding_dim, encoder_dim, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size, embedding_dim, input_length=num_timesteps)\n",
    "        self.rnn = tf.keras.layers.GRU(\n",
    "            encoder_dim, return_sequences=False, return_state=True)\n",
    "    \n",
    "    def call(self, x, state):\n",
    "        x = self.embedding(x)\n",
    "        x, state = self.rnn(x, initial_state=state)\n",
    "        return x, state\n",
    "    \n",
    "    def init_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.encoder_dim))\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, num_timesteps, embedding_dim, decoder_dim, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size, embedding_dim, input_length=num_timesteps)\n",
    "        self.rnn = tf.keras.layers.GRU(\n",
    "            decoder_dim, return_sequences=True, return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "    \n",
    "    def call(self, x, state):\n",
    "        x = self.embedding(x)\n",
    "        x, state = self.rnn(x, state)\n",
    "        x = self.dense(x)\n",
    "        return x, state\n",
    "    \n",
    "embedding_dim = EMBEDDING_DIM\n",
    "encoder_dim, decoder_dim = ENCODER_DIM, DECODER_DIM\n",
    "\n",
    "encoder = Encoder(vocab_size_en+1, embedding_dim, maxlen_en, encoder_dim)\n",
    "decoder = Decoder(vocab_size_fr+1, embedding_dim, maxlen_fr, decoder_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder input          : (64, 8)\n",
      "encoder output         : (64, 1024) state: (64, 1024)\n",
      "decoder output (logits): (64, 16, 7575) state: (64, 1024)\n",
      "decoder output (labels): (64, 16)\n"
     ]
    }
   ],
   "source": [
    "for encoder_in, decoder_in, decoder_out in train_dataset:\n",
    "    encoder_state = encoder.init_state(batch_size)\n",
    "    encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n",
    "    decoder_state = encoder_state\n",
    "    decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\n",
    "    break\n",
    "\n",
    "print('encoder input          :', encoder_in.shape)\n",
    "print('encoder output         :', encoder_out.shape, 'state:', encoder_state.shape)\n",
    "print('decoder output (logits):', decoder_pred.shape, 'state:', decoder_state.shape)\n",
    "print('decoder output (labels):', decoder_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 함수\n",
    "# 문장을 pad_sequences() 함수로 채웠기 때문에 레이블과 예측 사이에서\n",
    "# 패드 단어의 동등성을 고려하므로 결과를 편향시키고 싶지는 않다.\n",
    "# 이 함수는 레이블을 사용해 예측을 마스킹, 레이블의 패딩된 위치도 예측에서 제거,\n",
    "# 레이블과 예측 모두에서 0이 아닌 요소만 사용해 손실을 계산하는 함수이다.\n",
    "def loss_fn(ytrue, ypred):\n",
    "    scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    mask = tf.math.logical_not(tf.math.equal(ytrue, 0))\n",
    "    mask = tf.cast(mask, dtype=tf.int64)\n",
    "    loss = scce(ytrue, ypred, sample_weight=mask)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(encoder_in, decoder_in, decoder_out, encoder_state):\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n",
    "        decoder_state = encoder_state\n",
    "        decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\n",
    "        loss = loss_fn(decoder_out, decoder_pred)\n",
    "    \n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(encoder, decoder, batch_size, sents_en, data_en, sents_fr_out, word2idx_fr, idx2word_fr):\n",
    "    random_id = np.random.choice(len(sents_en))\n",
    "    print(\"input      : \", \" \".join(sents_en[random_id]))\n",
    "    print(\"label      : \", \" \".join(sents_fr_out[random_id]))\n",
    "    encoder_in = tf.expand_dims(data_en[random_id], axis=0)\n",
    "    encoder_out = tf.expand_dims(sents_fr_out[random_id], axis=0)\n",
    "\n",
    "    encoder_state = encoder.init_state(1)\n",
    "    encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n",
    "    decoder_state = encoder_state\n",
    "    decoder_in = tf.expand_dims(\n",
    "        tf.constant([word2idx_fr['BOS']]), axis=0)\n",
    "    pred_sent_fr = []\n",
    "    while True:\n",
    "        decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\n",
    "        decoder_pred = tf.argmax(decoder_pred, axis=-1)\n",
    "        pred_word = idx2word_fr[decoder_pred.numpy()[0][0]]\n",
    "        pred_sent_fr.append(pred_word)\n",
    "        if pred_word == 'EOS':\n",
    "            break\n",
    "        decoder_in = decoder_pred\n",
    "    print('predicted  : ', \" \".join(pred_sent_fr))\n",
    "\n",
    "def evaluate_bleu_score(encoder, decoder, test_dataset, word2idx_fr, idx2word_fr):\n",
    "    bleu_scores = []\n",
    "    smooth_fn = SmoothingFunction()\n",
    "    for encoder_in, decoder_in, decoder_out in test_dataset:\n",
    "        encoder_state = encoder.init_state(batch_size)\n",
    "        encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n",
    "        decoder_state = encoder_state\n",
    "        decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\n",
    "\n",
    "        # argmax 계산\n",
    "        decoder_out = decoder_out.numpy()\n",
    "        decoder_pred = tf.argmax(decoder_pred, axis=-1).numpy()\n",
    "\n",
    "        for i in range(decoder_out.shape[0]):\n",
    "            ref_sent = [idx2word_fr[j] for j in decoder_out[i].tolist() if j > 0]\n",
    "            hyp_sent = [idx2word_fr[j] for j in decoder_pred[i].tolist() if j > 0]\n",
    "            # 뒷부분 EOS 제거\n",
    "            ref_sent = ref_sent[0:-1]\n",
    "            hyp_sent = hyp_sent[0:-1]\n",
    "            bleu_score = sentence_bleu([ref_sent], hyp_sent, smoothing_function=smooth_fn.method1)\n",
    "            bleu_scores.append(bleu_score)\n",
    "\n",
    "    return np.mean(np.array(bleu_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is it for me ?\n",
      "label      :  est ce que c est pour moi ? EOS\n",
      "predicted  :  est ce que c est pour moi ? EOS\n",
      "Eval Score (BLEU): 0.177\n",
      "Epoch: 129, Loss: 0.0721\n",
      "input      :  this is so boring .\n",
      "label      :  c est tellement chiant . EOS\n",
      "predicted  :  c est tellement chiant . EOS\n",
      "Eval Score (BLEU): 0.177\n",
      "Epoch: 130, Loss: 0.0813\n",
      "input      :  you can trust us .\n",
      "label      :  tu peux nous faire confiance . EOS\n",
      "predicted  :  vous pouvez nous faire confiance . EOS\n",
      "Eval Score (BLEU): 0.177\n",
      "Epoch: 131, Loss: 0.0968\n",
      "input      :  are you a policeman ?\n",
      "label      :  es tu un policier ? EOS\n",
      "predicted  :  es tu un policier ? EOS\n",
      "Eval Score (BLEU): 0.177\n",
      "Epoch: 132, Loss: 0.0815\n",
      "input      :  what a day !\n",
      "label      :  quelle journee ! EOS\n",
      "predicted  :  quelle journee ! EOS\n",
      "Eval Score (BLEU): 0.177\n",
      "Epoch: 133, Loss: 0.0776\n",
      "input      :  do you have a comb ?\n",
      "label      :  as tu un peigne ? EOS\n",
      "predicted  :  as tu un peigne ? EOS\n",
      "Eval Score (BLEU): 0.177\n",
      "Epoch: 134, Loss: 0.1016\n",
      "input      :  you re all done .\n",
      "label      :  vous etes toutes finies . EOS\n",
      "predicted  :  vous etes tous finis . EOS\n",
      "Eval Score (BLEU): 0.176\n",
      "Epoch: 135, Loss: 0.0627\n",
      "input      :  i don t steal .\n",
      "label      :  je ne vole pas . EOS\n",
      "predicted  :  je ne vole pas . EOS\n",
      "Eval Score (BLEU): 0.179\n",
      "Epoch: 136, Loss: 0.0834\n",
      "input      :  let s move the bed .\n",
      "label      :  bougeons le lit ! EOS\n",
      "predicted  :  bougeons le lit ! EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 137, Loss: 0.0845\n",
      "input      :  i read your report .\n",
      "label      :  j ai lu ton rapport . EOS\n",
      "predicted  :  j ai lu ton rapport . EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 138, Loss: 0.0761\n",
      "input      :  those are good .\n",
      "label      :  ceux ci sont bons . EOS\n",
      "predicted  :  celles ci sont bonnes . EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 139, Loss: 0.0750\n",
      "input      :  the store is open .\n",
      "label      :  le magasin est ouvert . EOS\n",
      "predicted  :  le magasin est ouvert . EOS\n",
      "Eval Score (BLEU): 0.177\n",
      "Epoch: 140, Loss: 0.0902\n",
      "input      :  he can help you out .\n",
      "label      :  il peut vous donner un coup de main . EOS\n",
      "predicted  :  il peut te donner un coup de main . EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 141, Loss: 0.0899\n",
      "input      :  who is absent ?\n",
      "label      :  qui est absent ? EOS\n",
      "predicted  :  qui est absent ? EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 142, Loss: 0.0729\n",
      "input      :  drop the gun .\n",
      "label      :  lache l arme ! EOS\n",
      "predicted  :  lachez l arme ! EOS\n",
      "Eval Score (BLEU): 0.177\n",
      "Epoch: 143, Loss: 0.0845\n",
      "input      :  everybody hates me .\n",
      "label      :  tout le monde me deteste . EOS\n",
      "predicted  :  tout le monde me deteste . EOS\n",
      "Eval Score (BLEU): 0.176\n",
      "Epoch: 144, Loss: 0.0654\n",
      "input      :  i have a computer .\n",
      "label      :  je dispose d un ordinateur . EOS\n",
      "predicted  :  je dispose d un ordinateur . EOS\n",
      "Eval Score (BLEU): 0.179\n",
      "Epoch: 145, Loss: 0.0851\n",
      "input      :  i ll pay for lunch .\n",
      "label      :  je paierai pour le dejeuner . EOS\n",
      "predicted  :  je paierai pour le dejeuner . EOS\n",
      "Eval Score (BLEU): 0.179\n",
      "Epoch: 146, Loss: 0.0793\n",
      "input      :  do you see me ?\n",
      "label      :  me vois tu ? EOS\n",
      "predicted  :  me vois tu ? EOS\n",
      "Eval Score (BLEU): 0.177\n",
      "Epoch: 147, Loss: 0.0828\n",
      "input      :  you re naive .\n",
      "label      :  vous etes naives . EOS\n",
      "predicted  :  tu es naive . EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 148, Loss: 0.0775\n",
      "input      :  i want to kiss you .\n",
      "label      :  je veux t embrasser . EOS\n",
      "predicted  :  je veux t embrasser . EOS\n",
      "Eval Score (BLEU): 0.179\n",
      "Epoch: 149, Loss: 0.0862\n",
      "input      :  have a safe trip .\n",
      "label      :  faites un bon voyage ! EOS\n",
      "predicted  :  faites un bon voyage ! EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 150, Loss: 0.0895\n",
      "input      :  i m no hero .\n",
      "label      :  je ne suis pas un heros . EOS\n",
      "predicted  :  je ne suis pas un heros . EOS\n",
      "Eval Score (BLEU): 0.179\n",
      "Epoch: 151, Loss: 0.0898\n",
      "input      :  do you hear me ?\n",
      "label      :  tu m entends ? EOS\n",
      "predicted  :  tu m entends ? EOS\n",
      "Eval Score (BLEU): 0.177\n",
      "Epoch: 152, Loss: 0.0845\n",
      "input      :  i still have to try .\n",
      "label      :  je dois encore essayer . EOS\n",
      "predicted  :  il me faut encore essayer . EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 153, Loss: 0.0874\n",
      "input      :  you did it !\n",
      "label      :  c est vous qui l avez fait ! EOS\n",
      "predicted  :  c est vous qui l avez fait ! EOS\n",
      "Eval Score (BLEU): 0.177\n",
      "Epoch: 154, Loss: 0.0778\n",
      "input      :  i want to join you .\n",
      "label      :  je veux me joindre a vous . EOS\n",
      "predicted  :  je veux me joindre a toi . EOS\n",
      "Eval Score (BLEU): 0.179\n",
      "Epoch: 155, Loss: 0.0731\n",
      "input      :  it s not secure .\n",
      "label      :  ce n est pas securise . EOS\n",
      "predicted  :  ce n est pas securise . EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 156, Loss: 0.0814\n",
      "input      :  i did everything .\n",
      "label      :  j ai tout fait . EOS\n",
      "predicted  :  j ai tout fait . EOS\n",
      "Eval Score (BLEU): 0.179\n",
      "Epoch: 157, Loss: 0.0832\n",
      "input      :  let s hurry up .\n",
      "label      :  depechons . EOS\n",
      "predicted  :  depechons nous . EOS\n",
      "Eval Score (BLEU): 0.177\n",
      "Epoch: 158, Loss: 0.0874\n",
      "input      :  terrific !\n",
      "label      :  excellent ! EOS\n",
      "predicted  :  formidable ! EOS\n",
      "Eval Score (BLEU): 0.179\n",
      "Epoch: 159, Loss: 0.0666\n",
      "input      :  my foot hurts .\n",
      "label      :  mon pied est douloureux . EOS\n",
      "predicted  :  mon pied me fait mal . EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 160, Loss: 0.0761\n",
      "input      :  where s your hotel ?\n",
      "label      :  ou est ton hotel ? EOS\n",
      "predicted  :  ou est votre hotel ? EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 161, Loss: 0.0933\n",
      "input      :  is anyone hurt ?\n",
      "label      :  quelqu un est il blesse ? EOS\n",
      "predicted  :  quelqu un est il froisse ? EOS\n",
      "Eval Score (BLEU): 0.177\n",
      "Epoch: 162, Loss: 0.0906\n",
      "input      :  leave me alone .\n",
      "label      :  laissez moi tranquille ! EOS\n",
      "predicted  :  fous moi la paix ! EOS\n",
      "Eval Score (BLEU): 0.177\n",
      "Epoch: 163, Loss: 0.0930\n",
      "input      :  you re out of time .\n",
      "label      :  votre temps est ecoule . EOS\n",
      "predicted  :  ton temps est ecoule . EOS\n",
      "Eval Score (BLEU): 0.177\n",
      "Epoch: 164, Loss: 0.0806\n",
      "input      :  tom said no .\n",
      "label      :  tom a dit non . EOS\n",
      "predicted  :  tom a dit non . EOS\n",
      "Eval Score (BLEU): 0.176\n",
      "Epoch: 165, Loss: 0.0790\n",
      "input      :  don t give up .\n",
      "label      :  ne laissez pas tomber . EOS\n",
      "predicted  :  ne laisse pas tomber . EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 166, Loss: 0.0826\n",
      "input      :  i m out of shape .\n",
      "label      :  je ne suis pas en forme . EOS\n",
      "predicted  :  je ne suis pas en forme . EOS\n",
      "Eval Score (BLEU): 0.180\n",
      "Epoch: 167, Loss: 0.0824\n",
      "input      :  they can t do that .\n",
      "label      :  ils n y arrivent pas . EOS\n",
      "predicted  :  elles n arrivent pas a le faire . EOS\n",
      "Eval Score (BLEU): 0.179\n",
      "Epoch: 168, Loss: 0.0923\n",
      "input      :  they loved tom .\n",
      "label      :  ils aimaient tom . EOS\n",
      "predicted  :  ils aimaient tom . EOS\n",
      "Eval Score (BLEU): 0.180\n",
      "Epoch: 169, Loss: 0.0730\n",
      "input      :  do you get it ?\n",
      "label      :  est ce que tu captes ? EOS\n",
      "predicted  :  tu captes ? EOS\n",
      "Eval Score (BLEU): 0.179\n",
      "Epoch: 170, Loss: 0.0887\n",
      "input      :  it matters to me .\n",
      "label      :  ca signifie quelque chose pour moi . EOS\n",
      "predicted  :  ca a de l importance pour moi . EOS\n",
      "Eval Score (BLEU): 0.177\n",
      "Epoch: 171, Loss: 0.0802\n",
      "input      :  he is about thirty .\n",
      "label      :  il a environ trente ans . EOS\n",
      "predicted  :  il a environ trente ans . EOS\n",
      "Eval Score (BLEU): 0.179\n",
      "Epoch: 172, Loss: 0.0728\n",
      "input      :  i love mary .\n",
      "label      :  j aime mary . EOS\n",
      "predicted  :  j aime mary . EOS\n",
      "Eval Score (BLEU): 0.179\n",
      "Epoch: 173, Loss: 0.0843\n",
      "input      :  here it is .\n",
      "label      :  tenez . EOS\n",
      "predicted  :  voila . EOS\n",
      "Eval Score (BLEU): 0.180\n",
      "Epoch: 174, Loss: 0.0777\n",
      "input      :  i m now unarmed .\n",
      "label      :  je ne suis pas actuellement arme . EOS\n",
      "predicted  :  je ne suis pas actuellement arme . EOS\n",
      "Eval Score (BLEU): 0.179\n",
      "Epoch: 175, Loss: 0.0809\n",
      "input      :  tom s coming .\n",
      "label      :  tom vient . EOS\n",
      "predicted  :  tom arrive . EOS\n",
      "Eval Score (BLEU): 0.177\n",
      "Epoch: 176, Loss: 0.0641\n",
      "input      :  we must talk .\n",
      "label      :  il nous faut discuter . EOS\n",
      "predicted  :  il nous faut discuter . EOS\n",
      "Eval Score (BLEU): 0.176\n",
      "Epoch: 177, Loss: 0.0836\n",
      "input      :  we re not alone .\n",
      "label      :  nous ne sommes pas seules . EOS\n",
      "predicted  :  nous ne sommes pas seuls . EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 178, Loss: 0.0879\n",
      "input      :  no one will care .\n",
      "label      :  personne ne s en souciera . EOS\n",
      "predicted  :  personne ne s en souciera . EOS\n",
      "Eval Score (BLEU): 0.179\n",
      "Epoch: 179, Loss: 0.0763\n",
      "input      :  i m overworked .\n",
      "label      :  je suis surcharge de travail . EOS\n",
      "predicted  :  je suis surchargee de travail . EOS\n",
      "Eval Score (BLEU): 0.180\n",
      "Epoch: 180, Loss: 0.0764\n",
      "input      :  tom sounds angry .\n",
      "label      :  tom semble en colere . EOS\n",
      "predicted  :  tom semble en colere . EOS\n",
      "Eval Score (BLEU): 0.177\n",
      "Epoch: 181, Loss: 0.0804\n",
      "input      :  are those yours ?\n",
      "label      :  est ce les votres ? EOS\n",
      "predicted  :  est ce les tiens ? EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 182, Loss: 0.0721\n",
      "input      :  i love my mom .\n",
      "label      :  j aime ma maman . EOS\n",
      "predicted  :  j aime ma maman . EOS\n",
      "Eval Score (BLEU): 0.179\n",
      "Epoch: 183, Loss: 0.0759\n",
      "input      :  are you up for it ?\n",
      "label      :  y es tu prete ? EOS\n",
      "predicted  :  y es tu prete ? EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 184, Loss: 0.0604\n",
      "input      :  i want to dance .\n",
      "label      :  je veux danser . EOS\n",
      "predicted  :  je veux danser . EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 185, Loss: 0.0745\n",
      "input      :  tom seems stressed .\n",
      "label      :  tom semble stresse . EOS\n",
      "predicted  :  tom semble stresse . EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 186, Loss: 0.0766\n",
      "input      :  i fixed it .\n",
      "label      :  je le reparai . EOS\n",
      "predicted  :  je le reparai . EOS\n",
      "Eval Score (BLEU): 0.179\n",
      "Epoch: 187, Loss: 0.1007\n",
      "input      :  we have no proof .\n",
      "label      :  nous ne disposons d aucune preuve . EOS\n",
      "predicted  :  nous ne disposons d aucune preuve . EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 188, Loss: 0.0981\n",
      "input      :  choose carefully .\n",
      "label      :  choisis soigneusement . EOS\n",
      "predicted  :  choisissez soigneusement . EOS\n",
      "Eval Score (BLEU): 0.177\n",
      "Epoch: 189, Loss: 0.0802\n",
      "input      :  that s free .\n",
      "label      :  c est gratuit . EOS\n",
      "predicted  :  c est gratuit . EOS\n",
      "Eval Score (BLEU): 0.179\n",
      "Epoch: 190, Loss: 0.0690\n",
      "input      :  this one is bigger .\n",
      "label      :  celle ci est plus grosse . EOS\n",
      "predicted  :  celle ci est plus grosse . EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 191, Loss: 0.0592\n",
      "input      :  tell tom .\n",
      "label      :  dis le a tom . EOS\n",
      "predicted  :  informez en tom . EOS\n",
      "Eval Score (BLEU): 0.179\n",
      "Epoch: 192, Loss: 0.0670\n",
      "input      :  can you do it ?\n",
      "label      :  tu peux le faire ? EOS\n",
      "predicted  :  peux tu le decrire ? EOS\n",
      "Eval Score (BLEU): 0.180\n",
      "Epoch: 193, Loss: 0.0660\n",
      "input      :  i like trains .\n",
      "label      :  j aime les trains . EOS\n",
      "predicted  :  j aime les trains . EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 194, Loss: 0.0880\n",
      "input      :  be content .\n",
      "label      :  soyez satisfaits ! EOS\n",
      "predicted  :  soyez satisfaites ! EOS\n",
      "Eval Score (BLEU): 0.179\n",
      "Epoch: 195, Loss: 0.0874\n",
      "input      :  what a huge dog !\n",
      "label      :  quel enorme chien ! EOS\n",
      "predicted  :  quel enorme chien ! EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 196, Loss: 0.0923\n",
      "input      :  i can drive you .\n",
      "label      :  je peux te conduire . EOS\n",
      "predicted  :  je peux te conduire . EOS\n",
      "Eval Score (BLEU): 0.179\n",
      "Epoch: 197, Loss: 0.0677\n",
      "input      :  it s awful .\n",
      "label      :  c est affreux . EOS\n",
      "predicted  :  c est affreux . EOS\n",
      "Eval Score (BLEU): 0.180\n",
      "Epoch: 198, Loss: 0.0774\n",
      "input      :  i left it unlocked .\n",
      "label      :  je l ai laissee deverrouillee . EOS\n",
      "predicted  :  je l ai laissee deverrouillee . EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 199, Loss: 0.0725\n",
      "input      :  no one s talking .\n",
      "label      :  personne ne parle . EOS\n",
      "predicted  :  personne ne parle . EOS\n",
      "Eval Score (BLEU): 0.179\n",
      "Epoch: 200, Loss: 0.0786\n",
      "input      :  i think i hear them .\n",
      "label      :  je pense que je les entends . EOS\n",
      "predicted  :  il me semble que je les entends . EOS\n",
      "Eval Score (BLEU): 0.180\n",
      "Epoch: 201, Loss: 0.0836\n",
      "input      :  there s a leak .\n",
      "label      :  il y a la une fuite . EOS\n",
      "predicted  :  il y a la une fuite . EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 202, Loss: 0.0650\n",
      "input      :  we need to talk .\n",
      "label      :  il nous est necessaire de parler . EOS\n",
      "predicted  :  il est necessaire que nous parlions . EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 203, Loss: 0.0679\n",
      "input      :  is that so ?\n",
      "label      :  est ce le cas ? EOS\n",
      "predicted  :  en est il ainsi ? EOS\n",
      "Eval Score (BLEU): 0.176\n",
      "Epoch: 204, Loss: 0.0747\n",
      "input      :  i ll be staying .\n",
      "label      :  je vais rester . EOS\n",
      "predicted  :  je vais rester . EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 205, Loss: 0.0936\n",
      "input      :  i know you are rich .\n",
      "label      :  je sais que vous etes riche . EOS\n",
      "predicted  :  je sais que vous etes riche . EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 206, Loss: 0.0690\n",
      "input      :  i came to see you .\n",
      "label      :  je suis venu pour vous voir . EOS\n",
      "predicted  :  je suis venu pour vous voir . EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 207, Loss: 0.0615\n",
      "input      :  you re the oldest .\n",
      "label      :  c est toi l aine . EOS\n",
      "predicted  :  c est vous l aine . EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 208, Loss: 0.0766\n",
      "input      :  i was wrong .\n",
      "label      :  j avais tort . EOS\n",
      "predicted  :  j eus tort . EOS\n",
      "Eval Score (BLEU): 0.179\n",
      "Epoch: 209, Loss: 0.0686\n",
      "input      :  i need his help .\n",
      "label      :  j ai besoin de son aide . EOS\n",
      "predicted  :  j ai besoin de son aide . EOS\n",
      "Eval Score (BLEU): 0.179\n",
      "Epoch: 210, Loss: 0.0784\n",
      "input      :  are you hurt ?\n",
      "label      :  vous etes vous blesses ? EOS\n",
      "predicted  :  etes vous blessee ? EOS\n",
      "Eval Score (BLEU): 0.180\n",
      "Epoch: 211, Loss: 0.0698\n",
      "input      :  it s a good choice .\n",
      "label      :  c est un bon choix . EOS\n",
      "predicted  :  c est un bon choix . EOS\n",
      "Eval Score (BLEU): 0.179\n",
      "Epoch: 212, Loss: 0.0814\n",
      "input      :  how s tom doing ?\n",
      "label      :  comment tom va t il ? EOS\n",
      "predicted  :  comment va tom ? EOS\n",
      "Eval Score (BLEU): 0.179\n",
      "Epoch: 213, Loss: 0.0755\n",
      "input      :  have a beer .\n",
      "label      :  prends une biere ! EOS\n",
      "predicted  :  prends une biere ! EOS\n",
      "Eval Score (BLEU): 0.179\n",
      "Epoch: 214, Loss: 0.0749\n",
      "input      :  you re so bossy .\n",
      "label      :  vous etes tellement autoritaire ! EOS\n",
      "predicted  :  tu es tellement autoritaire ! EOS\n",
      "Eval Score (BLEU): 0.177\n",
      "Epoch: 215, Loss: 0.0931\n",
      "input      :  i hate reptiles .\n",
      "label      :  j ai horreur des reptiles . EOS\n",
      "predicted  :  j ai horreur des reptiles . EOS\n",
      "Eval Score (BLEU): 0.180\n",
      "Epoch: 216, Loss: 0.0671\n",
      "input      :  i ll destroy it .\n",
      "label      :  je le detruirai . EOS\n",
      "predicted  :  je le detruirai . EOS\n",
      "Eval Score (BLEU): 0.179\n",
      "Epoch: 217, Loss: 0.0627\n",
      "input      :  raise your hands .\n",
      "label      :  levez les mains ! EOS\n",
      "predicted  :  leve les mains ! EOS\n",
      "Eval Score (BLEU): 0.180\n",
      "Epoch: 218, Loss: 0.0739\n",
      "input      :  i ll be at tom s .\n",
      "label      :  je serai chez tom . EOS\n",
      "predicted  :  je serai chez tom . EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 219, Loss: 0.0572\n",
      "input      :  i followed you .\n",
      "label      :  je vous ai suivie . EOS\n",
      "predicted  :  je vous ai suivie . EOS\n",
      "Eval Score (BLEU): 0.179\n",
      "Epoch: 220, Loss: 0.0821\n",
      "input      :  it was quite funny .\n",
      "label      :  c etait assez amusant . EOS\n",
      "predicted  :  c etait plutot amusant . EOS\n",
      "Eval Score (BLEU): 0.177\n",
      "Epoch: 221, Loss: 0.0713\n",
      "input      :  the check please .\n",
      "label      :  l addition s il vous plait . EOS\n",
      "predicted  :  l addition s il vous plait . EOS\n",
      "Eval Score (BLEU): 0.180\n",
      "Epoch: 222, Loss: 0.0817\n",
      "input      :  we apologize .\n",
      "label      :  nous presentons nos excuses . EOS\n",
      "predicted  :  nous presentons nos excuses . EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 223, Loss: 0.0906\n",
      "input      :  this hat is mine .\n",
      "label      :  ce chapeau est le mien . EOS\n",
      "predicted  :  ce chapeau est le mien . EOS\n",
      "Eval Score (BLEU): 0.177\n",
      "Epoch: 224, Loss: 0.0828\n",
      "input      :  i d be careful .\n",
      "label      :  je serais prudente . EOS\n",
      "predicted  :  je serais prudente . EOS\n",
      "Eval Score (BLEU): 0.179\n",
      "Epoch: 225, Loss: 0.0790\n",
      "input      :  i hate them .\n",
      "label      :  je les deteste . EOS\n",
      "predicted  :  je les deteste . EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 226, Loss: 0.0822\n",
      "input      :  do you have a knife ?\n",
      "label      :  avez vous un couteau ? EOS\n",
      "predicted  :  tu as un couteau ? EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 227, Loss: 0.0840\n",
      "input      :  i ll set you free .\n",
      "label      :  je vous libererai . EOS\n",
      "predicted  :  je vous libererai . EOS\n",
      "Eval Score (BLEU): 0.180\n",
      "Epoch: 228, Loss: 0.0778\n",
      "input      :  find somebody else .\n",
      "label      :  trouvez quelqu un d autre . EOS\n",
      "predicted  :  trouve quelqu un d autre . EOS\n",
      "Eval Score (BLEU): 0.177\n",
      "Epoch: 229, Loss: 0.0683\n",
      "input      :  are you busy ?\n",
      "label      :  es tu occupe ? EOS\n",
      "predicted  :  etes vous occupes ? EOS\n",
      "Eval Score (BLEU): 0.177\n",
      "Epoch: 230, Loss: 0.0649\n",
      "input      :  would you like one ?\n",
      "label      :  en voudriez vous un ? EOS\n",
      "predicted  :  en voudriez vous un ? EOS\n",
      "Eval Score (BLEU): 0.179\n",
      "Epoch: 231, Loss: 0.0947\n",
      "input      :  i buried it .\n",
      "label      :  je l ai enterre . EOS\n",
      "predicted  :  je l ai enterree . EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 232, Loss: 0.0999\n",
      "input      :  i love bearded men .\n",
      "label      :  j adore les barbus . EOS\n",
      "predicted  :  j adore les barbus . EOS\n",
      "Eval Score (BLEU): 0.180\n",
      "Epoch: 233, Loss: 0.0803\n",
      "input      :  the tea is hot .\n",
      "label      :  le the est brulant . EOS\n",
      "predicted  :  le the est brulant . EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 234, Loss: 0.0686\n",
      "input      :  i took off my coat .\n",
      "label      :  je retirai mon manteau . EOS\n",
      "predicted  :  je retirai mon manteau . EOS\n",
      "Eval Score (BLEU): 0.176\n",
      "Epoch: 235, Loss: 0.0559\n",
      "input      :  you re too loud .\n",
      "label      :  tu es trop bruyant . EOS\n",
      "predicted  :  vous etes trop bruyant . EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 236, Loss: 0.0818\n",
      "input      :  this dog bites .\n",
      "label      :  cette chienne mord . EOS\n",
      "predicted  :  ce chien mord . EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 237, Loss: 0.0685\n",
      "input      :  i like mathematics .\n",
      "label      :  j aime les mathematiques . EOS\n",
      "predicted  :  j aime les mathematiques . EOS\n",
      "Eval Score (BLEU): 0.177\n",
      "Epoch: 238, Loss: 0.0595\n",
      "input      :  has tom calmed down ?\n",
      "label      :  tom s est il calme ? EOS\n",
      "predicted  :  tom s est il calme ? EOS\n",
      "Eval Score (BLEU): 0.179\n",
      "Epoch: 239, Loss: 0.0826\n",
      "input      :  hey where are you ?\n",
      "label      :  he t es ou ? EOS\n",
      "predicted  :  he ou etes vous ? EOS\n",
      "Eval Score (BLEU): 0.177\n",
      "Epoch: 240, Loss: 0.0858\n",
      "input      :  i want to do this .\n",
      "label      :  je veux faire ca . EOS\n",
      "predicted  :  je veux faire ceci . EOS\n",
      "Eval Score (BLEU): 0.177\n",
      "Epoch: 241, Loss: 0.0770\n",
      "input      :  mom has a fever .\n",
      "label      :  maman est fievreuse . EOS\n",
      "predicted  :  maman est fievreuse . EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 242, Loss: 0.0718\n",
      "input      :  he took his book .\n",
      "label      :  il a pris son livre . EOS\n",
      "predicted  :  il a pris son ouvrage . EOS\n",
      "Eval Score (BLEU): 0.179\n",
      "Epoch: 243, Loss: 0.0731\n",
      "input      :  i cooked him dinner .\n",
      "label      :  je lui ai prepare a dejeuner . EOS\n",
      "predicted  :  je lui ai prepare a dejeuner . EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 244, Loss: 0.0900\n",
      "input      :  did you get it ?\n",
      "label      :  avez vous compris ? EOS\n",
      "predicted  :  as tu compris ? EOS\n",
      "Eval Score (BLEU): 0.180\n",
      "Epoch: 245, Loss: 0.0666\n",
      "input      :  they re both right .\n",
      "label      :  elles ont toutes les deux raison . EOS\n",
      "predicted  :  ils ont tous les deux raison . EOS\n",
      "Eval Score (BLEU): 0.179\n",
      "Epoch: 246, Loss: 0.0781\n",
      "input      :  that book is small .\n",
      "label      :  cet ouvrage est petit . EOS\n",
      "predicted  :  cet ouvrage est petit . EOS\n",
      "Eval Score (BLEU): 0.178\n",
      "Epoch: 247, Loss: 0.0741\n",
      "input      :  i swore .\n",
      "label      :  j ai promis . EOS\n",
      "predicted  :  j ai jure . EOS\n",
      "Eval Score (BLEU): 0.180\n",
      "Epoch: 248, Loss: 0.0881\n",
      "input      :  everybody hates you .\n",
      "label      :  tout le monde vous deteste . EOS\n",
      "predicted  :  tout le monde te deteste . EOS\n",
      "Eval Score (BLEU): 0.180\n",
      "Epoch: 249, Loss: 0.0669\n",
      "input      :  you re a prisoner .\n",
      "label      :  vous etes un prisonnier . EOS\n",
      "predicted  :  vous etes un prisonnier . EOS\n",
      "Eval Score (BLEU): 0.180\n",
      "Epoch: 250, Loss: 0.0585\n",
      "input      :  they re old .\n",
      "label      :  elles sont vieilles . EOS\n",
      "predicted  :  elles sont vieilles . EOS\n",
      "Eval Score (BLEU): 0.181\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./checkpoint/RNN_n_n_seq2seq/ckpt-26'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_dir = './checkpoint/RNN_n_n_seq2seq'\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "    encoder=encoder, decoder=decoder)\n",
    "\n",
    "num_epochs = NUM_EPOCHS\n",
    "eval_scores = []\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    encoder_state = encoder.init_state(batch_size)\n",
    "\n",
    "    for batch, data in enumerate(train_dataset):\n",
    "        encoder_in, decoder_in, decoder_out = data\n",
    "        #print(encoder_in.shape, decoder_in.shape, decoder_out.shape)\n",
    "        loss = train_step(encoder_in, decoder_in, decoder_out, encoder_state)\n",
    "    \n",
    "    print('Epoch: {}, Loss: {:.4f}'.format(e + 1, loss.numpy()))\n",
    "\n",
    "    if e % 10 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "    predict(encoder, decoder, batch_size, sents_en, data_en, sents_fr_out, word2idx_fr, idx2word_fr)\n",
    "\n",
    "    eval_score = evaluate_bleu_score(encoder, decoder, test_dataset, word2idx_fr, idx2word_fr)\n",
    "    print(\"Eval Score (BLEU): {:.3f}\".format(eval_score))\n",
    "    eval_scores.append(eval_score)\n",
    "\n",
    "checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어텐션 메커니즘(seq2seq 성능 향상)\n",
    "- 어텐션 메커니즘은 디코더의 모든 타임 스텝에서 모든 인코너 은닉 상태에 대한 액세스를 제공한다.\n",
    "- 디코더는 인코더 상태 중 어느 부분에 더 많은 주의(attention)를 기울여야 하는지 학습한다.\n",
    "- 어텐션은 최적의 딥러닝 모델을 만들기 위한 NLP 공식인 '임베딩,인코딩,어텐션,예측'의 핵심 구성 요소다.\n",
    "- 여기서는 단어 벡터 시퀀스를 단일 문장 벡터로 축소 할 때처럼 더 큰 표현에서 더 작은 표현으로 축소할 때 가능한 한 많은 정보를 보존하고자 어텐션을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기계 번역용 어텐션 있는 seq2seq 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, shutil, os, unicodedata, zipfile\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sent):\n",
    "    sent = \"\".join([c for c in unicodedata.normalize(\"NFD\", sent) \n",
    "        if unicodedata.category(c) != \"Mn\"])\n",
    "    sent = re.sub(r\"([!.?])\", r\" \\1\", sent)\n",
    "    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
    "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
    "    sent = sent.lower()\n",
    "    return sent\n",
    "\n",
    "def download_and_read(num_sent_pairs=30000):\n",
    "    local_file = os.path.join(\"datasets/RNN_n_n_seq2seq\", \"fra.txt\")\n",
    "    en_sents, fr_sents_in, fr_sents_out = [], [], []\n",
    "    with open(local_file, \"r\") as fin:\n",
    "        for i, line in enumerate(fin):\n",
    "            en_sent, fr_sent = line.strip().split('\\t')[0:2]\n",
    "            en_sent = [w for w in preprocess_sentence(en_sent).split()]\n",
    "            fr_sent = preprocess_sentence(fr_sent)\n",
    "            fr_sent_in = [w for w in (\"BOS \" + fr_sent).split()]\n",
    "            fr_sent_out = [w for w in (fr_sent + \" EOS\").split()]\n",
    "            en_sents.append(en_sent)\n",
    "            fr_sents_in.append(fr_sent_in)\n",
    "            fr_sents_out.append(fr_sent_out)\n",
    "            if i >= num_sent_pairs - 1:\n",
    "                break\n",
    "    return en_sents, fr_sents_in, fr_sents_out\n",
    "\n",
    "# 바다나우 어텐션(가산적 어텐션) 추가\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(num_units)\n",
    "        self.W2 = tf.keras.layers.Dense(num_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # 쿼리는 타임 스텝 j에서의 디코더 상태다.\n",
    "        # query.shape: (batch_size, num_units)\n",
    "        # 값은 모든 타임 스텝 1에서 인코더 상태다.\n",
    "        # values.shape: (batch_size, num_timesteps, num_units)\n",
    "\n",
    "        # 쿼리에 시각 축 추가: (batch_size, 1, num_units)\n",
    "        query_with_time_axis = tf.expand_dims(query, axis=1)\n",
    "        # 점수 계산\n",
    "        score = self.V(tf.keras.activations.tanh(\n",
    "            self.W1(values) + self.W2(query_with_time_axis)))\n",
    "        # 소프트맥스 계산\n",
    "        alignment = tf.nn.softmax(score, axis=1)\n",
    "        # 어텐션된 출력 계산\n",
    "        context = tf.reduce_sum(\n",
    "            tf.linalg.matmul(\n",
    "                tf.linalg.matrix_transpose(alignment),\n",
    "                values\n",
    "            ), axis=1\n",
    "        )\n",
    "        context = tf.expand_dims(context, axis=1)\n",
    "        return context, alignment\n",
    "\n",
    "# 루옹 어텐션(곱 어텐션) 추가\n",
    "class LuongAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_units):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        self.W = tf.keras.layers.Dense(num_units)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # 질의에 시각 축을 추가한다.\n",
    "        query_with_time_axis = tf.expand_dims(query, axis=1)\n",
    "        # 점수 계산\n",
    "        score = tf.linalg.matmul(\n",
    "            query_with_time_axis, self.W(values), transpose_b=True)\n",
    "        # 소프트맥스 계산\n",
    "        alignment = tf.nn.softmax(score, axis=2)\n",
    "        # 어텐션된 출력 계산\n",
    "        context = tf.matmul(alignment, values)\n",
    "        return context, alignment\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, num_timesteps, \n",
    "            embedding_dim, encoder_dim, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size, embedding_dim, input_length=num_timesteps)\n",
    "        self.rnn = tf.keras.layers.GRU(\n",
    "            encoder_dim, return_sequences=True, return_state=True)  # return_sequences=False -> True 로 변경\n",
    "\n",
    "    def call(self, x, state):\n",
    "        x = self.embedding(x)\n",
    "        x, state = self.rnn(x, initial_state=state)\n",
    "        return x, state\n",
    "\n",
    "    def init_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.encoder_dim))\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_timesteps,\n",
    "            decoder_dim, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        self.decoder_dim = decoder_dim\n",
    "\n",
    "        #self.attention = BahdanauAttention(embedding_dim) # 추가\n",
    "        self.attention = LuongAttention(embedding_dim)     # 추가\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size, embedding_dim, input_length=num_timesteps)\n",
    "        self.rnn = tf.keras.layers.GRU(\n",
    "            decoder_dim, return_sequences=True, return_state=True)\n",
    "\n",
    "        self.Wc = tf.keras.layers.Dense(decoder_dim, activation=\"tanh\")  # 추가\n",
    "        self.Ws = tf.keras.layers.Dense(vocab_size)                      # 추가\n",
    "\n",
    "    def call(self, x, state, encoder_out):\n",
    "        x = self.embedding(x)\n",
    "        context, alignment = self.attention(x, encoder_out)\n",
    "        x = tf.expand_dims(\n",
    "                tf.concat([\n",
    "                    x, tf.squeeze(context, axis=1)\n",
    "                ], axis=1), \n",
    "            axis=1)\n",
    "        x, state = self.rnn(x, state)\n",
    "        x = self.Wc(x)\n",
    "        x = self.Ws(x)\n",
    "        return x, state, alignment\n",
    "\n",
    "def loss_fn(ytrue, ypred):\n",
    "    scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    mask = tf.math.logical_not(tf.math.equal(ytrue, 0))\n",
    "    mask = tf.cast(mask, dtype=tf.int64)\n",
    "    loss = scce(ytrue, ypred, sample_weight=mask)\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def train_step(encoder_in, decoder_in, decoder_out, encoder_state):\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n",
    "        decoder_state = encoder_state\n",
    "\n",
    "        loss = 0\n",
    "        for t in range(decoder_out.shape[1]):\n",
    "            decoder_in_t = decoder_in[:, t]\n",
    "            decoder_pred_t, decoder_state, _ = decoder(decoder_in_t,\n",
    "                decoder_state, encoder_out)\n",
    "            loss += loss_fn(decoder_out[:, t], decoder_pred_t)\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return loss / decoder_out.shape[1]\n",
    "\n",
    "def predict(encoder, decoder, batch_size, sents_en, data_en, sents_fr_out, word2idx_fr, idx2word_fr):\n",
    "    random_id = np.random.choice(len(sents_en))\n",
    "    print(\"input    : \",  \" \".join(sents_en[random_id]))\n",
    "    print(\"label    : \", \" \".join(sents_fr_out[random_id]))\n",
    "\n",
    "    encoder_in = tf.expand_dims(data_en[random_id], axis=0)\n",
    "    decoder_out = tf.expand_dims(sents_fr_out[random_id], axis=0)\n",
    "\n",
    "    encoder_state = encoder.init_state(1)\n",
    "    encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n",
    "    decoder_state = encoder_state\n",
    "\n",
    "    pred_sent_fr = []\n",
    "    decoder_in = tf.expand_dims(\n",
    "        tf.constant(word2idx_fr[\"BOS\"]), axis=0)\n",
    "\n",
    "    while True:\n",
    "        decoder_pred, decoder_state, _ = decoder(\n",
    "            decoder_in, decoder_state, encoder_out)\n",
    "        decoder_pred = tf.argmax(decoder_pred, axis=-1)\n",
    "        pred_word = idx2word_fr[decoder_pred.numpy()[0][0]]\n",
    "        pred_sent_fr.append(pred_word)\n",
    "        if pred_word == \"EOS\":\n",
    "            break\n",
    "        decoder_in = tf.squeeze(decoder_pred, axis=1)\n",
    "\n",
    "    print(\"predicted: \", \" \".join(pred_sent_fr))\n",
    "\n",
    "def evaluate_bleu_score(encoder, decoder, test_dataset, word2idx_fr, idx2word_fr):\n",
    "\n",
    "    bleu_scores = []\n",
    "    smooth_fn = SmoothingFunction()\n",
    "\n",
    "    for encoder_in, decoder_in, decoder_out in test_dataset:\n",
    "        encoder_state = encoder.init_state(batch_size)\n",
    "        encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n",
    "        decoder_state = encoder_state\n",
    "\n",
    "        ref_sent_ids = np.zeros_like(decoder_out)\n",
    "        hyp_sent_ids = np.zeros_like(decoder_out)\n",
    "        for t in range(decoder_out.shape[1]):\n",
    "            decoder_out_t = decoder_out[:, t]\n",
    "            decoder_in_t = decoder_in[:, t]\n",
    "            decoder_pred_t, decoder_state, _ = decoder(\n",
    "                decoder_in_t, decoder_state, encoder_out)\n",
    "            # argmax 계산\n",
    "            decoder_pred_t = tf.argmax(decoder_pred_t, axis=-1)\n",
    "            for b in range(decoder_pred_t.shape[0]):\n",
    "                ref_sent_ids[b, t] = decoder_out_t.numpy()[0]\n",
    "                hyp_sent_ids[b, t] = decoder_pred_t.numpy()[0][0]\n",
    "        \n",
    "        for b in range(ref_sent_ids.shape[0]):\n",
    "            ref_sent = [idx2word_fr[i] for i in ref_sent_ids[b] if i > 0]\n",
    "            hyp_sent = [idx2word_fr[i] for i in hyp_sent_ids[b] if i > 0]\n",
    "            # 뒷부분 EOS 제거\n",
    "            ref_sent = ref_sent[0:-1]\n",
    "            hyp_sent = hyp_sent[0:-1]\n",
    "            bleu_score = sentence_bleu([ref_sent], hyp_sent,\n",
    "                smoothing_function=smooth_fn.method1)\n",
    "            bleu_scores.append(bleu_score)\n",
    "\n",
    "    return np.mean(np.array(bleu_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bahdanau: context.shape: (64, 1, 1024) alignments.shape: (64, 100, 1)\n",
      "Luong: context.shape: (64, 1, 1024) alignments.shape: (64, 1, 100)\n"
     ]
    }
   ],
   "source": [
    "# 가산적 어텐선, 곱 어텐션이 각각 서로의 드롭인 대체인지 확인하고자 다음의 일회성 코드를 수행해본다.\n",
    "# 입력은 임의의 입력을 만들어 두 어텐션 클래스로 보낸다.\n",
    "batch_size = 64\n",
    "num_timesteps = 100\n",
    "num_units = 1024\n",
    "\n",
    "## 이 코드를 실행할때만 사용할 것.\n",
    "#tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "np.random.seed(42)\n",
    "query = np.random.random(size=(batch_size, num_units))\n",
    "values = np.random.random(size=(batch_size, num_timesteps, num_units))\n",
    "\n",
    "# 바다나우 어텐션(가산적 어텐션) 차원 확인\n",
    "b_attn = BahdanauAttention(num_units)\n",
    "context, alignments = b_attn(query, values)\n",
    "print('Bahdanau: context.shape:', context.shape, 'alignments.shape:', alignments.shape)\n",
    "\n",
    "# 루옹 어텐션(곱 어텐션) 차원 확인\n",
    "l_attn = LuongAttention(num_units)\n",
    "context, alignments = l_attn(query, values)\n",
    "print('Luong: context.shape:', context.shape, 'alignments.shape:', alignments.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size (en): 4352, vocab size (fr): 7574\n",
      "seqlen (en): 8, (fr): 16\n"
     ]
    }
   ],
   "source": [
    "# 상수 정의\n",
    "NUM_SENT_PAIRS = 30000\n",
    "EMBEDDING_DIM = 256\n",
    "ENCODER_DIM, DECODER_DIM = 1024, 1024\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "sents_en, sents_fr_in, sents_fr_out = download_and_read()\n",
    "\n",
    "tokenizer_en = tf.keras.preprocessing.text.Tokenizer(\n",
    "    filters=\"\", lower=False)\n",
    "tokenizer_en.fit_on_texts(sents_en)\n",
    "data_en = tokenizer_en.texts_to_sequences(sents_en)\n",
    "data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en, padding=\"post\")\n",
    "\n",
    "tokenizer_fr = tf.keras.preprocessing.text.Tokenizer(\n",
    "    filters=\"\", lower=False)\n",
    "tokenizer_fr.fit_on_texts(sents_fr_in)\n",
    "tokenizer_fr.fit_on_texts(sents_fr_out)\n",
    "data_fr_in = tokenizer_fr.texts_to_sequences(sents_fr_in)\n",
    "data_fr_in = tf.keras.preprocessing.sequence.pad_sequences(data_fr_in, padding=\"post\")\n",
    "data_fr_out = tokenizer_fr.texts_to_sequences(sents_fr_out)\n",
    "data_fr_out = tf.keras.preprocessing.sequence.pad_sequences(data_fr_out, padding=\"post\")\n",
    "\n",
    "batch_size = BATCH_SIZE\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data_en, data_fr_in, data_fr_out))\n",
    "dataset = dataset.shuffle(10000)\n",
    "test_size = NUM_SENT_PAIRS // 4\n",
    "test_dataset = dataset.take(test_size).batch(batch_size, drop_remainder=True)\n",
    "train_dataset = dataset.skip(test_size).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "vocab_size_en = len(tokenizer_en.word_index)\n",
    "word2idx_en = tokenizer_en.word_index\n",
    "idx2word_en = {v:k for k, v in word2idx_en.items()}\n",
    "\n",
    "vocab_size_fr = len(tokenizer_fr.word_index)\n",
    "word2idx_fr = tokenizer_fr.word_index\n",
    "idx2word_fr = {v:k for k, v in word2idx_fr.items()}\n",
    "\n",
    "print(\"vocab size (en): {:d}, vocab size (fr): {:d}\".format(\n",
    "    vocab_size_en, vocab_size_fr))\n",
    "\n",
    "maxlen_en = data_en.shape[1]\n",
    "maxlen_fr = data_fr_out.shape[1]\n",
    "print(\"seqlen (en): {:d}, (fr): {:d}\".format(maxlen_en, maxlen_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_dim = EMBEDDING_DIM\n",
    "encoder_dim, decoder_dim = ENCODER_DIM, DECODER_DIM\n",
    "\n",
    "encoder = Encoder(vocab_size_en+1, embedding_dim, maxlen_en, encoder_dim)\n",
    "decoder = Decoder(vocab_size_fr+1, embedding_dim, maxlen_fr, decoder_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1.1956\n",
      "input    :  we look after him .\n",
      "label    :  nous nous occupons de lui . EOS\n",
      "predicted:  nous sommes tous deux . EOS\n",
      "Eval Score (BLEU): 0.024\n",
      "Epoch: 2, Loss: 1.0110\n",
      "input    :  do it by yourself .\n",
      "label    :  fais le toi meme ! EOS\n",
      "predicted:  c est mon nom . EOS\n",
      "Eval Score (BLEU): 0.023\n",
      "Epoch: 3, Loss: 0.8353\n",
      "input    :  grab the bottom .\n",
      "label    :  attrape le bas . EOS\n",
      "predicted:  restez en position ! EOS\n",
      "Eval Score (BLEU): 0.034\n",
      "Epoch: 4, Loss: 0.6669\n",
      "input    :  guess what happened .\n",
      "label    :  devine ce qui s est passe . EOS\n",
      "predicted:  monte le ton chien . EOS\n",
      "Eval Score (BLEU): 0.057\n",
      "Epoch: 5, Loss: 0.5671\n",
      "input    :  i strongly agree .\n",
      "label    :  je suis tout a fait d accord . EOS\n",
      "predicted:  je suis en securite . EOS\n",
      "Eval Score (BLEU): 0.059\n",
      "Epoch: 6, Loss: 0.4764\n",
      "input    :  is this ethical ?\n",
      "label    :  est ce ethique ? EOS\n",
      "predicted:  est ce garanti ? EOS\n",
      "Eval Score (BLEU): 0.075\n",
      "Epoch: 7, Loss: 0.4383\n",
      "input    :  how can i do that ?\n",
      "label    :  comment puis je faire cela ? EOS\n",
      "predicted:  comment puis je faire cela ? EOS\n",
      "Eval Score (BLEU): 0.071\n",
      "Epoch: 8, Loss: 0.3245\n",
      "input    :  shut up and listen !\n",
      "label    :  tais toi et ecoute ! EOS\n",
      "predicted:  ferme la balance . EOS\n",
      "Eval Score (BLEU): 0.076\n",
      "Epoch: 9, Loss: 0.2796\n",
      "input    :  that s not my car .\n",
      "label    :  ce n est pas ma voiture . EOS\n",
      "predicted:  ce n est pas ma voiture . EOS\n",
      "Eval Score (BLEU): 0.099\n",
      "Epoch: 10, Loss: 0.2484\n",
      "input    :  close that drawer .\n",
      "label    :  ferme ce tiroir . EOS\n",
      "predicted:  fermez ce tiroir . EOS\n",
      "Eval Score (BLEU): 0.096\n",
      "Epoch: 11, Loss: 0.2116\n",
      "input    :  you re contagious .\n",
      "label    :  tu es contagieux . EOS\n",
      "predicted:  vous etes contagieuse . EOS\n",
      "Eval Score (BLEU): 0.104\n",
      "Epoch: 12, Loss: 0.2660\n",
      "input    :  you re very timid .\n",
      "label    :  tu es tres craintif . EOS\n",
      "predicted:  tu es tres peureuse . EOS\n",
      "Eval Score (BLEU): 0.109\n",
      "Epoch: 13, Loss: 0.2029\n",
      "input    :  i am not a witch .\n",
      "label    :  je ne suis pas une sorciere . EOS\n",
      "predicted:  je ne suis pas un robot . EOS\n",
      "Eval Score (BLEU): 0.134\n",
      "Epoch: 14, Loss: 0.1960\n",
      "input    :  what s your name ?\n",
      "label    :  comment est ce que tu t appelles ? EOS\n",
      "predicted:  comment t appelles tu ? EOS\n",
      "Eval Score (BLEU): 0.130\n",
      "Epoch: 15, Loss: 0.2266\n",
      "input    :  you re a dreamer .\n",
      "label    :  vous etes un reveur . EOS\n",
      "predicted:  vous etes un reveur . EOS\n",
      "Eval Score (BLEU): 0.139\n",
      "Epoch: 16, Loss: 0.1771\n",
      "input    :  it was night .\n",
      "label    :  c etait la nuit . EOS\n",
      "predicted:  c etait cher . EOS\n",
      "Eval Score (BLEU): 0.132\n",
      "Epoch: 17, Loss: 0.1898\n",
      "input    :  he s a filthy liar .\n",
      "label    :  c est un sale menteur ! EOS\n",
      "predicted:  c est un sale menteur ! EOS\n",
      "Eval Score (BLEU): 0.136\n",
      "Epoch: 18, Loss: 0.1872\n",
      "input    :  unbelievable !\n",
      "label    :  incroyable ! EOS\n",
      "predicted:  incroyable ! EOS\n",
      "Eval Score (BLEU): 0.150\n",
      "Epoch: 19, Loss: 0.1412\n",
      "input    :  i m used to it .\n",
      "label    :  je suis habitue . EOS\n",
      "predicted:  j y suis accoutume . EOS\n",
      "Eval Score (BLEU): 0.146\n",
      "Epoch: 20, Loss: 0.1622\n",
      "input    :  well let s go .\n",
      "label    :  eh bien allons y . EOS\n",
      "predicted:  tout le monde y va . EOS\n",
      "Eval Score (BLEU): 0.162\n",
      "Epoch: 21, Loss: 0.1698\n",
      "input    :  i couldn t hear tom .\n",
      "label    :  je n arrivais pas a entendre tom . EOS\n",
      "predicted:  je n ai pas entendu tom . EOS\n",
      "Eval Score (BLEU): 0.140\n",
      "Epoch: 22, Loss: 0.1823\n",
      "input    :  you re the teacher .\n",
      "label    :  c est toi l instituteur . EOS\n",
      "predicted:  c est toi le professeur . EOS\n",
      "Eval Score (BLEU): 0.134\n",
      "Epoch: 23, Loss: 0.1594\n",
      "input    :  i can t go out .\n",
      "label    :  je ne peux pas sortir . EOS\n",
      "predicted:  je ne peux pas sortir . EOS\n",
      "Eval Score (BLEU): 0.146\n",
      "Epoch: 24, Loss: 0.1360\n",
      "input    :  you are deranged .\n",
      "label    :  tu es derange . EOS\n",
      "predicted:  vous etes derange . EOS\n",
      "Eval Score (BLEU): 0.161\n",
      "Epoch: 25, Loss: 0.1337\n",
      "input    :  i m by your side .\n",
      "label    :  je suis de votre cote . EOS\n",
      "predicted:  je suis avec vous . EOS\n",
      "Eval Score (BLEU): 0.153\n",
      "Epoch: 26, Loss: 0.1487\n",
      "input    :  he is busy .\n",
      "label    :  il a a faire . EOS\n",
      "predicted:  il est occupe a tout . EOS\n",
      "Eval Score (BLEU): 0.123\n",
      "Epoch: 27, Loss: 0.1514\n",
      "input    :  you re very clever .\n",
      "label    :  vous etes fort habile . EOS\n",
      "predicted:  vous etes tres habiles . EOS\n",
      "Eval Score (BLEU): 0.130\n",
      "Epoch: 28, Loss: 0.1759\n",
      "input    :  let s have sushi .\n",
      "label    :  prenons des sushi . EOS\n",
      "predicted:  prenons des sushi . EOS\n",
      "Eval Score (BLEU): 0.154\n",
      "Epoch: 29, Loss: 0.1373\n",
      "input    :  be objective .\n",
      "label    :  soyez objectif . EOS\n",
      "predicted:  soyez objective . EOS\n",
      "Eval Score (BLEU): 0.163\n",
      "Epoch: 30, Loss: 0.1349\n",
      "input    :  he s afraid of dogs .\n",
      "label    :  il a peur des chiens . EOS\n",
      "predicted:  il a peur des chiens . EOS\n",
      "Eval Score (BLEU): 0.155\n",
      "Epoch: 31, Loss: 0.1541\n",
      "input    :  are you here often ?\n",
      "label    :  etes vous souvent ici ? EOS\n",
      "predicted:  etes vous souvent ici ? EOS\n",
      "Eval Score (BLEU): 0.144\n",
      "Epoch: 32, Loss: 0.1348\n",
      "input    :  cows give milk .\n",
      "label    :  les vaches donnent du lait . EOS\n",
      "predicted:  les vaches produisent du lait . EOS\n",
      "Eval Score (BLEU): 0.169\n",
      "Epoch: 33, Loss: 0.1360\n",
      "input    :  i always lose .\n",
      "label    :  je perds toujours . EOS\n",
      "predicted:  je perds toujours . EOS\n",
      "Eval Score (BLEU): 0.180\n",
      "Epoch: 34, Loss: 0.1096\n",
      "input    :  wait here .\n",
      "label    :  attendez la . EOS\n",
      "predicted:  attends la . EOS\n",
      "Eval Score (BLEU): 0.151\n",
      "Epoch: 35, Loss: 0.1043\n",
      "input    :  i want you .\n",
      "label    :  je te veux ! EOS\n",
      "predicted:  je vous veux ! EOS\n",
      "Eval Score (BLEU): 0.171\n",
      "Epoch: 36, Loss: 0.1150\n",
      "input    :  i know you are rich .\n",
      "label    :  je sais que tu es riche . EOS\n",
      "predicted:  je sais que tu es riche . EOS\n",
      "Eval Score (BLEU): 0.155\n",
      "Epoch: 37, Loss: 0.1139\n",
      "input    :  that s impressive .\n",
      "label    :  c est impressionnant . EOS\n",
      "predicted:  c est impressionnant . EOS\n",
      "Eval Score (BLEU): 0.162\n",
      "Epoch: 38, Loss: 0.1092\n",
      "input    :  does that amuse you ?\n",
      "label    :  est ce que ca t amuse ? EOS\n",
      "predicted:  est ce que ca t amuse ? EOS\n",
      "Eval Score (BLEU): 0.163\n",
      "Epoch: 39, Loss: 0.0967\n",
      "input    :  keep off the grass .\n",
      "label    :  ne pas marcher sur la pelouse . EOS\n",
      "predicted:  pelouse interdite . EOS\n",
      "Eval Score (BLEU): 0.163\n",
      "Epoch: 40, Loss: 0.1004\n",
      "input    :  you re very smart .\n",
      "label    :  tu es tres intelligent . EOS\n",
      "predicted:  vous etes tres sages . EOS\n",
      "Eval Score (BLEU): 0.182\n",
      "Epoch: 41, Loss: 0.1054\n",
      "input    :  i know it was you .\n",
      "label    :  je sais que c etait toi . EOS\n",
      "predicted:  je sais que c etait toi . EOS\n",
      "Eval Score (BLEU): 0.174\n",
      "Epoch: 42, Loss: 0.0837\n",
      "input    :  we need to escape .\n",
      "label    :  nous devons nous echapper . EOS\n",
      "predicted:  il nous a nous faut nous . EOS\n",
      "Eval Score (BLEU): 0.163\n",
      "Epoch: 43, Loss: 0.1441\n",
      "input    :  i can now do that .\n",
      "label    :  je peux maintenant faire cela . EOS\n",
      "predicted:  je peux maintenant faire cela . EOS\n",
      "Eval Score (BLEU): 0.163\n",
      "Epoch: 44, Loss: 0.1326\n",
      "input    :  you can t eat here .\n",
      "label    :  tu ne peux pas manger ici . EOS\n",
      "predicted:  vous ne pouvez pas manger ici . EOS\n",
      "Eval Score (BLEU): 0.162\n",
      "Epoch: 45, Loss: 0.1231\n",
      "input    :  maybe you re right .\n",
      "label    :  peut etre avez vous raison . EOS\n",
      "predicted:  peut etre avez vous raison . EOS\n",
      "Eval Score (BLEU): 0.172\n",
      "Epoch: 46, Loss: 0.1305\n",
      "input    :  i m thrilled .\n",
      "label    :  je suis ravi . EOS\n",
      "predicted:  je suis ravi . EOS\n",
      "Eval Score (BLEU): 0.170\n",
      "Epoch: 47, Loss: 0.1195\n",
      "input    :  is this all yours ?\n",
      "label    :  tout ceci est il a vous ? EOS\n",
      "predicted:  tout ceci est il a vous ? EOS\n",
      "Eval Score (BLEU): 0.167\n",
      "Epoch: 48, Loss: 0.1331\n",
      "input    :  there s no food .\n",
      "label    :  il n y a pas de nourriture . EOS\n",
      "predicted:  il n y a aucune nourriture . EOS\n",
      "Eval Score (BLEU): 0.175\n",
      "Epoch: 49, Loss: 0.1278\n",
      "input    :  stop making a fuss .\n",
      "label    :  arrete de faire des chichis ! EOS\n",
      "predicted:  arrete de faire des chichis ! EOS\n",
      "Eval Score (BLEU): 0.170\n",
      "Epoch: 50, Loss: 0.1481\n",
      "input    :  i m afraid not .\n",
      "label    :  j ai bien peur que non . EOS\n",
      "predicted:  je ne crains pas . EOS\n",
      "Eval Score (BLEU): 0.175\n",
      "Scores Maximum: 0.182, Scores Minimum: 0.023, Scores Mean: 0.135, Standard Deviation: 0.042\n"
     ]
    }
   ],
   "source": [
    "#checkpoint_dir = './checkpoint/RNN_seq2seq_b_attn'\n",
    "checkpoint_dir = './checkpoint/RNN_seq2seq_l_attn'\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n",
    "\n",
    "num_epochs = NUM_EPOCHS\n",
    "eval_scores = []\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    encoder_state = encoder.init_state(batch_size)\n",
    "\n",
    "    for batch, data in enumerate(train_dataset):\n",
    "        encoder_in, decoder_in, decoder_out = data\n",
    "        loss = train_step(\n",
    "            encoder_in, decoder_in, decoder_out, encoder_state)\n",
    "    \n",
    "    print(\"Epoch: {}, Loss: {:.4f}\".format(e + 1, loss.numpy()))\n",
    "\n",
    "    if e % 10 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    \n",
    "    predict(encoder, decoder, batch_size, sents_en, data_en,\n",
    "        sents_fr_out, word2idx_fr, idx2word_fr)\n",
    "\n",
    "    eval_score = evaluate_bleu_score(encoder, decoder, test_dataset, word2idx_fr, idx2word_fr)\n",
    "    print(\"Eval Score (BLEU): {:.3f}\".format(eval_score))\n",
    "    eval_scores.append(eval_score)\n",
    "\n",
    "checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "print('Scores Maximum: {:.3f}, Scores Minimum: {:.3f}, Scores Mean: {:.3f}, Standard Deviation: {:.3f}'.format(\n",
    "    np.max(eval_scores), np.min(eval_scores), np.mean(eval_scores), np.std(eval_scores)\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
