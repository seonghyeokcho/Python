{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서플로 1.x 버전의 그래프 코드를 2.x 에서 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder:0' shape=(2,) dtype=float32>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "in_a = tf.placeholder(dtype=tf.float32, shape=(2))\n",
    "in_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'matmul/add:0' shape=(2, 2) dtype=float32>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model(x):\n",
    "    with tf.variable_scope(\"matmul\"):\n",
    "        w = tf.get_variable('w', initializer=tf.ones(shape=(2,2)))\n",
    "        b = tf.get_variable('b', initializer=tf.zeros(shape=(2)))\n",
    "        return x * w + b\n",
    "out_a = model(in_a)\n",
    "out_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    outs = sess.run([out_a],\n",
    "                   feed_dict={in_a: [1,0]})\n",
    "    writer = tf.summary.FileWriter('./logs/example', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[1., 0.],\n",
       "         [1., 0.]], dtype=float32)],\n",
       " <tensorflow.python.summary.writer.writer.FileWriter at 0x7faa7dad3b50>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs, writer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서플로 2.x의 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 오토그래프 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def linear_layer(x):\n",
    "    return 3 * x + 2\n",
    "@tf.function\n",
    "def simple_nn(x):\n",
    "    return tr.nn.relu(linear_layer(x))\n",
    "def simple_function(x):\n",
    "    return 3*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def tf__simple_nn(x):\n",
      "    do_return = False\n",
      "    retval_ = ag__.UndefinedReturnValue()\n",
      "    with ag__.FunctionScope('simple_nn', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\n",
      "        try:\n",
      "            do_return = True\n",
      "            retval_ = fscope.mark_return_value(ag__.converted_call(tr.nn.relu, (ag__.converted_call(linear_layer, (x,), None, fscope),), None, fscope))\n",
      "        except:\n",
      "            do_return = False\n",
      "            raise\n",
      "    (do_return,)\n",
      "    return ag__.retval(retval_)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 자동 생성된 코드의 내부 관찰\n",
    "print(tf.autograph.to_code(simple_nn.python_function, experimental_optional_features=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.def_function.Function at 0x7f9398e79210>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.simple_function(x)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 오토그래프 실행타임(어노테이션 vs none어노테이션)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph_time: 0.48972320099983335\n",
      "auto_graph_time: 0.08006397599979209\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import timeit\n",
    "\n",
    "cell = tf.keras.layers.LSTMCell(100)\n",
    "\n",
    "@tf.function  # 어노테이션\n",
    "def fn(input, state):\n",
    "    return cell(input, state)\n",
    "\n",
    "input = tf.zeros([100,100])\n",
    "state = [tf.zeros([100,100])] * 2\n",
    "\n",
    "# 워밍업\n",
    "cell(input, state)\n",
    "fn(input, state)\n",
    "\n",
    "graph_time = timeit.timeit(lambda: cell(input, state), number=100)\n",
    "auto_graph_time = timeit.timeit(lambda: fn(input, state), number=100)\n",
    "print('graph_time:', graph_time)\n",
    "print('auto_graph_time:', auto_graph_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 순차적 API 모델의 예"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"model.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 함수적 API 모델의 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def build_model():\n",
    "    # 가변 길이 정수의 시퀀스 1\n",
    "    text_input_a = tf.keras.Input(shape=(None,), dtype='int32')\n",
    "    \n",
    "    # 가변 길이 정수의 시퀀스 2\n",
    "    text_input_b = tf.keras.Input(shape=(None,), dtype='int32')\n",
    "    \n",
    "    # 1000개의 고유 단어를 128차원(128개의 컬럼) 벡터에 매핑해서 임베딩\n",
    "    shared_embedding = tf.keras.layers.Embedding(1000, 128)\n",
    "    \n",
    "    # 양쪽 입력을 인코딩하고자 동일한 계층 재사용\n",
    "    encoded_input_a = shared_embedding(text_input_a)\n",
    "    encoded_input_b = shared_embedding(text_input_b)\n",
    "    \n",
    "    # 최종적으로 2개의 로지스틱 예측\n",
    "    prediction_a = tf.keras.layers.Dense(1, activation='sigmoid', name='prediction_a')(encoded_input_a)\n",
    "    prediction_b = tf.keras.layers.Dense(1, activation='sigmoid', name='prediction_b')(encoded_input_b)\n",
    "    \n",
    "    # 이 모델은 2개의 입력과 2개의 출력을 가진다.\n",
    "    # 가운데는 공유 모델이 있다.\n",
    "    model = tf.keras.Model(inputs=[text_input_a, text_input_b],\n",
    "                          outputs=[prediction_a, prediction_b])\n",
    "    tf.keras.utils.plot_model(model, to_file='shared_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='model/shared_model.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 서브클래싱의 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "class Mylayer(layers.Layer):\n",
    "    # __init__ : 선택적으로 이 계층에서 사용할 모든 하위 계층을 정의하는데 사용한다.\n",
    "    # 모델을 선언할 떄의 생성자(constructor)다.\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(Mylayer, self).__init__(**kwargs)\n",
    "    \n",
    "    # build : 계층의 가중치를 생성할 때 사용한다.\n",
    "    # add_weight()로 가중치를 추가 할 수 있다.\n",
    "    def build(self, input_shape):\n",
    "        # 이 계층의 훈련 가능한 가중치 변수 생성\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                     shape=(input_shape[1], self.output_dim),\n",
    "                                     initializer='uniform',\n",
    "                                     trainable=True)\n",
    "    \n",
    "    # call : 순방향 전달을 정의한다.\n",
    "    # 계층이 호출되고 함수 형식으로 체인되는 곳이다.\n",
    "    def call(self, inputs):\n",
    "        # 곱셈 수행과 반환\n",
    "        return tf.matmul(inputs, self.kernel)\n",
    "    \n",
    "    # 선택적으로 get_config()를 사용해 계층을 직렬화(serialize)할 수 있고\n",
    "    # from_config()를 사용하면 역직렬화(deserialize)할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 콜백 코드 작성 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    # 텐서보드 로그를 './logs' 디렉터리에 작성\n",
    "    tf.keras.callbacks.Tensorboard(log_dir='./logs')\n",
    "]\n",
    "model.fit(data, labels, batch_size=BATCH_SIZE, epochs=EPOCH,\n",
    "         callbacks=callbacks, validation_data=(val_data, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델과 가중치 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./my_model') # 가중치를 텐서플로 체크포인트(checkpoint) 파일로 저장\n",
    "model.load_weights(file_path) # 모델 상태 복원"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가중치 이외 모델 json형식으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_string = model.to_json() # 저장\n",
    "model = tf.keras.models.model_from_json(json_string) # 복원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open('model/6.Deeopening_CNN_model_1', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights('model/6.Deeopening_CNN_model_1_weights.h5')\n",
    "\n",
    "path = ''\n",
    "model = tf.keras.models.model_from_json(open(path).read())\n",
    "m.load_weights('model/6.Deeopening_CNN_model_1_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가중치 이외 모델 YAML로 직렬화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_string = model.to_yaml() # 저장\n",
    "model = tf.keras.models.model_from_yaml(yaml_string) # 복원"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델을 가중치와 최적화 매개변수와 함께 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5') # 저장\n",
    "model = tf.keras.models.load_model('mt_model.h5') # 복원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abstract_reasoning', 'aeslc', 'aflw2k3d', 'amazon_us_reviews', 'arc', 'bair_robot_pushing_small', 'beans', 'big_patent', 'bigearthnet', 'billsum', 'binarized_mnist', 'binary_alpha_digits', 'blimp', 'c4', 'caltech101', 'caltech_birds2010', 'caltech_birds2011', 'cars196', 'cassava', 'cats_vs_dogs', 'celeb_a', 'celeb_a_hq', 'cfq', 'chexpert', 'cifar10', 'cifar100', 'cifar10_1', 'cifar10_corrupted', 'citrus_leaves', 'cityscapes', 'civil_comments', 'clevr', 'cmaterdb', 'cnn_dailymail', 'coco', 'coil100', 'colorectal_histology', 'colorectal_histology_large', 'common_voice', 'cos_e', 'crema_d', 'curated_breast_imaging_ddsm', 'cycle_gan', 'deep_weeds', 'definite_pronoun_resolution', 'dementiabank', 'diabetic_retinopathy_detection', 'div2k', 'dmlab', 'downsampled_imagenet', 'dsprites', 'dtd', 'duke_ultrasound', 'emnist', 'eraser_multi_rc', 'esnli', 'eurosat', 'fashion_mnist', 'flic', 'flores', 'food101', 'forest_fires', 'gap', 'geirhos_conflict_stimuli', 'german_credit_numeric', 'gigaword', 'glue', 'groove', 'higgs', 'horses_or_humans', 'i_naturalist2017', 'image_label_folder', 'imagenet2012', 'imagenet2012_corrupted', 'imagenet2012_subset', 'imagenet_resized', 'imagenette', 'imagewang', 'imdb_reviews', 'iris', 'kitti', 'kmnist', 'lfw', 'librispeech', 'librispeech_lm', 'libritts', 'ljspeech', 'lm1b', 'lost_and_found', 'lsun', 'malaria', 'math_dataset', 'mnist', 'mnist_corrupted', 'movie_rationales', 'moving_mnist', 'multi_news', 'multi_nli', 'multi_nli_mismatch', 'natural_questions', 'newsroom', 'nsynth', 'omniglot', 'open_images_challenge2019_detection', 'open_images_v4', 'opinosis', 'oxford_flowers102', 'oxford_iiit_pet', 'para_crawl', 'patch_camelyon', 'pet_finder', 'places365_small', 'plant_leaves', 'plant_village', 'plantae_k', 'qa4mre', 'quickdraw_bitmap', 'reddit', 'reddit_tifu', 'resisc45', 'robonet', 'rock_paper_scissors', 'rock_you', 'samsum', 'savee', 'scan', 'scene_parse150', 'scicite', 'scientific_papers', 'shapes3d', 'smallnorb', 'snli', 'so2sat', 'speech_commands', 'squad', 'stanford_dogs', 'stanford_online_products', 'starcraft_video', 'stl10', 'sun397', 'super_glue', 'svhn_cropped', 'ted_hrlr_translate', 'ted_multi_translate', 'tedlium', 'tf_flowers', 'the300w_lp', 'tiny_shakespeare', 'titanic', 'trivia_qa', 'uc_merced', 'ucf101', 'vgg_face2', 'visual_domain_decathlon', 'voc', 'voxceleb', 'waymo_open_dataset', 'web_questions', 'wider_face', 'wiki40b', 'wikihow', 'wikipedia', 'wmt14_translate', 'wmt15_translate', 'wmt16_translate', 'wmt17_translate', 'wmt18_translate', 'wmt19_translate', 'wmt_t2t_translate', 'wmt_translate', 'xnli', 'xsum', 'yelp_polarity_reviews']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# 등록된 데이터셋 보기\n",
    "builders = tfds.list_builders()\n",
    "print(builders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset mnist/3.0.1 (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /Users/HumanRevolution/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n",
      "local data directory. If you'd instead prefer to read directly from our public\n",
      "GCS bucket (recommended if you're running on GCP), you can instead pass\n",
      "`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5237c724852b4b68a7c925e28e313696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Dl Completed...', max=4.0, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1mDataset mnist downloaded and prepared to /Users/HumanRevolution/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n",
      "tfds.core.DatasetInfo(\n",
      "    name='mnist',\n",
      "    version=3.0.1,\n",
      "    description='The MNIST database of handwritten digits.',\n",
      "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
      "    features=FeaturesDict({\n",
      "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
      "    }),\n",
      "    total_num_examples=70000,\n",
      "    splits={\n",
      "        'test': 10000,\n",
      "        'train': 60000,\n",
      "    },\n",
      "    supervised_keys=('image', 'label'),\n",
      "    citation=\"\"\"@article{lecun2010mnist,\n",
      "      title={MNIST handwritten digit database},\n",
      "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
      "      journal={ATT Labs [Online]. Available: http://yann. lecun. com/exdb/mnist},\n",
      "      volume={2},\n",
      "      year={2010}\n",
      "    }\"\"\",\n",
      "    redistribution_info=,\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data, info = tfds.load('mnist', with_info=True)\n",
    "train_data, test_data = data['train'], data['test']\n",
    "\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy 배열로 데이터셋을 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "<TensorSliceDataset shapes: (), types: tf.int64>\n"
     ]
    }
   ],
   "source": [
    "## 데이터셋은 다운로드하거나 섞거나 배치시키거나 생성자에서 분할할 수 있다.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "num_items = 100\n",
    "num_list = np.arange(num_items)\n",
    "print(num_list)\n",
    "# numpy 배열로 데이터셋 생성\n",
    "num_list_dataset = tf.data.Dataset.from_tensor_slices(num_list)\n",
    "print(num_list_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'test': <PrefetchDataset shapes: ((), ()), types: (tf.string, tf.int64)>,\n",
       "  'train': <PrefetchDataset shapes: ((), ()), types: (tf.string, tf.int64)>,\n",
       "  'unsupervised': <PrefetchDataset shapes: ((), ()), types: (tf.string, tf.int64)>},\n",
       " tfds.core.DatasetInfo(\n",
       "     name='imdb_reviews',\n",
       "     version=1.0.0,\n",
       "     description='Large Movie Review Dataset.\n",
       " This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.',\n",
       "     homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
       "     features=FeaturesDict({\n",
       "         'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
       "         'text': Text(shape=(), dtype=tf.string),\n",
       "     }),\n",
       "     total_num_examples=100000,\n",
       "     splits={\n",
       "         'test': 25000,\n",
       "         'train': 25000,\n",
       "         'unsupervised': 50000,\n",
       "     },\n",
       "     supervised_keys=('text', 'label'),\n",
       "     citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
       "       author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
       "       title     = {Learning Word Vectors for Sentiment Analysis},\n",
       "       booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
       "       month     = {June},\n",
       "       year      = {2011},\n",
       "       address   = {Portland, Oregon, USA},\n",
       "       publisher = {Association for Computational Linguistics},\n",
       "       pages     = {142--150},\n",
       "       url       = {http://www.aclweb.org/anthology/P11-1015}\n",
       "     }\"\"\",\n",
       "     redistribution_info=,\n",
       " ))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)\n",
    "datasets, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "__iter__() is only supported inside of tf.function or when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-6afad2ff335e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    404\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n\u001b[0m\u001b[1;32m    407\u001b[0m                          \"or when eager execution is enabled.\")\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: __iter__() is only supported inside of tf.function or when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "train_dataset = datasets['train']\n",
    "train_dataset = train_dataset.batch(5).shuffle(50).take(2)\n",
    "\n",
    "for data in train_dataset:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "df = pd.read_csv('../project/modeling_2nd/visualization/processedData.csv', encoding='cp949')\n",
    "X = df.drop('transaction_real_price', axis=1)\n",
    "Y = df.transaction_real_price.values\n",
    "train_x,test_x,train_y,test_y = train_test_split(X,Y,test_size=0.2, random_state=42)\n",
    "\n",
    "train_x.shape,train_y.shape,test_x.shape,test_y.shape\n",
    "\n",
    "my_feature_columns = []\n",
    "for key in train_x.keys():\n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n",
    "\n",
    "# 각 은닉층에 10개의 노드가 있으며, 2개의 은닉층을 가진 DNN구축\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns = my_feature_columns,\n",
    "    hidden_units = [10,10],  # 각 10개의 노드를 가진 두 은닉층\n",
    "    n_classes = 3  # 모델은 3개 부류중에서 선택해야 한다.\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def input_fn(mode):\n",
    "    datasets, info = tfds.load(name='mnist',\n",
    "                                with_info=True,\n",
    "                                as_supervised=True)\n",
    "    mnist_dataset = (datasets['train'] if mode == tf.estimator.ModeKeys.TRAIN else\n",
    "                   datasets['test'])\n",
    "\n",
    "    def scale(image, label):\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image /= 255\n",
    "        return image, label\n",
    "\n",
    "    return mnist_dataset.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "test = input_fn('test')\n",
    "train = input_fn(tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "print(test)\n",
    "print(train)\n",
    "\n",
    "tf.estimator.train_and_evaluate(\n",
    "    classifier,\n",
    "    train_spec = tf.estimator.TrainSpec(input_fn=input_fn),\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn=input_fn)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(256.0, shape=(), dtype=float32)\n",
      "tf.Tensor(8.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.constant(4.0)\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "    g.watch(x)\n",
    "    y = x**2\n",
    "    z = y**2\n",
    "dz_dx = g.gradient(z, x)  # 256.0 (4*x^3 as x = 4)\n",
    "dy_dx = g.gradient(y, x)  # 8.0\n",
    "print(dz_dx)\n",
    "print(dy_dx)\n",
    "del g  # 테이프에 대한 참조 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.53553987 0.18084869 0.87275076 ... 0.17509083 0.37267074 0.54288733]\n",
      " [0.34120852 0.42946884 0.7360805  ... 0.39392975 0.22204018 0.9809915 ]\n",
      " [0.60030746 0.23963866 0.6672207  ... 0.8861854  0.82371837 0.13119906]\n",
      " ...\n",
      " [0.50404215 0.31159058 0.08733454 ... 0.5346817  0.8925573  0.6928494 ]\n",
      " [0.8850832  0.62569654 0.5277377  ... 0.9119927  0.1660605  0.9879514 ]\n",
      " [0.5085873  0.26216918 0.7740833  ... 0.3462564  0.63982284 0.76666874]], shape=(1048576, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras as k\n",
    "# n_train_examples = 1024*1024\n",
    "# n_features = 10\n",
    "# size_batches = 256\n",
    "\n",
    "# 반 개구간 [0.0, 1.0)에서 10개의 랜덤 floats\n",
    "x = np.random.random((1024*1024, 10))\n",
    "y = np.random.randint(2, size=(1024*1024, 1))\n",
    "x = tf.dtypes.cast(x, tf.float32)\n",
    "print(x)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "dataset = dataset.shuffle(buffer_size=1024*1024).batch(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1048576, 10]), (1048576, 1))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 10), (None, 1)), types: (tf.float32, tf.int64)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 16)                176       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 193\n",
      "Trainable params: 193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "10/10 [==============================] - 0s 782us/step - loss: 0.6954\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 0s 907us/step - loss: 0.7002\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 0s 843us/step - loss: 0.6956\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.6960\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.6933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe5a7c9ab90>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 분산전략\n",
    "distribution = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# 이 코드 부분이 다중 GPU로 분산된다.\n",
    "with distribution.scope():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(16, activation='relu', input_shape=(10,)))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    optimizer = tf.keras.optimizers.SGD(.2)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    model.summary()\n",
    "    \n",
    "# 평소대로 최적화하지만 실제로 GPU를 사용하고 있다.\n",
    "model.fit(dataset, epochs=5, steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6009 (pid 59118), started 2:24:09 ago. (Use '!kill 59118' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6a8548672e3971ab\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6a8548672e3971ab\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6009;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}