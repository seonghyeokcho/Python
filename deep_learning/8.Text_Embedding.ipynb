{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어 임베딩 - Text Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gensim을 사용한 임베딩 공간 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 이 코드는 tet8 데이터셋에서 Word2Vec 모델을 훈련시키고 이진 파일로 저장한다.\n",
    "# Word2Vec 모델에는 많은 매개변수가 있지만 여기서는 기본 설정 값을 사용한다.\n",
    "# 이 경우 창 크기가 5인(windows=5) CBOW 모델(sg=0)을 훈련하고 100차원 임베딩(size=100)을 생성한다.\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "info = api.info(\"text8\")\n",
    "assert(len(info) > 0)\n",
    "\n",
    "dataset = api.load(\"text8\")\n",
    "model = Word2Vec(dataset)\n",
    "\n",
    "model.save(\"data/text8-word2vec.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load('data/text8-word2vec.bin')\n",
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 어휘의 처음 몇 개 단어를 살펴보고 특정 단어가 있는지 살펴봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "words = word_vectors.vocab.keys()\n",
    "print([x for i, x in enumerate(words) if i < 10])\n",
    "assert(\"king\" in words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- 다음과 같이 특정단어('king')와 유사한 단어를 검색할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_most_similar(word_conf_pairs, k):\n",
    "    for i, (word, conf) in enumerate(word_conf_pairs):\n",
    "        print(\"{:.3f} {:s}\".format(conf, word))\n",
    "        if i >= k-1:\n",
    "            break\n",
    "    if k < len(word_conf_pairs):\n",
    "        print(\"...\")\n",
    "\n",
    "print_most_similar(word_vectors.most_similar(\"king\"), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ex) 프랑스의 수도는 파리다.\n",
    "- ex) 독일의 수도는 베를린이다.\n",
    "- 임베딩 공간에서 파리와 프랑스 사이의 거리가 베를린과 독일 사이의 거리와 같아야 한다.\n",
    "- 다시 말해 프랑스 - 파리 + 베를린 = 독일 이어야 한다.\n",
    "- cf) 단어 앞의 부동소수점 점수는 유사성 척도이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_most_similar(word_vectors.most_similar(\n",
    "positive=['france', 'berlin'], negative=['paris']), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 좀 더 나은 유사도 척도도 gensim API에 구현돼 있다.\n",
    "- 이 유사도값은 코사인 유사도다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_most_similar(word_vectors.most_similar_cosmul(\n",
    "positive=['france', 'berlin'], negative=['paris']), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gensim은 doesnt_match()라는 함수도 제공하는데, 단어의 목록 중 이상한 항목을 탐지하는데 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(word_vectors.doesnt_match(['hindus', 'parsis', 'singapore', 'christians']))\n",
    "\n",
    "# 결과는 예상대로 싱가폴, 종교들을 나열한 목록 중 싱가폴만이 유일한 국가명이기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 두 단어 사이의 유사성을 계산해주는 코드, 여기서는 관련된 단어 사이의 거리가\n",
    "# 관련 없는 단어 사이의 거리보다 더 짧다는 것을 보여준다.\n",
    "for word in ['woman','dog','whale','tree']:\n",
    "    print('similarity({:s}, {:s}) = {:.3f}'.format(\n",
    "    'man', word, word_vectors.similarity('man', word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- similar_by_word() 함수는 기능적으로는 similar() 함수와 같지만,<br>\n",
    "  후자의 경우 기본적으로 비교하기 전에 벡터가 정규화된다.<br>\n",
    "  또한 관련된 similar_by_vector() 함수가 있는데, 벡터를 입력으로 지정해<br>\n",
    "  유사한 단어를 찾을 수 있다.\n",
    "- singapore과 유사한 단어를 찾아봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(print_most_similar(\n",
    "word_vectors.similar_by_word('singapore'), 5))\n",
    "\n",
    "# 최소한 지리적 관점에서는 zambia를 제외하고 모든 대답이 옳은 것처럼 보인다.\n",
    "# 피부색의 관점에서는 모두 유사할수도(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- distance() 함수를 사용하면 임베딩 공간에서 두 단어 사이의 거리도 계산할 수 있다.\n",
    "- 이 값은 사실 단순히 1 - similarity() 와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"distance(singapore, malaysia) = {:.3f}\".format(\n",
    "word_vectors.distance('singapore', 'malaysia')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 어휘 단어 벡터는 다음에서 보듯 word_vectors 객체를 직접 살펴보거나<br>\n",
    "  word_vec() 래퍼를 사용해 찾아볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vec_song = word_vectors['song']\n",
    "print(\"\\n# output vector obtained directly, shape:\", vec_song.shape)\n",
    "vec_song_2 = word_vectors.word_vec('song', use_norm=True)\n",
    "print(\"# output vector obtained using word_vec, shape:\", vec_song_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 워드 임베딩을 사용한 스팸 탐지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다음 코드는 파일을 다운로드하고 파싱해 SMS 메시지 목록과 해당 레이블을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_and_read(url):\n",
    "    local_file = url.split('/')[-1]\n",
    "    p = tf.keras.utils.get_file(local_file, url, extract=True, cache_dir='.')\n",
    "    labels, texts = [],[]\n",
    "    local_file = os.path.join('datasets', 'SMSSpamCollection')\n",
    "    with open(local_file, 'r') as fin:\n",
    "        for line in fin:\n",
    "            label, text = line.strip().split('\\t')\n",
    "            labels.append(1 if label == 'spam' else 0)\n",
    "            texts.append(text)\n",
    "    return texts, labels\n",
    "\n",
    "DATASET_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    "texts, labels = download_and_read(DATASET_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터를 사용하기 위한 준비\n",
    "- SMS 텍스트는 정수 시퀀스로 망에 공급돼야 한다.\n",
    "- 여기서는 케라스 토크나이저를 사용해 각 SMS 텍스트를 단어 시퀀스로 변환한 후<br>\n",
    "  토크나이저의 fit_on_texts() 메소드를 사용해 어휘를 생성한다.<br>\n",
    "  그런 다음 texts_to_sequence()를 사용해 SMS 메세지를 정수 시퀀스로 변환한다.<br>\n",
    "  마지막으로 망은 고정된 길이의 정수 시퀀스와만 작동되므로 pad_sequences() 함수를<br>\n",
    "  호출해 더 짧은 SMS 메세지들은 0으로 채운다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 토큰화하고 텍스트 채우기\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "text_sequences = tokenizer.texts_to_sequences(texts)\n",
    "text_sequences = tf.keras.preprocessing.sequence.pad_sequences(text_sequences)\n",
    "num_records = len(text_sequences)\n",
    "max_seqlen = len(text_sequences[0])\n",
    "print(\"{:d} sentences, max length: {:d}\".format(num_records, max_seqlen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 레이블은 범주형 또는 원 핫 인코딩 형식으로 변환, 사용하려는 손실 함수(범주형 교차 엔트로피)<br>\n",
    "  가 해당 형식의 레이블을 받아들이기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이블\n",
    "NUM_CLASSES = 2\n",
    "cat_labels = tf.keras.utils.to_categorical(labels, num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞서 정의한 tokenizer는 기본적으로 word_index 속성으로 작성된 어휘에 액세스할 수 있게<br>\n",
    "  해주는데, 이는 기본적으로 어휘의 색인 위치에 대한 어휘 단어 사전이다.\n",
    "- 인덱스를 입력하면 단어를 알려주는거 하나, 단어를 입력하면 인덱스를 출력하는거 하나<br>\n",
    "  총 2개의 사전을 만듬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 어휘\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "word2idx['PAD'] = 0\n",
    "idx2word[0] = 'PAD'\n",
    "vocab_size = len(word2idx)\n",
    "print('vocab size: {:d}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 마지막으로 dataset 객체를 만든다. dataset객체를 사용하면 batch size와 같은 일부 속성을<br>\n",
    "  선언적으로 설정할 수 있다. 여기서는 채워진 정수와 범주 레이블 시퀀스에서<br>\n",
    "  데이터셋을 작성하고 데이터를 섞은 다음 이를 훈련, 검증, 테스트 집합으로 분할한다.<br>\n",
    "  그리고 세 가지 데이터셋 각각에 대한 batch size를 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋\n",
    "dataset = tf.data.Dataset.from_tensor_slices((text_sequences, cat_labels))\n",
    "dataset = dataset.shuffle(10000)\n",
    "test_size = num_records // 4  # 5574 / 4 의 정수부분\n",
    "val_size = (num_records - test_size) // 10  # (5574-1393)/10 의 정수부분\n",
    "test_dataset = dataset.take(test_size)\n",
    "val_dataset = dataset.skip(test_size).take(val_size)\n",
    "train_dataset = dataset.skip(test_size + val_size)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 임베딩 행렬 구축\n",
    "- gensim 툴킷은 다양한 훈련된 임베딩 모델에 접근할 수 있게 해주는데,\n",
    "- Word2Vec: 두가지 형식으로 제공, 하나는 구글뉴스(30억개토큰,300만개 단어벡터)로 훈련됐고, 또 하나는 러시아 말뭉치들(word2vec-ruscorpora-300,word2vec-google-news-300)로 훈련됐다.\n",
    "- GloVe: 두가지 형식으로 제공, 하나는 Gigawords 말뭉치(60억개토큰,40만개 단어벡터)로 훈련된 것으로, 50d,100d,200d,300d 벡터로 제공. 다른 하나는 트위터(27억개토큰,120만개 단어벡터)에서 훈련된 것으로 25d, 50d, 100d, 200d 벡터로 제공\n",
    "- fastTest: Wikipedia 2017의 부분 단어 정보로 훈련된 100만 단어벡터, UMBC웹 말뭉치와 statmt.org 뉴스 데이터셋(160억개 토큰)이다.\n",
    "- ConceptNet Numberbatch: ConceptNet 의미 신경망, PPDB(Para Phrase DataBase), Word2Vec의 앙상블을 사용하고 GloVe를 입력으로 한다. 600차원 벡터를 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이 예에서는 Gigaword 말뭉치로 훈련된 300차원 GloVe 임베딩을 선택\n",
    "- 모델 크기를 작게 유지하고자 어휘에 존재하는 단어의 임베딩만을 고려"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_embedding_matrix(sequences, word2idx, embedding_dim, embedding_file):\n",
    "    if os.path.exists(embedding_file):\n",
    "        E = np.load(embedding_file)\n",
    "    else:\n",
    "        vocab_size = len(word2idx)\n",
    "        E = np.zeros((vocab_size, embedding_dim))\n",
    "        word_vectors = api.load(EMBEDDING_MODEL)\n",
    "        for word, idx in word2idx.items():\n",
    "            try:\n",
    "                E[idx] = word_vectors.word_vec(word)\n",
    "            except KeyError:  # 임베딩에 없는 단어\n",
    "                pass\n",
    "            np.save(embedding_file, E)\n",
    "    return E\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "DATA_DIR = 'data'\n",
    "EMBEDDING_NUMPY_FILE = os.path.join(DATA_DIR, \"E.npy\")\n",
    "EMBEDDING_MODEL = 'glove-wiki-gigaword-300'\n",
    "# 임베딩 행렬\n",
    "E = build_embedding_matrix(text_sequences, word2idx, EMBEDDING_DIM,\n",
    "    EMBEDDING_NUMPY_FILE)\n",
    "print('Embedding matrix:', E.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 임베딩 출력 형태(9010, 300)은 어휘의 9010개 토큰과 제3자 GloVe 임베딩의 300개 특징에 해당된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 스팸 분류기 정의\n",
    "- 감정 분석을 위해 1차원 컨볼루션 신경망을 사용함\n",
    "- 입력은 정수 시퀀스\n",
    "- 첫 번째 계층은 임베딩계층, 각 입력정수를 크기가 embedding_dim(300)인 벡터로 변환\n",
    "- @@ 실행모드에 따라, 즉 임베딩을 처음부터 학습시키는지, 전이학습을 하는지, 아니면 미세 조정을 하는지에 따라 신경망의 임베딩 계층은 약간 다르다.\n",
    "- @@ 신경망을 무작위로 초기화된 임베딩 가중치로 시작하고(run_mode == \"scratch\") 훈련 중 가중치를 학습하려면 trainable = True로 설정, 전이학습의 경우(run_mode == \"vectorizer\")에는 임베딩 행렬 E에서 가중치를 설정하지만 trainalbe = False로 설정, 미세조정의 경우(run_mode == \"finetuning\")에는 임베딩 가중치를 외부 행렬 E에서 설정하고 계층도 trainable = True로 설정\n",
    "- 임베딩의 출력은 컨볼루션 계층에 공급\n",
    "- 타임 스텝이라고도 하는 고정된 크기 3토큰너비의 1차원 창(kernel_size=3)은 256개 랜덤필터(num_filters=256)에 대해 컨볼루션돼 256크기의 벡터를 생성\n",
    "- 출력 모양은 (batch_size,time_steps,num_filters)이다.\n",
    "- 단어 임베딩 컨볼루션 계층의 출력은 1차원 공간 드롭아웃 계층으로 전송\n",
    "- @@ 이는 컨볼루션 계층에서 출력된 전체 특징 맵을 임의로 삭제함 이것은 과적합을 방지하는 정규화 기술이다.\n",
    "- 그런 다음 전역 최대풀링 계층을 통해 전송\n",
    "- 이 계층은 각 필터의 각 타임 스텝에서 최댓값을 취하고 (batch_size,num_filters) 형태의 벡터를 생성\n",
    "- 드롭아웃 계층의 출력은 밀집 계층(Dense)으로 공급\n",
    "- (batch_size,num_filters) 형태의 벡터를 (batch_size,num_classes)로 변환\n",
    "- softmax 활성화함수는 각(스팸,햄)에 대한 점수를 확률 분포로 변환, 입력 SMS가 각각 스팸 또는 햄일 확률을 나타냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamClassifierModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_sz, embed_sz, input_length, num_filters, kernel_sz, output_sz, run_mode, embedding_weights, **kwargs):\n",
    "        super(SpamClassifierModel, self).__init__(**kwargs)\n",
    "        if run_mode == 'scratch':\n",
    "            self.embedding = tf.keras.layers.Embedding(vocab_sz,\n",
    "                embed_sz,\n",
    "                input_length=input_length,\n",
    "                trainable=True)\n",
    "        elif run_mode == 'vectorizer':\n",
    "            self.embedding = tf.keras.layers.Embedding(vocab_sz,\n",
    "                embed_sz,\n",
    "                input_length=input_length,\n",
    "                weights=[embedding_weights],\n",
    "                trainable=False)\n",
    "        else:\n",
    "            self.embedding = tf.keras.layers.Embedding(vocab_sz,\n",
    "                embed_sz, input_length=input_length,\n",
    "                weights=[embedding_weights],\n",
    "                trainable=True)\n",
    "        self.conv = tf.keras.layers.Conv1D(filters=num_filters,\n",
    "            kernel_size=kernel_sz,\n",
    "            activation='relu')\n",
    "        self.dropout = tf.keras.layers.SpatialDropout1D(0.2)\n",
    "        self.pool = tf.keras.layers.GlobalMaxPooling1D()\n",
    "        self.dense = tf.keras.layers.Dense(output_sz,\n",
    "            activation='softmax')\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "'''parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--mode\", help=\"run mode\",\n",
    "    choices=[\n",
    "        \"scratch\",\n",
    "        \"vectorizer\",\n",
    "        \"finetuning\"\n",
    "    ])\n",
    "args = parser.parse_args()\n",
    "run_mode = args.mode'''\n",
    "### SystemExit: 2 Error ###\n",
    "\n",
    "\n",
    "# 모델 정의\n",
    "conv_num_filters = 256\n",
    "conv_kernel_size = 3\n",
    "model = SpamClassifierModel(\n",
    "    vocab_size, EMBEDDING_DIM, max_seqlen,\n",
    "    conv_num_filters, conv_kernel_size, NUM_CLASSES,\n",
    "    'finetuning', E)\n",
    "model.build(input_shape=(None, max_seqlen))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- 마지막으로 범주형 교차 엔트로피 손실 함수와 Adam 최적화기를 사용해 모델을 컴파일한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컴파일\n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델의 훈련과 평가\n",
    "- 한 가지 주목할 점은 데이터셋이 다소 불균형하다는 것이다.\n",
    "- 햄 인스턴스가 4827개인 것에 비해 스팸은 단 747개만 있다.\n",
    "- 신경망이 단순히 항상 다부류(이 경우는 햄)로만 예측해도 무려 87%에 가까운 정확도를 달성할 수 있는 셈이다.\n",
    "- 이 문제를 완화하고자 스팸 SMS의 오류가 햄 SMS의 오류보다 8배 더 중요하다는 것을 나타내려고 부류 가중치를 설정함\n",
    "- 이는 CLASSES_WEIGHTS 변수에 표시되며, 추가 매개변수로 model.fit()을 호출할 때 전달됨\n",
    "- 훈련은 3에폭만 훈련, 모델의 정확도와 혼동행렬을 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_0, label_1 = [],[]\n",
    "for i in labels:\n",
    "    if i == 0:\n",
    "        label_0.append(i)\n",
    "    else:\n",
    "        label_1.append(i)\n",
    "\n",
    "print('전체 SMS:',len(labels))\n",
    "print('햄:',len(label_0))\n",
    "print('스팸:',len(label_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 3\n",
    "# 데이터 분포를 보면 햄은 4827, 스팸은 747(전체는 5574)로,\n",
    "# 약 87%가 햄이고 스팸은 단 13%다. 따라서 비율에 따라\n",
    "# 스팸(1) 아이템은 햄(0)보다 약 8배의 가중치를 준다.\n",
    "CLASS_WEIGHTS = {0:1, 1:8}\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(train_dataset, epochs=NUM_EPOCHS,\n",
    "    validation_data=val_dataset,\n",
    "    class_weight=CLASS_WEIGHTS)\n",
    "\n",
    "# 테스트 집합으로 평가\n",
    "pred_labels, predictions = [],[]\n",
    "for Xtest, Ytest in test_dataset:\n",
    "    Ytest_ = model.predict_on_batch(Xtest)\n",
    "    ytest = np.argmax(Ytest, axis=1)\n",
    "    ytest_ = np.argmax(Ytest_, axis=1)\n",
    "    pred_labels.extend(ytest.tolist())\n",
    "    predictions.extend(ytest.tolist())\n",
    "\n",
    "print(\"test accuracy: {:.3f}\".format(accuracy_score(pred_labels,predictions)))\n",
    "print(\"confusion matrix\")\n",
    "print(confusion_matrix(pred_labels,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터셋은 작고 모델은 매우 단순하다. 3에폭이라는 최소한의 훈련만으로도 매우 우수한 결과를 달성할 수 있었다.\n",
    "- 세 경우 모두 신경망은 1094개의 햄 메세지와 186개의 스팸 메세지를 정확하게 예측해 만점을 받았다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 신경망 임베딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 신경망 임베딩 : 단어 임베딩을 단어가 아닌 설정에 적용하는것, 단어 임베딩은 유사한 문맥에서 등장하는 단어들은 서로 비슷한 의미를 갖는 경향이 있다는 분포 가설을 활용, 문맥이란 일반적으로 대상 단어 주위의 고정크기(단어 개수)의 창이다. 신경망 임베딩 아이디어도 매우 유사, 즉 비슷한 맥락에서 발생하는 개체들은 서로 밀접하게 관련되는 경향이 있다는것. 이런 방법은 일반적으로 상황에 의존한다.\n",
    "- Item2vec : 어떤 한 사용자와 유사한 구매 이력이 있는 다른 사용자의 구매를 기반으로 사용자에게 상품을 추천한다는 아이디어이다. 웹 스토어의 항목을 단어로, 아이템셋(시간에 따른 사용자의 순차적 구매 항목)을 문장으로 사용하여 단어 문맥을 도출한다. 예를 들어 슈퍼마켓에서 쇼핑객에게 상품을 추천하는 문제를 생각해볼 때, 슈퍼마켓에서 5000개의 상품을 판매한다고 사정하면 각 상품은 크기가 5000인 희소 원핫 인코딩 벡터로 표현가능. Word2Vec과 유사하게 문맥 창을 적용해 스킵그램 모델을 훈련시키면 가능한 항목 쌍을 예측 가능. 학습된 임베딩 모델은 유사한 항목이 서로 가깝게 위치하도록 밀집 저차원 공간으로 항목을 매핑해 유사한 항목을 추천하는데 사용할 수 있다.\n",
    "- node2vec : 그래프에서 노드의 특징을 학습하는 기법으로 제안됨. 그래프에서 다수의 고정 길이 랜덤워크를 실행함으로써 그래프 구조의 임베딩을 학습. 노드는 단어이고 랜덤워크는 문장이며 이를 통해 단어문맥이 도출된다.\n",
    "- 자신만의 신경망 임베딩을 생성하고자 node2vec과 유사한 모델을 생성함\n",
    "- 자세히 말하자면 NeurIPS 컨퍼런스에 제출된 논문들에 등장하는 단어들 사이의 동시 발생을 활용해 제시한 그래프 기반 임베딩의 조상 격인 DeepWalk에 기반을 둔 모델이다.\n",
    "- 데이터셋은 11463 x 5812 행렬로 단어 개수를 나타내며, 행은 단어 열은 컨퍼런스 논문을 나타냄. 여기서는 논문 그래프는 만드는데, 두 논문 사이의 에지(edge)는 두 논문 모두에 등장하는 단어를 나타낸다. node2vec과 DeepWalk는 모두 무방향과 무가중치 그래프를 가정한다. 한 쌍의 논문 사이의 관계는 양방향이므로 무방향 그래프가 된다. 그러나 에지(edge)는 두 논문 사이의 단어 동시 발생 횟수에 따라 가중치를 갖고 있을 수도 있다. 여기선 0보다 큰 값을 가진 모든 경우에 단순히 유효한 비가중치 에지로 간주함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = './datasets'\n",
    "UCI_DATA_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00371/NIPS_1987-2015.csv'\n",
    "\n",
    "def download_and_read(url):\n",
    "    local_file = url.split('/')[-1]\n",
    "    p = tf.keras.utils.get_file(local_file, url, cache_dir='.')\n",
    "    row_ids, col_ids, data = [],[],[]\n",
    "    rid = 0\n",
    "    f = open(p, 'r')\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"\\\"\\\",\"):\n",
    "            # 헤더\n",
    "            continue\n",
    "        # 현재 행의 0이 아닌 원소를 계산\n",
    "        counts = np.array([int(x) for x in line.split(',')[1:]])\n",
    "        nz_col_ids = np.nonzero(counts)[0]\n",
    "        nz_data = counts[nz_col_ids]\n",
    "        nz_row_ids = np.repeat(rid, len(nz_col_ids))\n",
    "        rid += 1\n",
    "        # 빅 리스트에 데이터를 추가\n",
    "        row_ids.extend(nz_row_ids.tolist())\n",
    "        col_ids.extend(nz_col_ids.tolist())\n",
    "        data.extend(nz_data.tolist())\n",
    "    print(\"{:d} rows read, COMPLETE\".format(rid))\n",
    "    f.close()\n",
    "    TD = csr_matrix((\n",
    "        np.array(data), (\n",
    "            np.array(row_ids), np.array(col_ids)\n",
    "        )\n",
    "    ), shape=(rid, counts.shape[0]))\n",
    "    return TD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 읽고 용어-문서 행렬로 변환\n",
    "TD = download_and_read(UCI_DATA_URL)\n",
    "# 무방향 무가중치 에지 행렬 계산\n",
    "E = TD.T * TD\n",
    "# 이진화\n",
    "E[E > 0] = 1\n",
    "print(E.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다음 코드는 램덤 워크를 수정하고 그 경로를 RANDOM_WALKS_FILE에서 지정한 파일에 저장한다. 이 과정은 매우 느리다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WALKS_PER_VERTEX = 32\n",
    "MAX_PATH_LENGTH = 40\n",
    "RESTART_PROB = 0.15\n",
    "\n",
    "RANDOM_WALKS_FILE = os.path.join(DATA_DIR, 'random-walks.txt')\n",
    "\n",
    "def construct_random_walks(E, n, alpha, l, ofile):\n",
    "    if os.path.exists(ofile):\n",
    "        print('random walks generated already, skipping')\n",
    "        return\n",
    "    \n",
    "    f = open(ofile, 'w')\n",
    "    for i in range(E.shape[0]):  # 각 꼭짓점마다\n",
    "        if i % 100 == 0:\n",
    "            print(\"{:d} random walks generated from {:d} vertices\".format(n * i, i))\n",
    "        if i <= 3273:\n",
    "            continue\n",
    "        for j in range(n):       # n 랜던 워크 구축\n",
    "            curr = i\n",
    "            walk = [curr]\n",
    "            target_nodes = np.nonzero(E[curr])[1]\n",
    "            for k in range(l):   # each of max length l\n",
    "                # 시작할 것인가?\n",
    "                if np.random.random() < alpha and len(walk) > 5:\n",
    "                    break\n",
    "                # 외향 에지를 하나 골라서 워크에 첨가\n",
    "                try:\n",
    "                    curr = np.random.choice(target_nodes)\n",
    "                    walk.append(curr)\n",
    "                    target_nodes = np.nonzero(E[curr])[1]\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            f.write(\"{:s}\\n\".format(\" \".join([str(x) for x in walk])))\n",
    "    \n",
    "    print(\"{:d} random walks generated from {:d} vertices, COMPLETE\".format(n * i, i))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 워크 구성(주의 : 상당히 시간이 걸리는 프로세스!)\n",
    "#construct_random_walks(E, NUM_WALKS_PER_VERTEX, RESTART_PROB, MAX_PATH_LENGTH, RANDOM_WALKS_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random-walks.txt 의 몇 줄을 작성해보면<br>\n",
    "0 1405 4845 754 4391 3524 4282 2357 3922 1667<br>\n",
    "0 1341 456 495 1647 4200 5379 473 2311<br>\n",
    "0 3422 3455 118 4527 2304 772 3659 2852 4515 5135 3439 1273\n",
    "- 이는 단어들의 어휘가 그래프의 모든 노드id인 언어로 작성된 문장처럼 상상할 수 있다.\n",
    "- 단어 임베딩이 언어의 구조를 이용해 단어의 분포 표현을 생성한다는 것을 바탕으로 DeepWalk나 node2vec과 같은 그래프 임베딩은 램덤 워크로 생성된 문장에서 동일한 기능을 수행한다. 이러한 임베딩은 다음에서 보는 것처럼 직접적인 이웃을 넘어서서 그래프의 노드 간 유사성까지 포착할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = './model/neurips_papers_n2v'\n",
    "W2V_MODEL_FILE = os.path.join(MODEL_DIR, 'w2v-neurips-papers.model')\n",
    "\n",
    "class Documents(object):\n",
    "    def __init__(self, input_file):\n",
    "        self.input_file = input_file\n",
    "\n",
    "    def __iter__(self):\n",
    "        with open(self.input_file, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i % 1000 == 0:\n",
    "                    if i % 1000 == 0:\n",
    "                        logging.info(\"{:d} random walks extracted\".format(i))\n",
    "                yield line.strip().split()\n",
    "    \n",
    "def train_word2vec_model(random_walks_file, model_file):\n",
    "    if os.path.exists(model_file):\n",
    "        print('Model file {:s} already present, skipping training'.format(model_file))\n",
    "        return\n",
    "    docs = Documents(random_walks_file)\n",
    "    model = gensim.models.Word2Vec(docs,\n",
    "        size=128,    # 임베딩 벡터 크기\n",
    "        window=10,   # 윈도우 크기(고정된 창)\n",
    "        sg=1,        # 스킵그램 모델\n",
    "        min_count=2,\n",
    "        workers=4\n",
    "    )\n",
    "    model.train(\n",
    "        docs,\n",
    "        total_examples=model.corpus_count,\n",
    "        epochs=50)\n",
    "    model.save(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model file ./model/neurips_papers_n2v/w2v-neurips-papers.model already present, skipping training\n"
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "train_word2vec_model(RANDOM_WALKS_FILE, W2V_MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결과 DeepWalk 모델은 바로 Word2Vec 모델이므로 단어 맥락에서 Word2Vec 으로 할 수 있는 모든 것은 이 모델에서는 꼭짓점의 맥락에서 할 수 있다. 이 모델을 사용해 문서 사이의 유사점을 참아봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(td_matrix, model_file, source_id):\n",
    "    model = gensim.models.Word2Vec.load(model_file).wv\n",
    "    most_similar = model.most_similar(str(source_id))\n",
    "    scores = [x[1] for x in most_similar]\n",
    "    target_ids = [x[0] for x in most_similar]\n",
    "    # 소스의 각 타깃 사이에 상위 10개 코사인 유사성 비교\n",
    "    X = np.repeat(td_matrix[source_id].todense(), 10, axis=0)\n",
    "    Y = td_matrix[target_ids].todense()\n",
    "    cosims = [cosine_similarity(X[i], Y[i])[0, 0] for i in range(10)]\n",
    "    for i in range(10):\n",
    "        print(\"{:d} {:s} {:.3f}\".format(\n",
    "            source_id, target_ids[i], cosims[i], scores[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2020-06-26 15:45:38,811 : INFO : loading Word2Vec object from ./model/neurips_papers_n2v/w2v-neurips-papers.model\n2020-06-26 15:45:38,869 : INFO : loading wv recursively from ./model/neurips_papers_n2v/w2v-neurips-papers.model.wv.* with mmap=None\n2020-06-26 15:45:38,870 : INFO : setting ignored attribute vectors_norm to None\n2020-06-26 15:45:38,870 : INFO : loading vocabulary recursively from ./model/neurips_papers_n2v/w2v-neurips-papers.model.vocabulary.* with mmap=None\n2020-06-26 15:45:38,871 : INFO : loading trainables recursively from ./model/neurips_papers_n2v/w2v-neurips-papers.model.trainables.* with mmap=None\n2020-06-26 15:45:38,872 : INFO : setting ignored attribute cum_table to None\n2020-06-26 15:45:38,872 : INFO : loaded ./model/neurips_papers_n2v/w2v-neurips-papers.model\n2020-06-26 15:45:38,887 : INFO : precomputing L2-norms of word weight vectors\n"
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "cannot perform reduce with flexible type",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-313e2707c064>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msource_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2V_MODEL_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-2bea08f9380b>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(td_matrix, model_file, source_id)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# 소스의 각 타깃 사이에 상위 10개 코사인 유사성 비교\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtd_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msource_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtd_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mcosims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \"\"\"\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Dispatch to specialized methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINT_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m_validate_indices\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    137\u001b[0m                 \u001b[0mrow\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_asindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misintlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m_asindices\u001b[0;34m(self, idx, length)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;31m# Check bounds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mmax_indx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmax_indx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'index (%d) out of range'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmax_indx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     28\u001b[0m def _amax(a, axis=None, out=None, keepdims=False,\n\u001b[1;32m     29\u001b[0m           initial=_NoValue, where=True):\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m def _amin(a, axis=None, out=None, keepdims=False,\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot perform reduce with flexible type"
     ]
    }
   ],
   "source": [
    "source_id = np.random.choice(E.shape[0])\n",
    "evaluate_model(TD, W2V_MODEL_FILE, source_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 동적 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2020-06-27 20:39:35,018 : INFO : Using /var/folders/fs/qjp88bt91vn_8kbs8qkcc_z00000gn/T/tfhub_modules to cache modules.\n2020-06-27 20:39:35,021 : INFO : Downloading TF-Hub Module 'https://tfhub.dev/google/elmo/2'.\n2020-06-27 20:39:57,245 : INFO : Downloading https://tfhub.dev/google/elmo/2: 30.35MB\n2020-06-27 20:40:12,913 : INFO : Downloading https://tfhub.dev/google/elmo/2: 50.35MB\n2020-06-27 20:40:30,714 : INFO : Downloading https://tfhub.dev/google/elmo/2: 60.35MB\n2020-06-27 20:40:50,522 : INFO : Downloading https://tfhub.dev/google/elmo/2: 90.35MB\n2020-06-27 20:41:17,602 : INFO : Downloading https://tfhub.dev/google/elmo/2: 130.35MB\n2020-06-27 20:41:41,592 : INFO : Downloading https://tfhub.dev/google/elmo/2: 170.35MB\n2020-06-27 20:41:59,873 : INFO : Downloading https://tfhub.dev/google/elmo/2: 190.35MB\n2020-06-27 20:42:15,820 : INFO : Downloading https://tfhub.dev/google/elmo/2: 200.35MB\n2020-06-27 20:42:38,888 : INFO : Downloading https://tfhub.dev/google/elmo/2: 230.35MB\n2020-06-27 20:42:55,066 : INFO : Downloading https://tfhub.dev/google/elmo/2: 250.35MB\n2020-06-27 20:43:11,779 : INFO : Downloading https://tfhub.dev/google/elmo/2: 280.35MB\n2020-06-27 20:43:47,017 : INFO : Downloading https://tfhub.dev/google/elmo/2: 310.35MB\n2020-06-27 20:44:07,620 : INFO : Downloading https://tfhub.dev/google/elmo/2: 340.35MB\n2020-06-27 20:44:17,239 : INFO : Downloaded https://tfhub.dev/google/elmo/2, Total size: 357.40MB\n2020-06-27 20:44:17,244 : INFO : Downloaded TF-Hub Module 'https://tfhub.dev/google/elmo/2'.\nINFO:tensorflow:Saver not created because there are no variables in the graph to restore\n2020-06-27 20:44:18,068 : INFO : Saver not created because there are no variables in the graph to restore\n(2, 7, 1024)\n"
    }
   ],
   "source": [
    "# ELMo 동적 임베딩\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "module_url = \"https://tfhub.dev/google/elmo/2\"\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "elmo = hub.Module(module_url, trainable=False)\n",
    "embeddings = elmo([\n",
    "    \"i like green eggs and ham\",\n",
    "    \"would you eat them in a box\"],\n",
    "    signature=\"default\",\n",
    "    as_dict=True\n",
    ")['elmo']\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장과 문단 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2020-06-27 21:00:51,986 : INFO : Downloading TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder/2'.\n2020-06-27 21:01:09,134 : INFO : Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 18.00MB\n2020-06-27 21:01:29,641 : INFO : Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 48.00MB\n2020-06-27 21:01:47,686 : INFO : Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 78.00MB\n2020-06-27 21:02:03,051 : INFO : Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 118.00MB\n2020-06-27 21:02:21,101 : INFO : Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 158.00MB\n2020-06-27 21:02:38,945 : INFO : Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 198.00MB\n2020-06-27 21:02:57,685 : INFO : Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 248.00MB\n2020-06-27 21:03:13,331 : INFO : Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 278.00MB\n2020-06-27 21:03:28,805 : INFO : Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 318.00MB\n2020-06-27 21:03:44,847 : INFO : Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 348.00MB\n2020-06-27 21:04:01,297 : INFO : Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 388.00MB\n2020-06-27 21:04:18,376 : INFO : Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 428.00MB\n2020-06-27 21:04:35,239 : INFO : Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 458.00MB\n2020-06-27 21:04:52,479 : INFO : Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 498.00MB\n2020-06-27 21:05:12,648 : INFO : Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 528.00MB\n2020-06-27 21:05:30,259 : INFO : Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 558.00MB\n2020-06-27 21:05:48,859 : INFO : Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 598.00MB\n2020-06-27 21:06:06,960 : INFO : Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 638.00MB\n2020-06-27 21:06:22,596 : INFO : Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 668.00MB\n2020-06-27 21:06:38,575 : INFO : Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 708.00MB\n2020-06-27 21:06:56,138 : INFO : Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 748.00MB\n2020-06-27 21:07:13,298 : INFO : Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 798.00MB\n2020-06-27 21:07:29,427 : INFO : Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 828.00MB\n2020-06-27 21:07:49,379 : INFO : Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 878.00MB\n2020-06-27 21:08:04,583 : INFO : Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 918.00MB\n2020-06-27 21:08:23,449 : INFO : Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 948.00MB\n2020-06-27 21:08:48,389 : INFO : Downloading https://tfhub.dev/google/universal-sentence-encoder/2: 978.00MB\n2020-06-27 21:08:54,025 : INFO : Downloaded https://tfhub.dev/google/universal-sentence-encoder/2, Total size: 993.27MB\n2020-06-27 21:08:54,027 : INFO : Downloaded TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder/2'.\nINFO:tensorflow:Saver not created because there are no variables in the graph to restore\n2020-06-27 21:08:56,519 : INFO : Saver not created because there are no variables in the graph to restore\n(2, 512)\n"
    }
   ],
   "source": [
    "# Google Universal Sentence Encoder\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\"\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "model = hub.Module(module_url)\n",
    "embeddings = model([\n",
    "    \"i like green eggs and ham\",\n",
    "    \"would you eat them in a box\"\n",
    "])\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run([\n",
    "        tf.compat.v1.global_variables_initializer(),\n",
    "        tf.compat.v1.tables_initializer()\n",
    "    ])\n",
    "    embeddings_value = sess.run(embeddings)\n",
    "\n",
    "print(embeddings_value.shape)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}