{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서플로 2.0 기본 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "w = tf.Variable(tf.ones(shape=(2,2)), name='w')\n",
    "b = tf.Variable(tf.zeros(shape=(2)), name='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'w:0' shape=(2, 2) dtype=float32, numpy=\n",
       "array([[1., 1.],\n",
       "       [1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'b:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[1., 0.],\n",
       "       [1., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tf.function\n",
    "def model(x):\n",
    "    return w * x + b\n",
    "\n",
    "out_a = model([1,0])\n",
    "out_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서플로 2.0 첫번째 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras as k\n",
    "\n",
    "NB_CLASSES = 10\n",
    "RESHAPED = 784\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(k.layers.Dense(NB_CLASSES, input_shape=(RESHAPED,),\n",
    "                            kernel_initializer='zeros', name='dense_layer',\n",
    "                            activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x7f9c0323a3d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예제 필기체 숫자 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras as k\n",
    "\n",
    "## 신경망과 훈련 매개변수 ## \n",
    "# epochs = 200\n",
    "# batch_size = 128\n",
    "# verbose = 1\n",
    "# nb_classes = 10  # 출력 개수 = 숫자의 개수\n",
    "# n_hidden = 128\n",
    "# validation_split = .2  # 검증을 위해 남겨준 훈련 데이터\n",
    "\n",
    "## mnist 데이터셋 로드 ##\n",
    "mnist = k.datasets.mnist\n",
    "(X_train,Y_train), (X_test,Y_test) = mnist.load_data()\n",
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 784), dtype('float32'), dtype('float32'))"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 60000 * 28 * 28 의 값을 60000 * 784 형태로 변환 ##\n",
    "# reshaped = 784\n",
    "\n",
    "X_train = X_train.reshape(60000, 784).astype('float32')\n",
    "X_test = X_test.reshape(10000, 784).astype('float32')\n",
    "X_train.shape, X_test.shape, X_train.dtype, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "## 입력을 [0,1] 사이로 정규화 ##\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_layer (Dense)          (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## 모델 구축 ##\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(k.layers.Dense(10, input_shape=(784,), name='dense_layer', activation='softmax'))  # 활성화함수 : softmax\n",
    "\n",
    "## 모델 요약 ##\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 1.3795 - accuracy: 0.6730 - val_loss: 0.8915 - val_accuracy: 0.8311\n",
      "Epoch 2/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.7895 - accuracy: 0.8307 - val_loss: 0.6543 - val_accuracy: 0.8602\n",
      "Epoch 3/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.6400 - accuracy: 0.8535 - val_loss: 0.5589 - val_accuracy: 0.8718\n",
      "Epoch 4/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.5680 - accuracy: 0.8637 - val_loss: 0.5063 - val_accuracy: 0.8780\n",
      "Epoch 5/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.5242 - accuracy: 0.8701 - val_loss: 0.4726 - val_accuracy: 0.8846\n",
      "Epoch 6/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.4943 - accuracy: 0.8750 - val_loss: 0.4485 - val_accuracy: 0.8888\n",
      "Epoch 7/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.4721 - accuracy: 0.8783 - val_loss: 0.4306 - val_accuracy: 0.8913\n",
      "Epoch 8/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.4549 - accuracy: 0.8820 - val_loss: 0.4164 - val_accuracy: 0.8938\n",
      "Epoch 9/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.4411 - accuracy: 0.8842 - val_loss: 0.4052 - val_accuracy: 0.8956\n",
      "Epoch 10/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.4296 - accuracy: 0.8858 - val_loss: 0.3956 - val_accuracy: 0.8970\n",
      "Epoch 11/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.4200 - accuracy: 0.8879 - val_loss: 0.3875 - val_accuracy: 0.8982\n",
      "Epoch 12/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.4116 - accuracy: 0.8895 - val_loss: 0.3806 - val_accuracy: 0.8994\n",
      "Epoch 13/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.4044 - accuracy: 0.8912 - val_loss: 0.3747 - val_accuracy: 0.9007\n",
      "Epoch 14/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3980 - accuracy: 0.8925 - val_loss: 0.3693 - val_accuracy: 0.9014\n",
      "Epoch 15/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3923 - accuracy: 0.8935 - val_loss: 0.3647 - val_accuracy: 0.9024\n",
      "Epoch 16/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3871 - accuracy: 0.8948 - val_loss: 0.3604 - val_accuracy: 0.9028\n",
      "Epoch 17/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3825 - accuracy: 0.8963 - val_loss: 0.3566 - val_accuracy: 0.9039\n",
      "Epoch 18/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3783 - accuracy: 0.8970 - val_loss: 0.3531 - val_accuracy: 0.9044\n",
      "Epoch 19/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3744 - accuracy: 0.8977 - val_loss: 0.3500 - val_accuracy: 0.9050\n",
      "Epoch 20/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3708 - accuracy: 0.8981 - val_loss: 0.3470 - val_accuracy: 0.9058\n",
      "Epoch 21/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3675 - accuracy: 0.8996 - val_loss: 0.3443 - val_accuracy: 0.9058\n",
      "Epoch 22/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3645 - accuracy: 0.8998 - val_loss: 0.3417 - val_accuracy: 0.9067\n",
      "Epoch 23/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3616 - accuracy: 0.9006 - val_loss: 0.3394 - val_accuracy: 0.9073\n",
      "Epoch 24/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3589 - accuracy: 0.9014 - val_loss: 0.3373 - val_accuracy: 0.9081\n",
      "Epoch 25/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3564 - accuracy: 0.9022 - val_loss: 0.3352 - val_accuracy: 0.9086\n",
      "Epoch 26/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3541 - accuracy: 0.9025 - val_loss: 0.3334 - val_accuracy: 0.9089\n",
      "Epoch 27/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3518 - accuracy: 0.9033 - val_loss: 0.3315 - val_accuracy: 0.9095\n",
      "Epoch 28/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3497 - accuracy: 0.9038 - val_loss: 0.3297 - val_accuracy: 0.9097\n",
      "Epoch 29/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3477 - accuracy: 0.9041 - val_loss: 0.3281 - val_accuracy: 0.9103\n",
      "Epoch 30/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3458 - accuracy: 0.9049 - val_loss: 0.3267 - val_accuracy: 0.9103\n",
      "Epoch 31/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3440 - accuracy: 0.9052 - val_loss: 0.3252 - val_accuracy: 0.9112\n",
      "Epoch 32/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3423 - accuracy: 0.9055 - val_loss: 0.3238 - val_accuracy: 0.9114\n",
      "Epoch 33/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3407 - accuracy: 0.9062 - val_loss: 0.3226 - val_accuracy: 0.9117\n",
      "Epoch 34/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3392 - accuracy: 0.9061 - val_loss: 0.3212 - val_accuracy: 0.9120\n",
      "Epoch 35/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3376 - accuracy: 0.9066 - val_loss: 0.3200 - val_accuracy: 0.9121\n",
      "Epoch 36/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3362 - accuracy: 0.9071 - val_loss: 0.3191 - val_accuracy: 0.9121\n",
      "Epoch 37/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3348 - accuracy: 0.9072 - val_loss: 0.3178 - val_accuracy: 0.9123\n",
      "Epoch 38/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3335 - accuracy: 0.9076 - val_loss: 0.3168 - val_accuracy: 0.9126\n",
      "Epoch 39/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3322 - accuracy: 0.9080 - val_loss: 0.3158 - val_accuracy: 0.9129\n",
      "Epoch 40/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3310 - accuracy: 0.9082 - val_loss: 0.3149 - val_accuracy: 0.9130\n",
      "Epoch 41/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3298 - accuracy: 0.9086 - val_loss: 0.3140 - val_accuracy: 0.9135\n",
      "Epoch 42/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3287 - accuracy: 0.9092 - val_loss: 0.3130 - val_accuracy: 0.9133\n",
      "Epoch 43/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3276 - accuracy: 0.9094 - val_loss: 0.3123 - val_accuracy: 0.9138\n",
      "Epoch 44/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3266 - accuracy: 0.9094 - val_loss: 0.3113 - val_accuracy: 0.9136\n",
      "Epoch 45/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3255 - accuracy: 0.9102 - val_loss: 0.3104 - val_accuracy: 0.9143\n",
      "Epoch 46/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3246 - accuracy: 0.9100 - val_loss: 0.3097 - val_accuracy: 0.9144\n",
      "Epoch 47/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3236 - accuracy: 0.9103 - val_loss: 0.3090 - val_accuracy: 0.9142\n",
      "Epoch 48/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3226 - accuracy: 0.9105 - val_loss: 0.3085 - val_accuracy: 0.9146\n",
      "Epoch 49/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3218 - accuracy: 0.9110 - val_loss: 0.3075 - val_accuracy: 0.9148\n",
      "Epoch 50/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3209 - accuracy: 0.9113 - val_loss: 0.3068 - val_accuracy: 0.9149\n",
      "Epoch 51/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3200 - accuracy: 0.9115 - val_loss: 0.3063 - val_accuracy: 0.9153\n",
      "Epoch 52/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3192 - accuracy: 0.9117 - val_loss: 0.3056 - val_accuracy: 0.9157\n",
      "Epoch 53/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3184 - accuracy: 0.9121 - val_loss: 0.3049 - val_accuracy: 0.9160\n",
      "Epoch 54/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3176 - accuracy: 0.9122 - val_loss: 0.3043 - val_accuracy: 0.9160\n",
      "Epoch 55/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3169 - accuracy: 0.9125 - val_loss: 0.3038 - val_accuracy: 0.9160\n",
      "Epoch 56/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3161 - accuracy: 0.9128 - val_loss: 0.3032 - val_accuracy: 0.9164\n",
      "Epoch 57/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3154 - accuracy: 0.9130 - val_loss: 0.3026 - val_accuracy: 0.9170\n",
      "Epoch 58/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3147 - accuracy: 0.9131 - val_loss: 0.3021 - val_accuracy: 0.9169\n",
      "Epoch 59/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3140 - accuracy: 0.9131 - val_loss: 0.3016 - val_accuracy: 0.9168\n",
      "Epoch 60/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3133 - accuracy: 0.9132 - val_loss: 0.3011 - val_accuracy: 0.9168\n",
      "Epoch 61/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3127 - accuracy: 0.9134 - val_loss: 0.3006 - val_accuracy: 0.9165\n",
      "Epoch 62/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3120 - accuracy: 0.9139 - val_loss: 0.3002 - val_accuracy: 0.9170\n",
      "Epoch 63/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3115 - accuracy: 0.9141 - val_loss: 0.2996 - val_accuracy: 0.9172\n",
      "Epoch 64/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3109 - accuracy: 0.9143 - val_loss: 0.2992 - val_accuracy: 0.9168\n",
      "Epoch 65/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3102 - accuracy: 0.9144 - val_loss: 0.2987 - val_accuracy: 0.9169\n",
      "Epoch 66/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3096 - accuracy: 0.9147 - val_loss: 0.2983 - val_accuracy: 0.9172\n",
      "Epoch 67/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3091 - accuracy: 0.9147 - val_loss: 0.2978 - val_accuracy: 0.9172\n",
      "Epoch 68/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3085 - accuracy: 0.9148 - val_loss: 0.2974 - val_accuracy: 0.9178\n",
      "Epoch 69/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3080 - accuracy: 0.9151 - val_loss: 0.2970 - val_accuracy: 0.9178\n",
      "Epoch 70/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3074 - accuracy: 0.9153 - val_loss: 0.2968 - val_accuracy: 0.9173\n",
      "Epoch 71/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3070 - accuracy: 0.9154 - val_loss: 0.2963 - val_accuracy: 0.9178\n",
      "Epoch 72/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3064 - accuracy: 0.9155 - val_loss: 0.2959 - val_accuracy: 0.9181\n",
      "Epoch 73/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3059 - accuracy: 0.9156 - val_loss: 0.2956 - val_accuracy: 0.9176\n",
      "Epoch 74/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3054 - accuracy: 0.9158 - val_loss: 0.2952 - val_accuracy: 0.9182\n",
      "Epoch 75/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3050 - accuracy: 0.9160 - val_loss: 0.2947 - val_accuracy: 0.9183\n",
      "Epoch 76/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3045 - accuracy: 0.9161 - val_loss: 0.2944 - val_accuracy: 0.9181\n",
      "Epoch 77/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3040 - accuracy: 0.9159 - val_loss: 0.2942 - val_accuracy: 0.9179\n",
      "Epoch 78/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3035 - accuracy: 0.9162 - val_loss: 0.2938 - val_accuracy: 0.9189\n",
      "Epoch 79/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3031 - accuracy: 0.9161 - val_loss: 0.2935 - val_accuracy: 0.9185\n",
      "Epoch 80/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3027 - accuracy: 0.9163 - val_loss: 0.2931 - val_accuracy: 0.9190\n",
      "Epoch 81/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3022 - accuracy: 0.9165 - val_loss: 0.2930 - val_accuracy: 0.9183\n",
      "Epoch 82/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3019 - accuracy: 0.9165 - val_loss: 0.2925 - val_accuracy: 0.9190\n",
      "Epoch 83/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3014 - accuracy: 0.9165 - val_loss: 0.2924 - val_accuracy: 0.9193\n",
      "Epoch 84/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3010 - accuracy: 0.9166 - val_loss: 0.2919 - val_accuracy: 0.9193\n",
      "Epoch 85/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3006 - accuracy: 0.9168 - val_loss: 0.2916 - val_accuracy: 0.9193\n",
      "Epoch 86/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.3002 - accuracy: 0.9168 - val_loss: 0.2914 - val_accuracy: 0.9193\n",
      "Epoch 87/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2998 - accuracy: 0.9171 - val_loss: 0.2911 - val_accuracy: 0.9197\n",
      "Epoch 88/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2994 - accuracy: 0.9170 - val_loss: 0.2909 - val_accuracy: 0.9193\n",
      "Epoch 89/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2990 - accuracy: 0.9172 - val_loss: 0.2905 - val_accuracy: 0.9195\n",
      "Epoch 90/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2987 - accuracy: 0.9171 - val_loss: 0.2903 - val_accuracy: 0.9194\n",
      "Epoch 91/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2983 - accuracy: 0.9172 - val_loss: 0.2900 - val_accuracy: 0.9199\n",
      "Epoch 92/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2980 - accuracy: 0.9174 - val_loss: 0.2898 - val_accuracy: 0.9195\n",
      "Epoch 93/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2976 - accuracy: 0.9175 - val_loss: 0.2895 - val_accuracy: 0.9202\n",
      "Epoch 94/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2973 - accuracy: 0.9174 - val_loss: 0.2893 - val_accuracy: 0.9199\n",
      "Epoch 95/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2969 - accuracy: 0.9180 - val_loss: 0.2890 - val_accuracy: 0.9206\n",
      "Epoch 96/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2966 - accuracy: 0.9179 - val_loss: 0.2887 - val_accuracy: 0.9201\n",
      "Epoch 97/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2963 - accuracy: 0.9179 - val_loss: 0.2886 - val_accuracy: 0.9202\n",
      "Epoch 98/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2960 - accuracy: 0.9182 - val_loss: 0.2883 - val_accuracy: 0.9207\n",
      "Epoch 99/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2956 - accuracy: 0.9182 - val_loss: 0.2880 - val_accuracy: 0.9207\n",
      "Epoch 100/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2953 - accuracy: 0.9180 - val_loss: 0.2879 - val_accuracy: 0.9203\n",
      "Epoch 101/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2950 - accuracy: 0.9181 - val_loss: 0.2877 - val_accuracy: 0.9206\n",
      "Epoch 102/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2946 - accuracy: 0.9184 - val_loss: 0.2874 - val_accuracy: 0.9206\n",
      "Epoch 103/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2944 - accuracy: 0.9185 - val_loss: 0.2872 - val_accuracy: 0.9207\n",
      "Epoch 104/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2940 - accuracy: 0.9184 - val_loss: 0.2870 - val_accuracy: 0.9212\n",
      "Epoch 105/200\n",
      "48000/48000 [==============================] - 1s 10us/sample - loss: 0.2938 - accuracy: 0.9186 - val_loss: 0.2868 - val_accuracy: 0.9209\n",
      "Epoch 106/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2935 - accuracy: 0.9186 - val_loss: 0.2866 - val_accuracy: 0.9213\n",
      "Epoch 107/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2932 - accuracy: 0.9186 - val_loss: 0.2863 - val_accuracy: 0.9212\n",
      "Epoch 108/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2929 - accuracy: 0.9190 - val_loss: 0.2862 - val_accuracy: 0.9212\n",
      "Epoch 109/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2926 - accuracy: 0.9189 - val_loss: 0.2860 - val_accuracy: 0.9212\n",
      "Epoch 110/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2923 - accuracy: 0.9188 - val_loss: 0.2859 - val_accuracy: 0.9212\n",
      "Epoch 111/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2921 - accuracy: 0.9191 - val_loss: 0.2857 - val_accuracy: 0.9210\n",
      "Epoch 112/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2918 - accuracy: 0.9194 - val_loss: 0.2855 - val_accuracy: 0.9208\n",
      "Epoch 113/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2915 - accuracy: 0.9188 - val_loss: 0.2852 - val_accuracy: 0.9216\n",
      "Epoch 114/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2913 - accuracy: 0.9194 - val_loss: 0.2851 - val_accuracy: 0.9212\n",
      "Epoch 115/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2910 - accuracy: 0.9191 - val_loss: 0.2849 - val_accuracy: 0.9212\n",
      "Epoch 116/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2908 - accuracy: 0.9195 - val_loss: 0.2848 - val_accuracy: 0.9212\n",
      "Epoch 117/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2905 - accuracy: 0.9192 - val_loss: 0.2846 - val_accuracy: 0.9211\n",
      "Epoch 118/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2902 - accuracy: 0.9192 - val_loss: 0.2844 - val_accuracy: 0.9213\n",
      "Epoch 119/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2899 - accuracy: 0.9195 - val_loss: 0.2842 - val_accuracy: 0.9218\n",
      "Epoch 120/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2897 - accuracy: 0.9195 - val_loss: 0.2841 - val_accuracy: 0.9218\n",
      "Epoch 121/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2895 - accuracy: 0.9196 - val_loss: 0.2839 - val_accuracy: 0.9216\n",
      "Epoch 122/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2893 - accuracy: 0.9197 - val_loss: 0.2838 - val_accuracy: 0.9217\n",
      "Epoch 123/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2890 - accuracy: 0.9197 - val_loss: 0.2835 - val_accuracy: 0.9212\n",
      "Epoch 124/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2888 - accuracy: 0.9200 - val_loss: 0.2836 - val_accuracy: 0.9214\n",
      "Epoch 125/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2886 - accuracy: 0.9198 - val_loss: 0.2833 - val_accuracy: 0.9215\n",
      "Epoch 126/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2883 - accuracy: 0.9198 - val_loss: 0.2831 - val_accuracy: 0.9218\n",
      "Epoch 127/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2881 - accuracy: 0.9200 - val_loss: 0.2830 - val_accuracy: 0.9214\n",
      "Epoch 128/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2878 - accuracy: 0.9198 - val_loss: 0.2827 - val_accuracy: 0.9218\n",
      "Epoch 129/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2877 - accuracy: 0.9199 - val_loss: 0.2827 - val_accuracy: 0.9215\n",
      "Epoch 130/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2874 - accuracy: 0.9200 - val_loss: 0.2826 - val_accuracy: 0.9217\n",
      "Epoch 131/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2872 - accuracy: 0.9201 - val_loss: 0.2824 - val_accuracy: 0.9214\n",
      "Epoch 132/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2870 - accuracy: 0.9200 - val_loss: 0.2822 - val_accuracy: 0.9217\n",
      "Epoch 133/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2868 - accuracy: 0.9205 - val_loss: 0.2822 - val_accuracy: 0.9217\n",
      "Epoch 134/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2865 - accuracy: 0.9200 - val_loss: 0.2820 - val_accuracy: 0.9218\n",
      "Epoch 135/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2863 - accuracy: 0.9201 - val_loss: 0.2820 - val_accuracy: 0.9218\n",
      "Epoch 136/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2862 - accuracy: 0.9202 - val_loss: 0.2818 - val_accuracy: 0.9220\n",
      "Epoch 137/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2860 - accuracy: 0.9201 - val_loss: 0.2816 - val_accuracy: 0.9219\n",
      "Epoch 138/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2858 - accuracy: 0.9204 - val_loss: 0.2815 - val_accuracy: 0.9212\n",
      "Epoch 139/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2855 - accuracy: 0.9208 - val_loss: 0.2814 - val_accuracy: 0.9223\n",
      "Epoch 140/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2854 - accuracy: 0.9206 - val_loss: 0.2812 - val_accuracy: 0.9219\n",
      "Epoch 141/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2852 - accuracy: 0.9205 - val_loss: 0.2811 - val_accuracy: 0.9216\n",
      "Epoch 142/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2850 - accuracy: 0.9209 - val_loss: 0.2810 - val_accuracy: 0.9214\n",
      "Epoch 143/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2847 - accuracy: 0.9205 - val_loss: 0.2809 - val_accuracy: 0.9213\n",
      "Epoch 144/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2846 - accuracy: 0.9208 - val_loss: 0.2807 - val_accuracy: 0.9214\n",
      "Epoch 145/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2844 - accuracy: 0.9209 - val_loss: 0.2807 - val_accuracy: 0.9217\n",
      "Epoch 146/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2842 - accuracy: 0.9206 - val_loss: 0.2806 - val_accuracy: 0.9218\n",
      "Epoch 147/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2840 - accuracy: 0.9210 - val_loss: 0.2803 - val_accuracy: 0.9222\n",
      "Epoch 148/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2839 - accuracy: 0.9207 - val_loss: 0.2802 - val_accuracy: 0.9220\n",
      "Epoch 149/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2836 - accuracy: 0.9208 - val_loss: 0.2801 - val_accuracy: 0.9221\n",
      "Epoch 150/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2835 - accuracy: 0.9209 - val_loss: 0.2800 - val_accuracy: 0.9215\n",
      "Epoch 151/200\n",
      "48000/48000 [==============================] - 1s 10us/sample - loss: 0.2833 - accuracy: 0.9209 - val_loss: 0.2799 - val_accuracy: 0.9218\n",
      "Epoch 152/200\n",
      "48000/48000 [==============================] - 1s 10us/sample - loss: 0.2832 - accuracy: 0.9209 - val_loss: 0.2797 - val_accuracy: 0.9217\n",
      "Epoch 153/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2829 - accuracy: 0.9209 - val_loss: 0.2797 - val_accuracy: 0.9220\n",
      "Epoch 154/200\n",
      "48000/48000 [==============================] - 1s 10us/sample - loss: 0.2828 - accuracy: 0.9212 - val_loss: 0.2795 - val_accuracy: 0.9226\n",
      "Epoch 155/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2826 - accuracy: 0.9210 - val_loss: 0.2794 - val_accuracy: 0.9224\n",
      "Epoch 156/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2825 - accuracy: 0.9213 - val_loss: 0.2793 - val_accuracy: 0.9226\n",
      "Epoch 157/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2823 - accuracy: 0.9212 - val_loss: 0.2793 - val_accuracy: 0.9218\n",
      "Epoch 158/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2821 - accuracy: 0.9212 - val_loss: 0.2791 - val_accuracy: 0.9222\n",
      "Epoch 159/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2819 - accuracy: 0.9213 - val_loss: 0.2790 - val_accuracy: 0.9224\n",
      "Epoch 160/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2818 - accuracy: 0.9215 - val_loss: 0.2789 - val_accuracy: 0.9222\n",
      "Epoch 161/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2816 - accuracy: 0.9214 - val_loss: 0.2789 - val_accuracy: 0.9224\n",
      "Epoch 162/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2814 - accuracy: 0.9213 - val_loss: 0.2788 - val_accuracy: 0.9225\n",
      "Epoch 163/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2813 - accuracy: 0.9214 - val_loss: 0.2786 - val_accuracy: 0.9224\n",
      "Epoch 164/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2811 - accuracy: 0.9217 - val_loss: 0.2786 - val_accuracy: 0.9224\n",
      "Epoch 165/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2810 - accuracy: 0.9214 - val_loss: 0.2785 - val_accuracy: 0.9223\n",
      "Epoch 166/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2808 - accuracy: 0.9218 - val_loss: 0.2783 - val_accuracy: 0.9222\n",
      "Epoch 167/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2807 - accuracy: 0.9218 - val_loss: 0.2784 - val_accuracy: 0.9226\n",
      "Epoch 168/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2805 - accuracy: 0.9214 - val_loss: 0.2782 - val_accuracy: 0.9226\n",
      "Epoch 169/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2803 - accuracy: 0.9217 - val_loss: 0.2780 - val_accuracy: 0.9226\n",
      "Epoch 170/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2802 - accuracy: 0.9220 - val_loss: 0.2780 - val_accuracy: 0.9224\n",
      "Epoch 171/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2800 - accuracy: 0.9218 - val_loss: 0.2779 - val_accuracy: 0.9225\n",
      "Epoch 172/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2799 - accuracy: 0.9220 - val_loss: 0.2778 - val_accuracy: 0.9226\n",
      "Epoch 173/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2798 - accuracy: 0.9218 - val_loss: 0.2777 - val_accuracy: 0.9223\n",
      "Epoch 174/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2796 - accuracy: 0.9219 - val_loss: 0.2776 - val_accuracy: 0.9223\n",
      "Epoch 175/200\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.2795 - accuracy: 0.9222 - val_loss: 0.2776 - val_accuracy: 0.9228\n",
      "Epoch 176/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2793 - accuracy: 0.9221 - val_loss: 0.2774 - val_accuracy: 0.9227\n",
      "Epoch 177/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2792 - accuracy: 0.9221 - val_loss: 0.2774 - val_accuracy: 0.9227\n",
      "Epoch 178/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2790 - accuracy: 0.9221 - val_loss: 0.2773 - val_accuracy: 0.9227\n",
      "Epoch 179/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2789 - accuracy: 0.9221 - val_loss: 0.2772 - val_accuracy: 0.9226\n",
      "Epoch 180/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2788 - accuracy: 0.9221 - val_loss: 0.2771 - val_accuracy: 0.9227\n",
      "Epoch 181/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2786 - accuracy: 0.9221 - val_loss: 0.2771 - val_accuracy: 0.9230\n",
      "Epoch 182/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2785 - accuracy: 0.9223 - val_loss: 0.2770 - val_accuracy: 0.9225\n",
      "Epoch 183/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2784 - accuracy: 0.9222 - val_loss: 0.2768 - val_accuracy: 0.9229\n",
      "Epoch 184/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2782 - accuracy: 0.9221 - val_loss: 0.2768 - val_accuracy: 0.9225\n",
      "Epoch 185/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2781 - accuracy: 0.9225 - val_loss: 0.2768 - val_accuracy: 0.9232\n",
      "Epoch 186/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2779 - accuracy: 0.9222 - val_loss: 0.2766 - val_accuracy: 0.9228\n",
      "Epoch 187/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2778 - accuracy: 0.9224 - val_loss: 0.2766 - val_accuracy: 0.9227\n",
      "Epoch 188/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2777 - accuracy: 0.9226 - val_loss: 0.2765 - val_accuracy: 0.9230\n",
      "Epoch 189/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2775 - accuracy: 0.9225 - val_loss: 0.2764 - val_accuracy: 0.9229\n",
      "Epoch 190/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2774 - accuracy: 0.9224 - val_loss: 0.2763 - val_accuracy: 0.9230\n",
      "Epoch 191/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2773 - accuracy: 0.9224 - val_loss: 0.2762 - val_accuracy: 0.9230\n",
      "Epoch 192/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2771 - accuracy: 0.9228 - val_loss: 0.2762 - val_accuracy: 0.9232\n",
      "Epoch 193/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2770 - accuracy: 0.9225 - val_loss: 0.2761 - val_accuracy: 0.9231\n",
      "Epoch 194/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2769 - accuracy: 0.9226 - val_loss: 0.2760 - val_accuracy: 0.9232\n",
      "Epoch 195/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2768 - accuracy: 0.9226 - val_loss: 0.2759 - val_accuracy: 0.9231\n",
      "Epoch 196/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2767 - accuracy: 0.9226 - val_loss: 0.2759 - val_accuracy: 0.9235\n",
      "Epoch 197/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2765 - accuracy: 0.9225 - val_loss: 0.2759 - val_accuracy: 0.9229\n",
      "Epoch 198/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2764 - accuracy: 0.9228 - val_loss: 0.2758 - val_accuracy: 0.9235\n",
      "Epoch 199/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2763 - accuracy: 0.9225 - val_loss: 0.2756 - val_accuracy: 0.9234\n",
      "Epoch 200/200\n",
      "48000/48000 [==============================] - 0s 10us/sample - loss: 0.2762 - accuracy: 0.9226 - val_loss: 0.2757 - val_accuracy: 0.9234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd538d108d0>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 모델 컴파일 ##\n",
    "model.compile(optimizer='SGD', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# optimizer : 최적화기(SGD:확률적 그래디언트 하강) / loss : 목적함수 / metics : 척도(정확도,정밀도,재현율 등)\n",
    "\n",
    "## 모델 훈련 ##\n",
    "model.fit(X_train, Y_train,\n",
    "         batch_size=128, epochs=200, verbose=1, validation_split=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 16us/sample - loss: 0.2774 - accuracy: 0.9223\n",
      "Test accuracy: 0.9223\n"
     ]
    }
   ],
   "source": [
    "## 모델 평가 ##\n",
    "test_locc, test_acc = model.evaluate(X_test, Y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model/mnist_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 은닉층 추가로 모델 개선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras as k\n",
    "\n",
    "## 신경망과 훈련 매개변수 ## \n",
    "# epochs = 50\n",
    "# batch_size = 128\n",
    "# verbose = 1\n",
    "# nb_classes = 10  # 출력 개수 = 숫자의 개수\n",
    "# n_hidden = 128\n",
    "# validation_split = .2  # 검증을 위해 남겨준 훈련 데이터\n",
    "\n",
    "## mnist 데이터셋 로드 ##\n",
    "mnist = k.datasets.mnist\n",
    "(X_train,Y_train), (X_test,Y_test) = mnist.load_data()\n",
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 784), dtype('float32'), dtype('float32'))"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 60000 * 28 * 28 의 값을 60000 * 784 형태로 변환 후 [0,1] 사이로 정규화 ##\n",
    "# reshaped = 784\n",
    "\n",
    "X_train = X_train.reshape(60000, 784).astype('float32') / 255\n",
    "X_test = X_test.reshape(10000, 784).astype('float32') / 255\n",
    "X_train.shape, X_test.shape, X_train.dtype, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 10), (10000, 10), dtype('float32'), dtype('float32'))"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 레이블을 원 핫 코드로 표기 ##\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train, 10)\n",
    "Y_test = tf.keras.utils.to_categorical(Y_test, 10)\n",
    "Y_train.shape, Y_test.shape, Y_train.dtype, Y_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dence_layer (Dense)          (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dence_layer_2 (Dense)        (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dence_layer_3 (Dense)        (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## 모델 구축 ##\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(k.layers.Dense(128, input_shape=(784,), name='dence_layer', activation='relu'))\n",
    "model.add(k.layers.Dense(128, name='dence_layer_2', activation='relu'))\n",
    "model.add(k.layers.Dense(10, name='dence_layer_3', activation='softmax'))\n",
    "\n",
    "## 모델 요약 ##\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.4308 - accuracy: 0.6385 - val_loss: 0.7198 - val_accuracy: 0.8449\n",
      "Epoch 2/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.5836 - accuracy: 0.8533 - val_loss: 0.4448 - val_accuracy: 0.8863\n",
      "Epoch 3/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.4320 - accuracy: 0.8833 - val_loss: 0.3672 - val_accuracy: 0.9028\n",
      "Epoch 4/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3740 - accuracy: 0.8959 - val_loss: 0.3314 - val_accuracy: 0.9078\n",
      "Epoch 5/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3419 - accuracy: 0.9035 - val_loss: 0.3091 - val_accuracy: 0.9130\n",
      "Epoch 6/50\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3203 - accuracy: 0.9083 - val_loss: 0.2913 - val_accuracy: 0.9185\n",
      "Epoch 7/50\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3037 - accuracy: 0.9132 - val_loss: 0.2789 - val_accuracy: 0.9218\n",
      "Epoch 8/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.2901 - accuracy: 0.9163 - val_loss: 0.2693 - val_accuracy: 0.9241\n",
      "Epoch 9/50\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.2784 - accuracy: 0.9199 - val_loss: 0.2592 - val_accuracy: 0.9260\n",
      "Epoch 10/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.2679 - accuracy: 0.9222 - val_loss: 0.2525 - val_accuracy: 0.9279\n",
      "Epoch 11/50\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.2584 - accuracy: 0.9250 - val_loss: 0.2436 - val_accuracy: 0.9315\n",
      "Epoch 12/50\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2499 - accuracy: 0.9277 - val_loss: 0.2360 - val_accuracy: 0.9332\n",
      "Epoch 13/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.2418 - accuracy: 0.9306 - val_loss: 0.2296 - val_accuracy: 0.9353\n",
      "Epoch 14/50\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2342 - accuracy: 0.9328 - val_loss: 0.2245 - val_accuracy: 0.9372\n",
      "Epoch 15/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.2270 - accuracy: 0.9352 - val_loss: 0.2181 - val_accuracy: 0.9393\n",
      "Epoch 16/50\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2200 - accuracy: 0.9371 - val_loss: 0.2132 - val_accuracy: 0.9405\n",
      "Epoch 17/50\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2138 - accuracy: 0.9393 - val_loss: 0.2072 - val_accuracy: 0.9432\n",
      "Epoch 18/50\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2075 - accuracy: 0.9406 - val_loss: 0.2031 - val_accuracy: 0.9448\n",
      "Epoch 19/50\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2019 - accuracy: 0.9423 - val_loss: 0.1976 - val_accuracy: 0.9458\n",
      "Epoch 20/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1962 - accuracy: 0.9442 - val_loss: 0.1928 - val_accuracy: 0.9478\n",
      "Epoch 21/50\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.1909 - accuracy: 0.9460 - val_loss: 0.1899 - val_accuracy: 0.9477\n",
      "Epoch 22/50\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1860 - accuracy: 0.9465 - val_loss: 0.1847 - val_accuracy: 0.9490\n",
      "Epoch 23/50\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1812 - accuracy: 0.9481 - val_loss: 0.1827 - val_accuracy: 0.9492\n",
      "Epoch 24/50\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1766 - accuracy: 0.9490 - val_loss: 0.1769 - val_accuracy: 0.9513\n",
      "Epoch 25/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1721 - accuracy: 0.9507 - val_loss: 0.1744 - val_accuracy: 0.9509\n",
      "Epoch 26/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1679 - accuracy: 0.9518 - val_loss: 0.1713 - val_accuracy: 0.9527\n",
      "Epoch 27/50\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.1637 - accuracy: 0.9529 - val_loss: 0.1683 - val_accuracy: 0.9534\n",
      "Epoch 28/50\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1600 - accuracy: 0.9540 - val_loss: 0.1649 - val_accuracy: 0.9547\n",
      "Epoch 29/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1562 - accuracy: 0.9553 - val_loss: 0.1627 - val_accuracy: 0.9548\n",
      "Epoch 30/50\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.1527 - accuracy: 0.9563 - val_loss: 0.1589 - val_accuracy: 0.9551\n",
      "Epoch 31/50\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.1493 - accuracy: 0.9575 - val_loss: 0.1570 - val_accuracy: 0.9560\n",
      "Epoch 32/50\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1460 - accuracy: 0.9583 - val_loss: 0.1549 - val_accuracy: 0.9572\n",
      "Epoch 33/50\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.1427 - accuracy: 0.9594 - val_loss: 0.1518 - val_accuracy: 0.9578\n",
      "Epoch 34/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1395 - accuracy: 0.9604 - val_loss: 0.1516 - val_accuracy: 0.9575\n",
      "Epoch 35/50\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.1366 - accuracy: 0.9612 - val_loss: 0.1478 - val_accuracy: 0.9592\n",
      "Epoch 36/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1336 - accuracy: 0.9621 - val_loss: 0.1478 - val_accuracy: 0.9597\n",
      "Epoch 37/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.1310 - accuracy: 0.9631 - val_loss: 0.1429 - val_accuracy: 0.9607\n",
      "Epoch 38/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1281 - accuracy: 0.9637 - val_loss: 0.1410 - val_accuracy: 0.9617\n",
      "Epoch 39/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1256 - accuracy: 0.9652 - val_loss: 0.1396 - val_accuracy: 0.9617\n",
      "Epoch 40/50\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1230 - accuracy: 0.9652 - val_loss: 0.1383 - val_accuracy: 0.9622\n",
      "Epoch 41/50\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1207 - accuracy: 0.9663 - val_loss: 0.1360 - val_accuracy: 0.9629\n",
      "Epoch 42/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1182 - accuracy: 0.9672 - val_loss: 0.1355 - val_accuracy: 0.9624\n",
      "Epoch 43/50\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1158 - accuracy: 0.9679 - val_loss: 0.1334 - val_accuracy: 0.9632\n",
      "Epoch 44/50\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1137 - accuracy: 0.9682 - val_loss: 0.1316 - val_accuracy: 0.9634\n",
      "Epoch 45/50\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1115 - accuracy: 0.9692 - val_loss: 0.1297 - val_accuracy: 0.9646\n",
      "Epoch 46/50\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1096 - accuracy: 0.9695 - val_loss: 0.1290 - val_accuracy: 0.9643\n",
      "Epoch 47/50\n",
      "48000/48000 [==============================] - 1s 11us/sample - loss: 0.1075 - accuracy: 0.9697 - val_loss: 0.1276 - val_accuracy: 0.9653\n",
      "Epoch 48/50\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1054 - accuracy: 0.9712 - val_loss: 0.1266 - val_accuracy: 0.9651\n",
      "Epoch 49/50\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1035 - accuracy: 0.9718 - val_loss: 0.1260 - val_accuracy: 0.9654\n",
      "Epoch 50/50\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1018 - accuracy: 0.9712 - val_loss: 0.1243 - val_accuracy: 0.9663\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd53a2a4810>"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 모델 컴파일 ##\n",
    "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# optimizer : 최적화기(SGD:확률적 그래디언트 하강) / loss : 목적함수 / metics : 척도(정확도,정밀도,재현율 등)\n",
    "\n",
    "## 모델 훈련 ##\n",
    "model.fit(X_train, Y_train,\n",
    "         batch_size=128, epochs=50, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 22us/sample - loss: 0.1227 - accuracy: 0.9635\n",
      "Test accuracy: 0.9635\n"
     ]
    }
   ],
   "source": [
    "test_locc, test_acc = model.evaluate(X_test, Y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model/mnist_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 드롭아웃으로 단순망 개선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras as k\n",
    "\n",
    "#for tensorboard\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "tensorboard_callback = TensorBoard('.logdir')\n",
    "\n",
    "## 신경망과 훈련 매개변수 ## \n",
    "# epochs = 200\n",
    "# batch_size = 128\n",
    "# verbose = 1\n",
    "# nb_classes = 10  # 출력 개수 = 숫자의 개수\n",
    "# n_hidden = 128\n",
    "# validation_split = .2  # 검증을 위해 남겨준 훈련 데이터\n",
    "# dropout = .3\n",
    "\n",
    "## mnist 데이터셋 로드 ##\n",
    "mnist = k.datasets.mnist\n",
    "(X_train,Y_train), (X_test,Y_test) = mnist.load_data()\n",
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 784), dtype('float32'), dtype('float32'))"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 60000 * 28 * 28 의 값을 60000 * 784 형태로 변환 ##\n",
    "# reshaped = 784\n",
    "\n",
    "X_train = X_train.reshape(60000, 784).astype('float32')\n",
    "X_test = X_test.reshape(10000, 784).astype('float32')\n",
    "X_train.shape, X_test.shape, X_train.dtype, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "## 입력을 [0,1] 사이로 정규화 ##\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 10), (10000, 10), dtype('float32'), dtype('float32'))"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 레이블을 원 핫 코드로 표기 ##\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train, 10)\n",
    "Y_test = tf.keras.utils.to_categorical(Y_test, 10)\n",
    "Y_train.shape, Y_test.shape, Y_train.dtype, Y_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_layer (Dense)          (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer_2 (Dense)        (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer_3 (Dense)        (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## 모델 구축 ##\n",
    "model = tf.keras.Sequential()\n",
    "model.add(k.layers.Dense(128, input_shape=(784,), name='dense_layer', activation='relu'))  # 활성화함수 : relu\n",
    "model.add(k.layers.Dropout(.3))\n",
    "model.add(k.layers.Dense(128, name='dense_layer_2', activation='relu'))\n",
    "model.add(k.layers.Dropout(.3))\n",
    "model.add(k.layers.Dense(10, name='dense_layer_3', activation='softmax'))\n",
    "\n",
    "## 모델 요약 ##\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/200\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 1.6559 - accuracy: 0.4691 - val_loss: 0.8681 - val_accuracy: 0.8285\n",
      "Epoch 2/200\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.8913 - accuracy: 0.7237 - val_loss: 0.5187 - val_accuracy: 0.8727\n",
      "Epoch 3/200\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.6795 - accuracy: 0.7898 - val_loss: 0.4156 - val_accuracy: 0.8917\n",
      "Epoch 4/200\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.5832 - accuracy: 0.8230 - val_loss: 0.3663 - val_accuracy: 0.9013\n",
      "Epoch 5/200\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.5204 - accuracy: 0.8445 - val_loss: 0.3327 - val_accuracy: 0.9071\n",
      "Epoch 6/200\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.4816 - accuracy: 0.8550 - val_loss: 0.3078 - val_accuracy: 0.9122\n",
      "Epoch 7/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.4487 - accuracy: 0.8667 - val_loss: 0.2920 - val_accuracy: 0.9165\n",
      "Epoch 8/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.4230 - accuracy: 0.8750 - val_loss: 0.2770 - val_accuracy: 0.9208\n",
      "Epoch 9/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.3993 - accuracy: 0.8827 - val_loss: 0.2641 - val_accuracy: 0.9233\n",
      "Epoch 10/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3837 - accuracy: 0.8861 - val_loss: 0.2526 - val_accuracy: 0.9258\n",
      "Epoch 11/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.3676 - accuracy: 0.8921 - val_loss: 0.2430 - val_accuracy: 0.9292\n",
      "Epoch 12/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.3541 - accuracy: 0.8967 - val_loss: 0.2344 - val_accuracy: 0.9318\n",
      "Epoch 13/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.3432 - accuracy: 0.8994 - val_loss: 0.2272 - val_accuracy: 0.9337\n",
      "Epoch 14/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3311 - accuracy: 0.9036 - val_loss: 0.2199 - val_accuracy: 0.9343\n",
      "Epoch 15/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.3238 - accuracy: 0.9053 - val_loss: 0.2144 - val_accuracy: 0.9368\n",
      "Epoch 16/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3118 - accuracy: 0.9082 - val_loss: 0.2072 - val_accuracy: 0.9389\n",
      "Epoch 17/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3021 - accuracy: 0.9116 - val_loss: 0.2026 - val_accuracy: 0.9400\n",
      "Epoch 18/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2966 - accuracy: 0.9126 - val_loss: 0.1986 - val_accuracy: 0.9417\n",
      "Epoch 19/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.2871 - accuracy: 0.9175 - val_loss: 0.1925 - val_accuracy: 0.9441\n",
      "Epoch 20/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.2832 - accuracy: 0.9179 - val_loss: 0.1892 - val_accuracy: 0.9448\n",
      "Epoch 21/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.2759 - accuracy: 0.9190 - val_loss: 0.1839 - val_accuracy: 0.9460\n",
      "Epoch 22/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.2682 - accuracy: 0.9209 - val_loss: 0.1815 - val_accuracy: 0.9483\n",
      "Epoch 23/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.2633 - accuracy: 0.9233 - val_loss: 0.1777 - val_accuracy: 0.9503\n",
      "Epoch 24/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2599 - accuracy: 0.9236 - val_loss: 0.1736 - val_accuracy: 0.9504\n",
      "Epoch 25/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.2528 - accuracy: 0.9252 - val_loss: 0.1708 - val_accuracy: 0.9518\n",
      "Epoch 26/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.2501 - accuracy: 0.9266 - val_loss: 0.1681 - val_accuracy: 0.9527\n",
      "Epoch 27/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.2457 - accuracy: 0.9285 - val_loss: 0.1647 - val_accuracy: 0.9540\n",
      "Epoch 28/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.2420 - accuracy: 0.9289 - val_loss: 0.1616 - val_accuracy: 0.9554\n",
      "Epoch 29/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.2387 - accuracy: 0.9303 - val_loss: 0.1592 - val_accuracy: 0.9548\n",
      "Epoch 30/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.2331 - accuracy: 0.9321 - val_loss: 0.1575 - val_accuracy: 0.9555\n",
      "Epoch 31/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.2265 - accuracy: 0.9333 - val_loss: 0.1545 - val_accuracy: 0.9563\n",
      "Epoch 32/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.2236 - accuracy: 0.9346 - val_loss: 0.1533 - val_accuracy: 0.9567\n",
      "Epoch 33/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.2206 - accuracy: 0.9356 - val_loss: 0.1497 - val_accuracy: 0.9573\n",
      "Epoch 34/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2166 - accuracy: 0.9362 - val_loss: 0.1484 - val_accuracy: 0.9578\n",
      "Epoch 35/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.2148 - accuracy: 0.9360 - val_loss: 0.1462 - val_accuracy: 0.9572\n",
      "Epoch 36/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.2109 - accuracy: 0.9382 - val_loss: 0.1448 - val_accuracy: 0.9577\n",
      "Epoch 37/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2076 - accuracy: 0.9388 - val_loss: 0.1429 - val_accuracy: 0.9582\n",
      "Epoch 38/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2054 - accuracy: 0.9394 - val_loss: 0.1412 - val_accuracy: 0.9590\n",
      "Epoch 39/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.2040 - accuracy: 0.9399 - val_loss: 0.1403 - val_accuracy: 0.9593\n",
      "Epoch 40/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1983 - accuracy: 0.9411 - val_loss: 0.1379 - val_accuracy: 0.9602\n",
      "Epoch 41/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1979 - accuracy: 0.9420 - val_loss: 0.1363 - val_accuracy: 0.9597\n",
      "Epoch 42/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1985 - accuracy: 0.9417 - val_loss: 0.1351 - val_accuracy: 0.9603\n",
      "Epoch 43/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1973 - accuracy: 0.9416 - val_loss: 0.1330 - val_accuracy: 0.9607\n",
      "Epoch 44/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1921 - accuracy: 0.9428 - val_loss: 0.1325 - val_accuracy: 0.9608\n",
      "Epoch 45/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1891 - accuracy: 0.9442 - val_loss: 0.1307 - val_accuracy: 0.9616\n",
      "Epoch 46/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1870 - accuracy: 0.9451 - val_loss: 0.1302 - val_accuracy: 0.9615\n",
      "Epoch 47/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1834 - accuracy: 0.9462 - val_loss: 0.1286 - val_accuracy: 0.9623\n",
      "Epoch 48/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1798 - accuracy: 0.9473 - val_loss: 0.1270 - val_accuracy: 0.9622\n",
      "Epoch 49/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.1811 - accuracy: 0.9461 - val_loss: 0.1270 - val_accuracy: 0.9623\n",
      "Epoch 50/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1772 - accuracy: 0.9475 - val_loss: 0.1245 - val_accuracy: 0.9634\n",
      "Epoch 51/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1756 - accuracy: 0.9485 - val_loss: 0.1242 - val_accuracy: 0.9636\n",
      "Epoch 52/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1748 - accuracy: 0.9481 - val_loss: 0.1232 - val_accuracy: 0.9633\n",
      "Epoch 53/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1723 - accuracy: 0.9478 - val_loss: 0.1214 - val_accuracy: 0.9640\n",
      "Epoch 54/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1703 - accuracy: 0.9486 - val_loss: 0.1205 - val_accuracy: 0.9647\n",
      "Epoch 55/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.1695 - accuracy: 0.9499 - val_loss: 0.1195 - val_accuracy: 0.9647\n",
      "Epoch 56/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1666 - accuracy: 0.9506 - val_loss: 0.1188 - val_accuracy: 0.9649\n",
      "Epoch 57/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1666 - accuracy: 0.9500 - val_loss: 0.1181 - val_accuracy: 0.9647\n",
      "Epoch 58/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.1608 - accuracy: 0.9517 - val_loss: 0.1171 - val_accuracy: 0.9657\n",
      "Epoch 59/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1602 - accuracy: 0.9526 - val_loss: 0.1164 - val_accuracy: 0.9655\n",
      "Epoch 60/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1617 - accuracy: 0.9522 - val_loss: 0.1151 - val_accuracy: 0.9660\n",
      "Epoch 61/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1594 - accuracy: 0.9523 - val_loss: 0.1145 - val_accuracy: 0.9661\n",
      "Epoch 62/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1568 - accuracy: 0.9539 - val_loss: 0.1141 - val_accuracy: 0.9666\n",
      "Epoch 63/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1560 - accuracy: 0.9536 - val_loss: 0.1127 - val_accuracy: 0.9667\n",
      "Epoch 64/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1546 - accuracy: 0.9532 - val_loss: 0.1118 - val_accuracy: 0.9671\n",
      "Epoch 65/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1521 - accuracy: 0.9548 - val_loss: 0.1121 - val_accuracy: 0.9668\n",
      "Epoch 66/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1509 - accuracy: 0.9555 - val_loss: 0.1107 - val_accuracy: 0.9672\n",
      "Epoch 67/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1477 - accuracy: 0.9567 - val_loss: 0.1098 - val_accuracy: 0.9673\n",
      "Epoch 68/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1473 - accuracy: 0.9565 - val_loss: 0.1094 - val_accuracy: 0.9672\n",
      "Epoch 69/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.1484 - accuracy: 0.9554 - val_loss: 0.1088 - val_accuracy: 0.9673\n",
      "Epoch 70/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1473 - accuracy: 0.9563 - val_loss: 0.1084 - val_accuracy: 0.9673\n",
      "Epoch 71/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1448 - accuracy: 0.9562 - val_loss: 0.1075 - val_accuracy: 0.9682\n",
      "Epoch 72/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1453 - accuracy: 0.9569 - val_loss: 0.1071 - val_accuracy: 0.9678\n",
      "Epoch 73/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1418 - accuracy: 0.9577 - val_loss: 0.1069 - val_accuracy: 0.9678\n",
      "Epoch 74/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1401 - accuracy: 0.9589 - val_loss: 0.1062 - val_accuracy: 0.9679\n",
      "Epoch 75/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1396 - accuracy: 0.9585 - val_loss: 0.1055 - val_accuracy: 0.9687\n",
      "Epoch 76/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1406 - accuracy: 0.9571 - val_loss: 0.1046 - val_accuracy: 0.9687\n",
      "Epoch 77/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1380 - accuracy: 0.9586 - val_loss: 0.1047 - val_accuracy: 0.9689\n",
      "Epoch 78/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1348 - accuracy: 0.9601 - val_loss: 0.1039 - val_accuracy: 0.9688\n",
      "Epoch 79/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1355 - accuracy: 0.9582 - val_loss: 0.1036 - val_accuracy: 0.9689\n",
      "Epoch 80/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1340 - accuracy: 0.9603 - val_loss: 0.1027 - val_accuracy: 0.9692\n",
      "Epoch 81/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1345 - accuracy: 0.9591 - val_loss: 0.1033 - val_accuracy: 0.9689\n",
      "Epoch 82/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.1333 - accuracy: 0.9602 - val_loss: 0.1026 - val_accuracy: 0.9690\n",
      "Epoch 83/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1324 - accuracy: 0.9611 - val_loss: 0.1020 - val_accuracy: 0.9697\n",
      "Epoch 84/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1337 - accuracy: 0.9602 - val_loss: 0.1010 - val_accuracy: 0.9697\n",
      "Epoch 85/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1327 - accuracy: 0.9609 - val_loss: 0.1008 - val_accuracy: 0.9697\n",
      "Epoch 86/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1274 - accuracy: 0.9616 - val_loss: 0.1006 - val_accuracy: 0.9698\n",
      "Epoch 87/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1283 - accuracy: 0.9617 - val_loss: 0.0998 - val_accuracy: 0.9698\n",
      "Epoch 88/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1303 - accuracy: 0.9612 - val_loss: 0.0994 - val_accuracy: 0.9699\n",
      "Epoch 89/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1245 - accuracy: 0.9630 - val_loss: 0.0991 - val_accuracy: 0.9698\n",
      "Epoch 90/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1249 - accuracy: 0.9624 - val_loss: 0.0988 - val_accuracy: 0.9703\n",
      "Epoch 91/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1246 - accuracy: 0.9626 - val_loss: 0.0986 - val_accuracy: 0.9708\n",
      "Epoch 92/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1247 - accuracy: 0.9628 - val_loss: 0.0984 - val_accuracy: 0.9707\n",
      "Epoch 93/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1232 - accuracy: 0.9631 - val_loss: 0.0975 - val_accuracy: 0.9717\n",
      "Epoch 94/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1243 - accuracy: 0.9628 - val_loss: 0.0979 - val_accuracy: 0.9708\n",
      "Epoch 95/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1235 - accuracy: 0.9629 - val_loss: 0.0967 - val_accuracy: 0.9714\n",
      "Epoch 96/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1204 - accuracy: 0.9643 - val_loss: 0.0968 - val_accuracy: 0.9717\n",
      "Epoch 97/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1195 - accuracy: 0.9648 - val_loss: 0.0964 - val_accuracy: 0.9720\n",
      "Epoch 98/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1199 - accuracy: 0.9641 - val_loss: 0.0961 - val_accuracy: 0.9722\n",
      "Epoch 99/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1196 - accuracy: 0.9643 - val_loss: 0.0960 - val_accuracy: 0.9718\n",
      "Epoch 100/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1173 - accuracy: 0.9646 - val_loss: 0.0962 - val_accuracy: 0.9714\n",
      "Epoch 101/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1161 - accuracy: 0.9651 - val_loss: 0.0954 - val_accuracy: 0.9715\n",
      "Epoch 102/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1154 - accuracy: 0.9658 - val_loss: 0.0959 - val_accuracy: 0.9721\n",
      "Epoch 103/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1183 - accuracy: 0.9634 - val_loss: 0.0951 - val_accuracy: 0.9720\n",
      "Epoch 104/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1176 - accuracy: 0.9641 - val_loss: 0.0945 - val_accuracy: 0.9724\n",
      "Epoch 105/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1140 - accuracy: 0.9655 - val_loss: 0.0940 - val_accuracy: 0.9722\n",
      "Epoch 106/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1167 - accuracy: 0.9654 - val_loss: 0.0937 - val_accuracy: 0.9723\n",
      "Epoch 107/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1124 - accuracy: 0.9664 - val_loss: 0.0943 - val_accuracy: 0.9728\n",
      "Epoch 108/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1117 - accuracy: 0.9665 - val_loss: 0.0931 - val_accuracy: 0.9728\n",
      "Epoch 109/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1122 - accuracy: 0.9658 - val_loss: 0.0932 - val_accuracy: 0.9729\n",
      "Epoch 110/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1131 - accuracy: 0.9661 - val_loss: 0.0930 - val_accuracy: 0.9732\n",
      "Epoch 111/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1106 - accuracy: 0.9664 - val_loss: 0.0930 - val_accuracy: 0.9728\n",
      "Epoch 112/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1105 - accuracy: 0.9669 - val_loss: 0.0929 - val_accuracy: 0.9730\n",
      "Epoch 113/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1102 - accuracy: 0.9668 - val_loss: 0.0925 - val_accuracy: 0.9733\n",
      "Epoch 114/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.1087 - accuracy: 0.9678 - val_loss: 0.0929 - val_accuracy: 0.9732\n",
      "Epoch 115/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1079 - accuracy: 0.9675 - val_loss: 0.0925 - val_accuracy: 0.9732\n",
      "Epoch 116/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1055 - accuracy: 0.9687 - val_loss: 0.0917 - val_accuracy: 0.9733\n",
      "Epoch 117/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1072 - accuracy: 0.9683 - val_loss: 0.0914 - val_accuracy: 0.9737\n",
      "Epoch 118/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1046 - accuracy: 0.9685 - val_loss: 0.0913 - val_accuracy: 0.9737\n",
      "Epoch 119/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1060 - accuracy: 0.9677 - val_loss: 0.0911 - val_accuracy: 0.9735\n",
      "Epoch 120/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1064 - accuracy: 0.9678 - val_loss: 0.0909 - val_accuracy: 0.9733\n",
      "Epoch 121/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1068 - accuracy: 0.9682 - val_loss: 0.0910 - val_accuracy: 0.9732\n",
      "Epoch 122/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1040 - accuracy: 0.9680 - val_loss: 0.0909 - val_accuracy: 0.9732\n",
      "Epoch 123/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1049 - accuracy: 0.9679 - val_loss: 0.0901 - val_accuracy: 0.9734\n",
      "Epoch 124/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1009 - accuracy: 0.9698 - val_loss: 0.0905 - val_accuracy: 0.9731\n",
      "Epoch 125/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1006 - accuracy: 0.9698 - val_loss: 0.0904 - val_accuracy: 0.9725\n",
      "Epoch 126/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.0990 - accuracy: 0.9701 - val_loss: 0.0899 - val_accuracy: 0.9733\n",
      "Epoch 127/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1042 - accuracy: 0.9679 - val_loss: 0.0897 - val_accuracy: 0.9735\n",
      "Epoch 128/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.0985 - accuracy: 0.9696 - val_loss: 0.0901 - val_accuracy: 0.9730\n",
      "Epoch 129/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1005 - accuracy: 0.9700 - val_loss: 0.0896 - val_accuracy: 0.9734\n",
      "Epoch 130/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1004 - accuracy: 0.9694 - val_loss: 0.0895 - val_accuracy: 0.9737\n",
      "Epoch 131/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1007 - accuracy: 0.9694 - val_loss: 0.0886 - val_accuracy: 0.9739\n",
      "Epoch 132/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1002 - accuracy: 0.9695 - val_loss: 0.0892 - val_accuracy: 0.9737\n",
      "Epoch 133/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.0954 - accuracy: 0.9711 - val_loss: 0.0886 - val_accuracy: 0.9740\n",
      "Epoch 134/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0960 - accuracy: 0.9706 - val_loss: 0.0885 - val_accuracy: 0.9738\n",
      "Epoch 135/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.0950 - accuracy: 0.9706 - val_loss: 0.0884 - val_accuracy: 0.9738\n",
      "Epoch 136/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.0959 - accuracy: 0.9709 - val_loss: 0.0885 - val_accuracy: 0.9738\n",
      "Epoch 137/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0948 - accuracy: 0.9707 - val_loss: 0.0878 - val_accuracy: 0.9737\n",
      "Epoch 138/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0939 - accuracy: 0.9717 - val_loss: 0.0882 - val_accuracy: 0.9737\n",
      "Epoch 139/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0934 - accuracy: 0.9712 - val_loss: 0.0878 - val_accuracy: 0.9751\n",
      "Epoch 140/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.0940 - accuracy: 0.9715 - val_loss: 0.0874 - val_accuracy: 0.9745\n",
      "Epoch 141/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0925 - accuracy: 0.9718 - val_loss: 0.0870 - val_accuracy: 0.9747\n",
      "Epoch 142/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0923 - accuracy: 0.9722 - val_loss: 0.0877 - val_accuracy: 0.9743\n",
      "Epoch 143/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.0911 - accuracy: 0.9715 - val_loss: 0.0877 - val_accuracy: 0.9743\n",
      "Epoch 144/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0924 - accuracy: 0.9720 - val_loss: 0.0880 - val_accuracy: 0.9739\n",
      "Epoch 145/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.0926 - accuracy: 0.9714 - val_loss: 0.0871 - val_accuracy: 0.9735\n",
      "Epoch 146/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0906 - accuracy: 0.9724 - val_loss: 0.0870 - val_accuracy: 0.9743\n",
      "Epoch 147/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0927 - accuracy: 0.9714 - val_loss: 0.0870 - val_accuracy: 0.9745\n",
      "Epoch 148/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0924 - accuracy: 0.9719 - val_loss: 0.0867 - val_accuracy: 0.9739\n",
      "Epoch 149/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.0906 - accuracy: 0.9714 - val_loss: 0.0868 - val_accuracy: 0.9743\n",
      "Epoch 150/200\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0890 - accuracy: 0.9733 - val_loss: 0.0860 - val_accuracy: 0.9747\n",
      "Epoch 151/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.0873 - accuracy: 0.9734 - val_loss: 0.0861 - val_accuracy: 0.9751\n",
      "Epoch 152/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.0909 - accuracy: 0.9724 - val_loss: 0.0864 - val_accuracy: 0.9747\n",
      "Epoch 153/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.0901 - accuracy: 0.9727 - val_loss: 0.0857 - val_accuracy: 0.9743\n",
      "Epoch 154/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0900 - accuracy: 0.9720 - val_loss: 0.0862 - val_accuracy: 0.9742\n",
      "Epoch 155/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.0895 - accuracy: 0.9728 - val_loss: 0.0856 - val_accuracy: 0.9743\n",
      "Epoch 156/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0869 - accuracy: 0.9732 - val_loss: 0.0860 - val_accuracy: 0.9744\n",
      "Epoch 157/200\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0886 - accuracy: 0.9728 - val_loss: 0.0859 - val_accuracy: 0.9743\n",
      "Epoch 158/200\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.0865 - accuracy: 0.9729 - val_loss: 0.0860 - val_accuracy: 0.9740\n",
      "Epoch 159/200\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0861 - accuracy: 0.9730 - val_loss: 0.0854 - val_accuracy: 0.9747\n",
      "Epoch 160/200\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0854 - accuracy: 0.9743 - val_loss: 0.0855 - val_accuracy: 0.9744\n",
      "Epoch 161/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.0856 - accuracy: 0.9735 - val_loss: 0.0853 - val_accuracy: 0.9747\n",
      "Epoch 162/200\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0842 - accuracy: 0.9732 - val_loss: 0.0859 - val_accuracy: 0.9746\n",
      "Epoch 163/200\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0865 - accuracy: 0.9738 - val_loss: 0.0859 - val_accuracy: 0.9746\n",
      "Epoch 164/200\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0856 - accuracy: 0.9735 - val_loss: 0.0851 - val_accuracy: 0.9747\n",
      "Epoch 165/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0875 - accuracy: 0.9734 - val_loss: 0.0852 - val_accuracy: 0.9744\n",
      "Epoch 166/200\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0819 - accuracy: 0.9755 - val_loss: 0.0856 - val_accuracy: 0.9745\n",
      "Epoch 167/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.0848 - accuracy: 0.9742 - val_loss: 0.0847 - val_accuracy: 0.9746\n",
      "Epoch 168/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0837 - accuracy: 0.9739 - val_loss: 0.0851 - val_accuracy: 0.9741\n",
      "Epoch 169/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.0834 - accuracy: 0.9745 - val_loss: 0.0849 - val_accuracy: 0.9747\n",
      "Epoch 170/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.0840 - accuracy: 0.9736 - val_loss: 0.0845 - val_accuracy: 0.9749\n",
      "Epoch 171/200\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0806 - accuracy: 0.9757 - val_loss: 0.0847 - val_accuracy: 0.9749\n",
      "Epoch 172/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0824 - accuracy: 0.9749 - val_loss: 0.0843 - val_accuracy: 0.9747\n",
      "Epoch 173/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.0816 - accuracy: 0.9749 - val_loss: 0.0844 - val_accuracy: 0.9746\n",
      "Epoch 174/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.0806 - accuracy: 0.9753 - val_loss: 0.0848 - val_accuracy: 0.9749\n",
      "Epoch 175/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.0809 - accuracy: 0.9759 - val_loss: 0.0844 - val_accuracy: 0.9753\n",
      "Epoch 176/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0818 - accuracy: 0.9750 - val_loss: 0.0839 - val_accuracy: 0.9749\n",
      "Epoch 177/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.0805 - accuracy: 0.9750 - val_loss: 0.0838 - val_accuracy: 0.9751\n",
      "Epoch 178/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0807 - accuracy: 0.9755 - val_loss: 0.0836 - val_accuracy: 0.9751\n",
      "Epoch 179/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.0805 - accuracy: 0.9756 - val_loss: 0.0835 - val_accuracy: 0.9749\n",
      "Epoch 180/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.0804 - accuracy: 0.9744 - val_loss: 0.0840 - val_accuracy: 0.9750\n",
      "Epoch 181/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0788 - accuracy: 0.9756 - val_loss: 0.0839 - val_accuracy: 0.9753\n",
      "Epoch 182/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0779 - accuracy: 0.9759 - val_loss: 0.0840 - val_accuracy: 0.9749\n",
      "Epoch 183/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0786 - accuracy: 0.9760 - val_loss: 0.0834 - val_accuracy: 0.9757\n",
      "Epoch 184/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0780 - accuracy: 0.9760 - val_loss: 0.0834 - val_accuracy: 0.9758\n",
      "Epoch 185/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0776 - accuracy: 0.9761 - val_loss: 0.0833 - val_accuracy: 0.9755\n",
      "Epoch 186/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0765 - accuracy: 0.9760 - val_loss: 0.0834 - val_accuracy: 0.9752\n",
      "Epoch 187/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.0778 - accuracy: 0.9750 - val_loss: 0.0837 - val_accuracy: 0.9755\n",
      "Epoch 188/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0780 - accuracy: 0.9755 - val_loss: 0.0836 - val_accuracy: 0.9751\n",
      "Epoch 189/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0744 - accuracy: 0.9774 - val_loss: 0.0838 - val_accuracy: 0.9755\n",
      "Epoch 190/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0747 - accuracy: 0.9768 - val_loss: 0.0837 - val_accuracy: 0.9757\n",
      "Epoch 191/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0763 - accuracy: 0.9762 - val_loss: 0.0835 - val_accuracy: 0.9757\n",
      "Epoch 192/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0747 - accuracy: 0.9773 - val_loss: 0.0836 - val_accuracy: 0.9755\n",
      "Epoch 193/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0773 - accuracy: 0.9763 - val_loss: 0.0836 - val_accuracy: 0.9755\n",
      "Epoch 194/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0750 - accuracy: 0.9770 - val_loss: 0.0836 - val_accuracy: 0.9753\n",
      "Epoch 195/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0748 - accuracy: 0.9770 - val_loss: 0.0836 - val_accuracy: 0.9756\n",
      "Epoch 196/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0758 - accuracy: 0.9763 - val_loss: 0.0828 - val_accuracy: 0.9758\n",
      "Epoch 197/200\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.0743 - accuracy: 0.9770 - val_loss: 0.0825 - val_accuracy: 0.9764\n",
      "Epoch 198/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0724 - accuracy: 0.9777 - val_loss: 0.0826 - val_accuracy: 0.9755\n",
      "Epoch 199/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0742 - accuracy: 0.9770 - val_loss: 0.0831 - val_accuracy: 0.9755\n",
      "Epoch 200/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0737 - accuracy: 0.9774 - val_loss: 0.0828 - val_accuracy: 0.9754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd53c96b310>"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 모델 컴파일 ##\n",
    "model.compile(optimizer='SGD',  # 최적화기\n",
    "             loss='categorical_crossentropy',  # 목적함수\n",
    "             metrics=['accuracy'])  # 척도\n",
    "\n",
    "## 모델 학습 ##\n",
    "model.fit(X_train, Y_train,\n",
    "         batch_size=128, epochs=200, verbose=1, validation_split=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 23us/sample - loss: 0.0704 - accuracy: 0.9785\n",
      "Test accuracy: 0.9785\n"
     ]
    }
   ],
   "source": [
    "## 모델 평가 ##\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model/mnist_3(Adam).h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 컴파일 수정후 학습(목적함수 RMSProp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 1s 24us/sample - loss: 0.1078 - accuracy: 0.9666 - val_loss: 0.1012 - val_accuracy: 0.9726\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1005 - accuracy: 0.9681 - val_loss: 0.0961 - val_accuracy: 0.9739\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0941 - accuracy: 0.9709 - val_loss: 0.0957 - val_accuracy: 0.9744\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0878 - accuracy: 0.9732 - val_loss: 0.0988 - val_accuracy: 0.9755\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0840 - accuracy: 0.9738 - val_loss: 0.0928 - val_accuracy: 0.9766\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0834 - accuracy: 0.9745 - val_loss: 0.0916 - val_accuracy: 0.9772\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0798 - accuracy: 0.9761 - val_loss: 0.0951 - val_accuracy: 0.9759\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0769 - accuracy: 0.9775 - val_loss: 0.1050 - val_accuracy: 0.9743\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0740 - accuracy: 0.9782 - val_loss: 0.0955 - val_accuracy: 0.9760\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.0718 - accuracy: 0.9779 - val_loss: 0.0958 - val_accuracy: 0.9777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd53beac410>"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 모델 컴파일 ##\n",
    "model.compile(optimizer='RMSProp',  # 최적화기\n",
    "             loss='categorical_crossentropy',  # 목적함수\n",
    "             metrics=['accuracy'])  # 척도\n",
    "\n",
    "## 모델 학습 ##\n",
    "model.fit(X_train, Y_train,\n",
    "         batch_size=128, epochs=10, verbose=1, validation_split=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 23us/sample - loss: 0.0843 - accuracy: 0.9792\n",
      "Test accuracy: 0.9792\n"
     ]
    }
   ],
   "source": [
    "## 모델 평가 ##\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model/mnist_3(RMSProp).h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### epoch 수 증가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/250\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.0741 - accuracy: 0.9772 - val_loss: 0.0995 - val_accuracy: 0.9775\n",
      "Epoch 2/250\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.0671 - accuracy: 0.9792 - val_loss: 0.1024 - val_accuracy: 0.9776\n",
      "Epoch 3/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0696 - accuracy: 0.9790 - val_loss: 0.1012 - val_accuracy: 0.9778\n",
      "Epoch 4/250\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.0676 - accuracy: 0.9806 - val_loss: 0.0980 - val_accuracy: 0.9793\n",
      "Epoch 5/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0655 - accuracy: 0.9803 - val_loss: 0.1112 - val_accuracy: 0.9781\n",
      "Epoch 6/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0632 - accuracy: 0.9807 - val_loss: 0.1097 - val_accuracy: 0.9783\n",
      "Epoch 7/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0628 - accuracy: 0.9812 - val_loss: 0.1129 - val_accuracy: 0.9780\n",
      "Epoch 8/250\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.0653 - accuracy: 0.9806 - val_loss: 0.1143 - val_accuracy: 0.9781\n",
      "Epoch 9/250\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.0609 - accuracy: 0.9821 - val_loss: 0.1161 - val_accuracy: 0.9785\n",
      "Epoch 10/250\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.0595 - accuracy: 0.9829 - val_loss: 0.1128 - val_accuracy: 0.9773\n",
      "Epoch 11/250\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.0586 - accuracy: 0.9827 - val_loss: 0.1179 - val_accuracy: 0.9780\n",
      "Epoch 12/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0601 - accuracy: 0.9818 - val_loss: 0.1160 - val_accuracy: 0.9787\n",
      "Epoch 13/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0559 - accuracy: 0.9830 - val_loss: 0.1263 - val_accuracy: 0.9777\n",
      "Epoch 14/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0565 - accuracy: 0.9834 - val_loss: 0.1255 - val_accuracy: 0.9778\n",
      "Epoch 15/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0552 - accuracy: 0.9847 - val_loss: 0.1317 - val_accuracy: 0.9786\n",
      "Epoch 16/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0610 - accuracy: 0.9831 - val_loss: 0.1248 - val_accuracy: 0.9782\n",
      "Epoch 17/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0570 - accuracy: 0.9841 - val_loss: 0.1336 - val_accuracy: 0.9792\n",
      "Epoch 18/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0559 - accuracy: 0.9835 - val_loss: 0.1253 - val_accuracy: 0.9781\n",
      "Epoch 19/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0560 - accuracy: 0.9841 - val_loss: 0.1278 - val_accuracy: 0.9785\n",
      "Epoch 20/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0546 - accuracy: 0.9844 - val_loss: 0.1414 - val_accuracy: 0.9780\n",
      "Epoch 21/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0547 - accuracy: 0.9846 - val_loss: 0.1334 - val_accuracy: 0.9783\n",
      "Epoch 22/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0530 - accuracy: 0.9846 - val_loss: 0.1295 - val_accuracy: 0.9803\n",
      "Epoch 23/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0543 - accuracy: 0.9850 - val_loss: 0.1401 - val_accuracy: 0.9787\n",
      "Epoch 24/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0548 - accuracy: 0.9848 - val_loss: 0.1452 - val_accuracy: 0.9778\n",
      "Epoch 25/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0573 - accuracy: 0.9842 - val_loss: 0.1321 - val_accuracy: 0.9793\n",
      "Epoch 26/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0508 - accuracy: 0.9856 - val_loss: 0.1352 - val_accuracy: 0.9787\n",
      "Epoch 27/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0545 - accuracy: 0.9851 - val_loss: 0.1363 - val_accuracy: 0.9791\n",
      "Epoch 28/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0548 - accuracy: 0.9848 - val_loss: 0.1434 - val_accuracy: 0.9791\n",
      "Epoch 29/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0539 - accuracy: 0.9854 - val_loss: 0.1512 - val_accuracy: 0.9787\n",
      "Epoch 30/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0515 - accuracy: 0.9858 - val_loss: 0.1483 - val_accuracy: 0.9787\n",
      "Epoch 31/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0510 - accuracy: 0.9860 - val_loss: 0.1577 - val_accuracy: 0.9782\n",
      "Epoch 32/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0547 - accuracy: 0.9858 - val_loss: 0.1589 - val_accuracy: 0.9787\n",
      "Epoch 33/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0516 - accuracy: 0.9859 - val_loss: 0.1710 - val_accuracy: 0.9772\n",
      "Epoch 34/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0545 - accuracy: 0.9848 - val_loss: 0.1558 - val_accuracy: 0.9787\n",
      "Epoch 35/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0514 - accuracy: 0.9865 - val_loss: 0.1539 - val_accuracy: 0.9801\n",
      "Epoch 36/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0551 - accuracy: 0.9860 - val_loss: 0.1550 - val_accuracy: 0.9779\n",
      "Epoch 37/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0525 - accuracy: 0.9858 - val_loss: 0.1623 - val_accuracy: 0.9801\n",
      "Epoch 38/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0524 - accuracy: 0.9870 - val_loss: 0.1473 - val_accuracy: 0.9797\n",
      "Epoch 39/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0512 - accuracy: 0.9865 - val_loss: 0.1533 - val_accuracy: 0.9804\n",
      "Epoch 40/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0507 - accuracy: 0.9863 - val_loss: 0.1636 - val_accuracy: 0.9793\n",
      "Epoch 41/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0506 - accuracy: 0.9873 - val_loss: 0.1582 - val_accuracy: 0.9789\n",
      "Epoch 42/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0520 - accuracy: 0.9862 - val_loss: 0.1577 - val_accuracy: 0.9783\n",
      "Epoch 43/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0522 - accuracy: 0.9869 - val_loss: 0.1682 - val_accuracy: 0.9793\n",
      "Epoch 44/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0503 - accuracy: 0.9869 - val_loss: 0.1802 - val_accuracy: 0.9801\n",
      "Epoch 45/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0529 - accuracy: 0.9862 - val_loss: 0.1710 - val_accuracy: 0.9788\n",
      "Epoch 46/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0529 - accuracy: 0.9864 - val_loss: 0.1624 - val_accuracy: 0.9803\n",
      "Epoch 47/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0487 - accuracy: 0.9871 - val_loss: 0.1850 - val_accuracy: 0.9779\n",
      "Epoch 48/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0525 - accuracy: 0.9870 - val_loss: 0.1721 - val_accuracy: 0.9799\n",
      "Epoch 49/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0523 - accuracy: 0.9864 - val_loss: 0.1683 - val_accuracy: 0.9794\n",
      "Epoch 50/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0530 - accuracy: 0.9876 - val_loss: 0.1737 - val_accuracy: 0.9781\n",
      "Epoch 51/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0507 - accuracy: 0.9869 - val_loss: 0.1784 - val_accuracy: 0.9785\n",
      "Epoch 52/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0511 - accuracy: 0.9868 - val_loss: 0.1685 - val_accuracy: 0.9793\n",
      "Epoch 53/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0498 - accuracy: 0.9878 - val_loss: 0.1777 - val_accuracy: 0.9793\n",
      "Epoch 54/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0510 - accuracy: 0.9871 - val_loss: 0.1756 - val_accuracy: 0.9786\n",
      "Epoch 55/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0534 - accuracy: 0.9863 - val_loss: 0.1891 - val_accuracy: 0.9781\n",
      "Epoch 56/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0480 - accuracy: 0.9878 - val_loss: 0.1851 - val_accuracy: 0.9793\n",
      "Epoch 57/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0490 - accuracy: 0.9873 - val_loss: 0.1899 - val_accuracy: 0.9801\n",
      "Epoch 58/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0494 - accuracy: 0.9872 - val_loss: 0.1742 - val_accuracy: 0.9797\n",
      "Epoch 59/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0488 - accuracy: 0.9877 - val_loss: 0.1930 - val_accuracy: 0.9792\n",
      "Epoch 60/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0458 - accuracy: 0.9883 - val_loss: 0.1978 - val_accuracy: 0.9793\n",
      "Epoch 61/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0499 - accuracy: 0.9879 - val_loss: 0.2109 - val_accuracy: 0.9792\n",
      "Epoch 62/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0490 - accuracy: 0.9874 - val_loss: 0.1939 - val_accuracy: 0.9790\n",
      "Epoch 63/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0509 - accuracy: 0.9879 - val_loss: 0.1801 - val_accuracy: 0.9793\n",
      "Epoch 64/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0505 - accuracy: 0.9875 - val_loss: 0.2010 - val_accuracy: 0.9789\n",
      "Epoch 65/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0443 - accuracy: 0.9889 - val_loss: 0.1985 - val_accuracy: 0.9795\n",
      "Epoch 66/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0494 - accuracy: 0.9881 - val_loss: 0.1956 - val_accuracy: 0.9786\n",
      "Epoch 67/250\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0505 - accuracy: 0.9875 - val_loss: 0.2091 - val_accuracy: 0.9787\n",
      "Epoch 68/250\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.0455 - accuracy: 0.9885 - val_loss: 0.1996 - val_accuracy: 0.9797\n",
      "Epoch 69/250\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.0495 - accuracy: 0.9874 - val_loss: 0.1865 - val_accuracy: 0.9796\n",
      "Epoch 70/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0461 - accuracy: 0.9889 - val_loss: 0.2145 - val_accuracy: 0.9786\n",
      "Epoch 71/250\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.0534 - accuracy: 0.9878 - val_loss: 0.1908 - val_accuracy: 0.9804\n",
      "Epoch 72/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0489 - accuracy: 0.9885 - val_loss: 0.2088 - val_accuracy: 0.9790\n",
      "Epoch 73/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0526 - accuracy: 0.9875 - val_loss: 0.2037 - val_accuracy: 0.9779\n",
      "Epoch 74/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0513 - accuracy: 0.9875 - val_loss: 0.2083 - val_accuracy: 0.9789\n",
      "Epoch 75/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0521 - accuracy: 0.9879 - val_loss: 0.2086 - val_accuracy: 0.9781\n",
      "Epoch 76/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0520 - accuracy: 0.9874 - val_loss: 0.1996 - val_accuracy: 0.9793\n",
      "Epoch 77/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0513 - accuracy: 0.9875 - val_loss: 0.1997 - val_accuracy: 0.9799\n",
      "Epoch 78/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0496 - accuracy: 0.9885 - val_loss: 0.2014 - val_accuracy: 0.9796\n",
      "Epoch 79/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0537 - accuracy: 0.9879 - val_loss: 0.2276 - val_accuracy: 0.9778\n",
      "Epoch 80/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0516 - accuracy: 0.9879 - val_loss: 0.2041 - val_accuracy: 0.9802\n",
      "Epoch 81/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0507 - accuracy: 0.9884 - val_loss: 0.2158 - val_accuracy: 0.9794\n",
      "Epoch 82/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0534 - accuracy: 0.9879 - val_loss: 0.2196 - val_accuracy: 0.9797\n",
      "Epoch 83/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0503 - accuracy: 0.9881 - val_loss: 0.2196 - val_accuracy: 0.9785\n",
      "Epoch 84/250\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0524 - accuracy: 0.9878 - val_loss: 0.2176 - val_accuracy: 0.9792\n",
      "Epoch 85/250\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.0453 - accuracy: 0.9888 - val_loss: 0.2307 - val_accuracy: 0.9786\n",
      "Epoch 86/250\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0515 - accuracy: 0.9879 - val_loss: 0.2324 - val_accuracy: 0.9782\n",
      "Epoch 87/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0482 - accuracy: 0.9877 - val_loss: 0.2302 - val_accuracy: 0.9792\n",
      "Epoch 88/250\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0573 - accuracy: 0.9881 - val_loss: 0.2404 - val_accuracy: 0.9768\n",
      "Epoch 89/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0510 - accuracy: 0.9877 - val_loss: 0.2327 - val_accuracy: 0.9795\n",
      "Epoch 90/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0474 - accuracy: 0.9886 - val_loss: 0.2400 - val_accuracy: 0.9769\n",
      "Epoch 91/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0519 - accuracy: 0.9880 - val_loss: 0.2274 - val_accuracy: 0.9785\n",
      "Epoch 92/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0500 - accuracy: 0.9882 - val_loss: 0.2205 - val_accuracy: 0.9788\n",
      "Epoch 93/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0490 - accuracy: 0.9890 - val_loss: 0.2348 - val_accuracy: 0.9788\n",
      "Epoch 94/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0539 - accuracy: 0.9884 - val_loss: 0.2192 - val_accuracy: 0.9786\n",
      "Epoch 95/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0533 - accuracy: 0.9881 - val_loss: 0.2425 - val_accuracy: 0.9781\n",
      "Epoch 96/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0534 - accuracy: 0.9884 - val_loss: 0.2323 - val_accuracy: 0.9792\n",
      "Epoch 97/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0511 - accuracy: 0.9889 - val_loss: 0.2339 - val_accuracy: 0.9792\n",
      "Epoch 98/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0496 - accuracy: 0.9890 - val_loss: 0.2333 - val_accuracy: 0.9793\n",
      "Epoch 99/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0505 - accuracy: 0.9884 - val_loss: 0.2428 - val_accuracy: 0.9787\n",
      "Epoch 100/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0555 - accuracy: 0.9884 - val_loss: 0.2477 - val_accuracy: 0.9788\n",
      "Epoch 101/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0530 - accuracy: 0.9882 - val_loss: 0.2396 - val_accuracy: 0.9779\n",
      "Epoch 102/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0483 - accuracy: 0.9888 - val_loss: 0.2407 - val_accuracy: 0.9790\n",
      "Epoch 103/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0509 - accuracy: 0.9882 - val_loss: 0.2512 - val_accuracy: 0.9786\n",
      "Epoch 104/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0514 - accuracy: 0.9887 - val_loss: 0.2366 - val_accuracy: 0.9784\n",
      "Epoch 105/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0481 - accuracy: 0.9888 - val_loss: 0.2475 - val_accuracy: 0.9783\n",
      "Epoch 106/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0526 - accuracy: 0.9885 - val_loss: 0.2412 - val_accuracy: 0.9787\n",
      "Epoch 107/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0531 - accuracy: 0.9880 - val_loss: 0.2370 - val_accuracy: 0.9779\n",
      "Epoch 108/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0516 - accuracy: 0.9895 - val_loss: 0.2395 - val_accuracy: 0.9789\n",
      "Epoch 109/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0523 - accuracy: 0.9885 - val_loss: 0.2337 - val_accuracy: 0.9784\n",
      "Epoch 110/250\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 0.0428 - accuracy: 0.9900 - val_loss: 0.2504 - val_accuracy: 0.9798\n",
      "Epoch 111/250\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.0470 - accuracy: 0.9893 - val_loss: 0.2405 - val_accuracy: 0.9793\n",
      "Epoch 112/250\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0515 - accuracy: 0.9891 - val_loss: 0.2465 - val_accuracy: 0.9807\n",
      "Epoch 113/250\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.0502 - accuracy: 0.9887 - val_loss: 0.2366 - val_accuracy: 0.9793\n",
      "Epoch 114/250\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.0541 - accuracy: 0.9881 - val_loss: 0.2515 - val_accuracy: 0.9784\n",
      "Epoch 115/250\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.0485 - accuracy: 0.9888 - val_loss: 0.2615 - val_accuracy: 0.9776\n",
      "Epoch 116/250\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.0556 - accuracy: 0.9881 - val_loss: 0.2489 - val_accuracy: 0.9774\n",
      "Epoch 117/250\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.0557 - accuracy: 0.9881 - val_loss: 0.2468 - val_accuracy: 0.9775\n",
      "Epoch 118/250\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.0476 - accuracy: 0.9887 - val_loss: 0.2498 - val_accuracy: 0.9789\n",
      "Epoch 119/250\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.0504 - accuracy: 0.9896 - val_loss: 0.2660 - val_accuracy: 0.9784\n",
      "Epoch 120/250\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.0492 - accuracy: 0.9896 - val_loss: 0.2513 - val_accuracy: 0.9771\n",
      "Epoch 121/250\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.0490 - accuracy: 0.9884 - val_loss: 0.2710 - val_accuracy: 0.9793\n",
      "Epoch 122/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0553 - accuracy: 0.9881 - val_loss: 0.2481 - val_accuracy: 0.9775\n",
      "Epoch 123/250\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.0527 - accuracy: 0.9883 - val_loss: 0.2755 - val_accuracy: 0.9772\n",
      "Epoch 124/250\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.0453 - accuracy: 0.9894 - val_loss: 0.2529 - val_accuracy: 0.9784\n",
      "Epoch 125/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0527 - accuracy: 0.9883 - val_loss: 0.2605 - val_accuracy: 0.9793\n",
      "Epoch 126/250\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.0534 - accuracy: 0.9894 - val_loss: 0.2717 - val_accuracy: 0.9764\n",
      "Epoch 127/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0486 - accuracy: 0.9893 - val_loss: 0.2782 - val_accuracy: 0.9787\n",
      "Epoch 128/250\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0513 - accuracy: 0.9889 - val_loss: 0.2779 - val_accuracy: 0.9785\n",
      "Epoch 129/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0562 - accuracy: 0.9885 - val_loss: 0.2540 - val_accuracy: 0.9786\n",
      "Epoch 130/250\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.0550 - accuracy: 0.9893 - val_loss: 0.2710 - val_accuracy: 0.9780\n",
      "Epoch 131/250\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.0524 - accuracy: 0.9894 - val_loss: 0.2623 - val_accuracy: 0.9792\n",
      "Epoch 132/250\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0540 - accuracy: 0.9886 - val_loss: 0.2861 - val_accuracy: 0.9773\n",
      "Epoch 133/250\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.0531 - accuracy: 0.9888 - val_loss: 0.2645 - val_accuracy: 0.9785\n",
      "Epoch 134/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0535 - accuracy: 0.9887 - val_loss: 0.2807 - val_accuracy: 0.9777\n",
      "Epoch 135/250\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.0561 - accuracy: 0.9886 - val_loss: 0.2655 - val_accuracy: 0.9788\n",
      "Epoch 136/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0540 - accuracy: 0.9889 - val_loss: 0.2682 - val_accuracy: 0.9778\n",
      "Epoch 137/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0536 - accuracy: 0.9883 - val_loss: 0.2472 - val_accuracy: 0.9790\n",
      "Epoch 138/250\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.0505 - accuracy: 0.9897 - val_loss: 0.2757 - val_accuracy: 0.9775\n",
      "Epoch 139/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0530 - accuracy: 0.9885 - val_loss: 0.2643 - val_accuracy: 0.9788\n",
      "Epoch 140/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0551 - accuracy: 0.9891 - val_loss: 0.2633 - val_accuracy: 0.9783\n",
      "Epoch 141/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0467 - accuracy: 0.9894 - val_loss: 0.2789 - val_accuracy: 0.9771\n",
      "Epoch 142/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0516 - accuracy: 0.9886 - val_loss: 0.2833 - val_accuracy: 0.9777\n",
      "Epoch 143/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0555 - accuracy: 0.9885 - val_loss: 0.2762 - val_accuracy: 0.9770\n",
      "Epoch 144/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0494 - accuracy: 0.9885 - val_loss: 0.2867 - val_accuracy: 0.9787\n",
      "Epoch 145/250\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.0496 - accuracy: 0.9897 - val_loss: 0.2773 - val_accuracy: 0.9778\n",
      "Epoch 146/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0529 - accuracy: 0.9883 - val_loss: 0.2840 - val_accuracy: 0.9789\n",
      "Epoch 147/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0552 - accuracy: 0.9886 - val_loss: 0.2816 - val_accuracy: 0.9778\n",
      "Epoch 148/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0492 - accuracy: 0.9896 - val_loss: 0.2737 - val_accuracy: 0.9793\n",
      "Epoch 149/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0463 - accuracy: 0.9897 - val_loss: 0.2886 - val_accuracy: 0.9793\n",
      "Epoch 150/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0489 - accuracy: 0.9898 - val_loss: 0.2736 - val_accuracy: 0.9785\n",
      "Epoch 151/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0485 - accuracy: 0.9897 - val_loss: 0.2656 - val_accuracy: 0.9777\n",
      "Epoch 152/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0510 - accuracy: 0.9897 - val_loss: 0.2831 - val_accuracy: 0.9785\n",
      "Epoch 153/250\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.0536 - accuracy: 0.9889 - val_loss: 0.2965 - val_accuracy: 0.9780\n",
      "Epoch 154/250\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.0514 - accuracy: 0.9898 - val_loss: 0.3075 - val_accuracy: 0.9779\n",
      "Epoch 155/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0475 - accuracy: 0.9898 - val_loss: 0.2747 - val_accuracy: 0.9785\n",
      "Epoch 156/250\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.0564 - accuracy: 0.9885 - val_loss: 0.2985 - val_accuracy: 0.9780\n",
      "Epoch 157/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0520 - accuracy: 0.9890 - val_loss: 0.2912 - val_accuracy: 0.9779\n",
      "Epoch 158/250\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.0524 - accuracy: 0.9894 - val_loss: 0.2854 - val_accuracy: 0.9802\n",
      "Epoch 159/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0516 - accuracy: 0.9892 - val_loss: 0.3014 - val_accuracy: 0.9797\n",
      "Epoch 160/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0545 - accuracy: 0.9900 - val_loss: 0.3044 - val_accuracy: 0.9780\n",
      "Epoch 161/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0540 - accuracy: 0.9896 - val_loss: 0.3073 - val_accuracy: 0.9775\n",
      "Epoch 162/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0598 - accuracy: 0.9892 - val_loss: 0.2874 - val_accuracy: 0.9793\n",
      "Epoch 163/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0541 - accuracy: 0.9887 - val_loss: 0.3236 - val_accuracy: 0.9787\n",
      "Epoch 164/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0545 - accuracy: 0.9891 - val_loss: 0.3355 - val_accuracy: 0.9772\n",
      "Epoch 165/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0537 - accuracy: 0.9887 - val_loss: 0.2921 - val_accuracy: 0.9778\n",
      "Epoch 166/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0532 - accuracy: 0.9902 - val_loss: 0.2934 - val_accuracy: 0.9782\n",
      "Epoch 167/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0503 - accuracy: 0.9900 - val_loss: 0.3302 - val_accuracy: 0.9770\n",
      "Epoch 168/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0510 - accuracy: 0.9891 - val_loss: 0.3199 - val_accuracy: 0.9772\n",
      "Epoch 169/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0576 - accuracy: 0.9890 - val_loss: 0.3051 - val_accuracy: 0.9773\n",
      "Epoch 170/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0520 - accuracy: 0.9895 - val_loss: 0.3061 - val_accuracy: 0.9778\n",
      "Epoch 171/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0584 - accuracy: 0.9887 - val_loss: 0.3017 - val_accuracy: 0.9784\n",
      "Epoch 172/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0571 - accuracy: 0.9891 - val_loss: 0.3177 - val_accuracy: 0.9780\n",
      "Epoch 173/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0559 - accuracy: 0.9885 - val_loss: 0.2844 - val_accuracy: 0.9776\n",
      "Epoch 174/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0525 - accuracy: 0.9900 - val_loss: 0.3151 - val_accuracy: 0.9771\n",
      "Epoch 175/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0567 - accuracy: 0.9896 - val_loss: 0.3010 - val_accuracy: 0.9782\n",
      "Epoch 176/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0558 - accuracy: 0.9887 - val_loss: 0.2955 - val_accuracy: 0.9777\n",
      "Epoch 177/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0541 - accuracy: 0.9882 - val_loss: 0.3161 - val_accuracy: 0.9776\n",
      "Epoch 178/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0492 - accuracy: 0.9897 - val_loss: 0.3340 - val_accuracy: 0.9772\n",
      "Epoch 179/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0556 - accuracy: 0.9889 - val_loss: 0.3130 - val_accuracy: 0.9782\n",
      "Epoch 180/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0573 - accuracy: 0.9895 - val_loss: 0.3128 - val_accuracy: 0.9785\n",
      "Epoch 181/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0500 - accuracy: 0.9899 - val_loss: 0.3276 - val_accuracy: 0.9776\n",
      "Epoch 182/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0566 - accuracy: 0.9887 - val_loss: 0.3005 - val_accuracy: 0.9766\n",
      "Epoch 183/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0585 - accuracy: 0.9892 - val_loss: 0.3151 - val_accuracy: 0.9772\n",
      "Epoch 184/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0510 - accuracy: 0.9899 - val_loss: 0.3075 - val_accuracy: 0.9779\n",
      "Epoch 185/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0532 - accuracy: 0.9900 - val_loss: 0.3196 - val_accuracy: 0.9775\n",
      "Epoch 186/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0518 - accuracy: 0.9897 - val_loss: 0.3145 - val_accuracy: 0.9783\n",
      "Epoch 187/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0566 - accuracy: 0.9897 - val_loss: 0.3178 - val_accuracy: 0.9789\n",
      "Epoch 188/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0526 - accuracy: 0.9890 - val_loss: 0.3153 - val_accuracy: 0.9774\n",
      "Epoch 189/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0490 - accuracy: 0.9894 - val_loss: 0.3170 - val_accuracy: 0.9786\n",
      "Epoch 190/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0524 - accuracy: 0.9894 - val_loss: 0.3248 - val_accuracy: 0.9768\n",
      "Epoch 191/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0487 - accuracy: 0.9899 - val_loss: 0.3060 - val_accuracy: 0.9787\n",
      "Epoch 192/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0565 - accuracy: 0.9891 - val_loss: 0.3052 - val_accuracy: 0.9779\n",
      "Epoch 193/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0545 - accuracy: 0.9899 - val_loss: 0.3101 - val_accuracy: 0.9775\n",
      "Epoch 194/250\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.0584 - accuracy: 0.9890 - val_loss: 0.3052 - val_accuracy: 0.9778\n",
      "Epoch 195/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0553 - accuracy: 0.9900 - val_loss: 0.3264 - val_accuracy: 0.9778\n",
      "Epoch 196/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0538 - accuracy: 0.9885 - val_loss: 0.3133 - val_accuracy: 0.9778\n",
      "Epoch 197/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0511 - accuracy: 0.9894 - val_loss: 0.3257 - val_accuracy: 0.9777\n",
      "Epoch 198/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0542 - accuracy: 0.9889 - val_loss: 0.3149 - val_accuracy: 0.9778\n",
      "Epoch 199/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0555 - accuracy: 0.9896 - val_loss: 0.3487 - val_accuracy: 0.9785\n",
      "Epoch 200/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0515 - accuracy: 0.9902 - val_loss: 0.3324 - val_accuracy: 0.9796\n",
      "Epoch 201/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0526 - accuracy: 0.9902 - val_loss: 0.3427 - val_accuracy: 0.9777\n",
      "Epoch 202/250\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 0.0558 - accuracy: 0.9903 - val_loss: 0.3453 - val_accuracy: 0.9772\n",
      "Epoch 203/250\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 0.0519 - accuracy: 0.9894 - val_loss: 0.3552 - val_accuracy: 0.9780\n",
      "Epoch 204/250\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.0548 - accuracy: 0.9896 - val_loss: 0.3270 - val_accuracy: 0.9787\n",
      "Epoch 205/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0548 - accuracy: 0.9898 - val_loss: 0.3207 - val_accuracy: 0.9768\n",
      "Epoch 206/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0586 - accuracy: 0.9895 - val_loss: 0.3225 - val_accuracy: 0.9788\n",
      "Epoch 207/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0549 - accuracy: 0.9898 - val_loss: 0.3422 - val_accuracy: 0.9777\n",
      "Epoch 208/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0541 - accuracy: 0.9898 - val_loss: 0.3361 - val_accuracy: 0.9768\n",
      "Epoch 209/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0543 - accuracy: 0.9896 - val_loss: 0.3270 - val_accuracy: 0.9786\n",
      "Epoch 210/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0575 - accuracy: 0.9898 - val_loss: 0.3444 - val_accuracy: 0.9789\n",
      "Epoch 211/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0530 - accuracy: 0.9897 - val_loss: 0.3843 - val_accuracy: 0.9775\n",
      "Epoch 212/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0556 - accuracy: 0.9894 - val_loss: 0.3584 - val_accuracy: 0.9771\n",
      "Epoch 213/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0537 - accuracy: 0.9897 - val_loss: 0.3365 - val_accuracy: 0.9768\n",
      "Epoch 214/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0541 - accuracy: 0.9897 - val_loss: 0.3462 - val_accuracy: 0.9783\n",
      "Epoch 215/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0522 - accuracy: 0.9897 - val_loss: 0.3505 - val_accuracy: 0.9789\n",
      "Epoch 216/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0618 - accuracy: 0.9893 - val_loss: 0.3323 - val_accuracy: 0.9778\n",
      "Epoch 217/250\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0511 - accuracy: 0.9901 - val_loss: 0.3501 - val_accuracy: 0.9783\n",
      "Epoch 218/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0573 - accuracy: 0.9892 - val_loss: 0.3354 - val_accuracy: 0.9779\n",
      "Epoch 219/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0543 - accuracy: 0.9897 - val_loss: 0.3540 - val_accuracy: 0.9778\n",
      "Epoch 220/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0514 - accuracy: 0.9902 - val_loss: 0.3633 - val_accuracy: 0.9782\n",
      "Epoch 221/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0552 - accuracy: 0.9901 - val_loss: 0.3551 - val_accuracy: 0.9775\n",
      "Epoch 222/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0580 - accuracy: 0.9898 - val_loss: 0.3474 - val_accuracy: 0.9780\n",
      "Epoch 223/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0640 - accuracy: 0.9891 - val_loss: 0.3431 - val_accuracy: 0.9773\n",
      "Epoch 224/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0565 - accuracy: 0.9881 - val_loss: 0.3418 - val_accuracy: 0.9775\n",
      "Epoch 225/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0648 - accuracy: 0.9885 - val_loss: 0.3463 - val_accuracy: 0.9757\n",
      "Epoch 226/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0553 - accuracy: 0.9896 - val_loss: 0.3540 - val_accuracy: 0.9778\n",
      "Epoch 227/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0635 - accuracy: 0.9892 - val_loss: 0.3455 - val_accuracy: 0.9772\n",
      "Epoch 228/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0563 - accuracy: 0.9891 - val_loss: 0.3739 - val_accuracy: 0.9766\n",
      "Epoch 229/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0562 - accuracy: 0.9897 - val_loss: 0.3658 - val_accuracy: 0.9782\n",
      "Epoch 230/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0557 - accuracy: 0.9891 - val_loss: 0.3571 - val_accuracy: 0.9780\n",
      "Epoch 231/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0510 - accuracy: 0.9900 - val_loss: 0.3804 - val_accuracy: 0.9778\n",
      "Epoch 232/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0534 - accuracy: 0.9894 - val_loss: 0.3577 - val_accuracy: 0.9772\n",
      "Epoch 233/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0517 - accuracy: 0.9897 - val_loss: 0.3643 - val_accuracy: 0.9776\n",
      "Epoch 234/250\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0534 - accuracy: 0.9905 - val_loss: 0.3512 - val_accuracy: 0.9769\n",
      "Epoch 235/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0523 - accuracy: 0.9905 - val_loss: 0.3593 - val_accuracy: 0.9780\n",
      "Epoch 236/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0501 - accuracy: 0.9907 - val_loss: 0.3562 - val_accuracy: 0.9771\n",
      "Epoch 237/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0550 - accuracy: 0.9902 - val_loss: 0.3658 - val_accuracy: 0.9786\n",
      "Epoch 238/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0510 - accuracy: 0.9904 - val_loss: 0.3775 - val_accuracy: 0.9772\n",
      "Epoch 239/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0513 - accuracy: 0.9900 - val_loss: 0.3605 - val_accuracy: 0.9774\n",
      "Epoch 240/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0619 - accuracy: 0.9898 - val_loss: 0.3449 - val_accuracy: 0.9778\n",
      "Epoch 241/250\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0579 - accuracy: 0.9899 - val_loss: 0.3682 - val_accuracy: 0.9762\n",
      "Epoch 242/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0669 - accuracy: 0.9890 - val_loss: 0.3775 - val_accuracy: 0.9775\n",
      "Epoch 243/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0567 - accuracy: 0.9904 - val_loss: 0.3852 - val_accuracy: 0.9766\n",
      "Epoch 244/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0472 - accuracy: 0.9903 - val_loss: 0.3868 - val_accuracy: 0.9768\n",
      "Epoch 245/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0565 - accuracy: 0.9898 - val_loss: 0.3704 - val_accuracy: 0.9772\n",
      "Epoch 246/250\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0555 - accuracy: 0.9895 - val_loss: 0.3428 - val_accuracy: 0.9784\n",
      "Epoch 247/250\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0486 - accuracy: 0.9906 - val_loss: 0.3630 - val_accuracy: 0.9783\n",
      "Epoch 248/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0596 - accuracy: 0.9895 - val_loss: 0.3649 - val_accuracy: 0.9783\n",
      "Epoch 249/250\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0588 - accuracy: 0.9897 - val_loss: 0.3697 - val_accuracy: 0.9778\n",
      "Epoch 250/250\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0568 - accuracy: 0.9907 - val_loss: 0.3605 - val_accuracy: 0.9773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd580e72350>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 모델 학습 ##\n",
    "model.fit(X_train, Y_train,\n",
    "         batch_size=128, epochs=250, verbose=1, validation_split=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 22us/sample - loss: 0.3380 - accuracy: 0.9779\n",
      "Test accuracy: 0.9779\n"
     ]
    }
   ],
   "source": [
    "## 모델 평가 ##\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 컴파일 교체(목적함수 Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "48000/48000 [==============================] - 1s 26us/sample - loss: 0.0412 - accuracy: 0.9865 - val_loss: 0.0812 - val_accuracy: 0.9798\n",
      "Epoch 2/50\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.0419 - accuracy: 0.9864 - val_loss: 0.0816 - val_accuracy: 0.9787\n",
      "Epoch 3/50\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.0402 - accuracy: 0.9868 - val_loss: 0.0778 - val_accuracy: 0.9805\n",
      "Epoch 4/50\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.0385 - accuracy: 0.9875 - val_loss: 0.0833 - val_accuracy: 0.9786\n",
      "Epoch 5/50\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.0360 - accuracy: 0.9882 - val_loss: 0.0907 - val_accuracy: 0.9780\n",
      "Epoch 6/50\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.0376 - accuracy: 0.9876 - val_loss: 0.0929 - val_accuracy: 0.9772\n",
      "Epoch 7/50\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.0362 - accuracy: 0.9880 - val_loss: 0.0852 - val_accuracy: 0.9794\n",
      "Epoch 8/50\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0375 - accuracy: 0.9872 - val_loss: 0.0874 - val_accuracy: 0.9792\n",
      "Epoch 9/50\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0368 - accuracy: 0.9881 - val_loss: 0.0811 - val_accuracy: 0.9794\n",
      "Epoch 10/50\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0328 - accuracy: 0.9893 - val_loss: 0.0877 - val_accuracy: 0.9793\n",
      "Epoch 11/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0338 - accuracy: 0.9891 - val_loss: 0.0877 - val_accuracy: 0.9792\n",
      "Epoch 12/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0338 - accuracy: 0.9895 - val_loss: 0.0904 - val_accuracy: 0.9787\n",
      "Epoch 13/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0345 - accuracy: 0.9884 - val_loss: 0.0853 - val_accuracy: 0.9793\n",
      "Epoch 14/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0360 - accuracy: 0.9884 - val_loss: 0.0830 - val_accuracy: 0.9799\n",
      "Epoch 15/50\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.0345 - accuracy: 0.9888 - val_loss: 0.0889 - val_accuracy: 0.9787\n",
      "Epoch 16/50\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.0315 - accuracy: 0.9893 - val_loss: 0.0960 - val_accuracy: 0.9787\n",
      "Epoch 17/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0324 - accuracy: 0.9892 - val_loss: 0.0873 - val_accuracy: 0.9797\n",
      "Epoch 18/50\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.0334 - accuracy: 0.9892 - val_loss: 0.0889 - val_accuracy: 0.9808\n",
      "Epoch 19/50\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.0290 - accuracy: 0.9901 - val_loss: 0.0914 - val_accuracy: 0.9802\n",
      "Epoch 20/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0312 - accuracy: 0.9899 - val_loss: 0.0887 - val_accuracy: 0.9803\n",
      "Epoch 21/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0329 - accuracy: 0.9895 - val_loss: 0.0889 - val_accuracy: 0.9807\n",
      "Epoch 22/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0288 - accuracy: 0.9905 - val_loss: 0.0868 - val_accuracy: 0.9804\n",
      "Epoch 23/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0315 - accuracy: 0.9899 - val_loss: 0.0916 - val_accuracy: 0.9790\n",
      "Epoch 24/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0308 - accuracy: 0.9897 - val_loss: 0.0923 - val_accuracy: 0.9778\n",
      "Epoch 25/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0313 - accuracy: 0.9898 - val_loss: 0.0858 - val_accuracy: 0.9803\n",
      "Epoch 26/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0291 - accuracy: 0.9907 - val_loss: 0.0886 - val_accuracy: 0.9808\n",
      "Epoch 27/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0272 - accuracy: 0.9914 - val_loss: 0.0914 - val_accuracy: 0.9797\n",
      "Epoch 28/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0318 - accuracy: 0.9898 - val_loss: 0.0927 - val_accuracy: 0.9787\n",
      "Epoch 29/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0310 - accuracy: 0.9902 - val_loss: 0.0891 - val_accuracy: 0.9805\n",
      "Epoch 30/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0306 - accuracy: 0.9899 - val_loss: 0.0934 - val_accuracy: 0.9803\n",
      "Epoch 31/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0288 - accuracy: 0.9911 - val_loss: 0.0935 - val_accuracy: 0.9790\n",
      "Epoch 32/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0251 - accuracy: 0.9919 - val_loss: 0.0974 - val_accuracy: 0.9800\n",
      "Epoch 33/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0256 - accuracy: 0.9916 - val_loss: 0.0938 - val_accuracy: 0.9800\n",
      "Epoch 34/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0268 - accuracy: 0.9909 - val_loss: 0.0995 - val_accuracy: 0.9786\n",
      "Epoch 35/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0269 - accuracy: 0.9910 - val_loss: 0.0961 - val_accuracy: 0.9802\n",
      "Epoch 36/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0282 - accuracy: 0.9912 - val_loss: 0.0918 - val_accuracy: 0.9798\n",
      "Epoch 37/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0306 - accuracy: 0.9906 - val_loss: 0.0926 - val_accuracy: 0.9799\n",
      "Epoch 38/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0269 - accuracy: 0.9908 - val_loss: 0.0930 - val_accuracy: 0.9806\n",
      "Epoch 39/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0258 - accuracy: 0.9911 - val_loss: 0.0983 - val_accuracy: 0.9792\n",
      "Epoch 40/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0279 - accuracy: 0.9912 - val_loss: 0.0920 - val_accuracy: 0.9786\n",
      "Epoch 41/50\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.0256 - accuracy: 0.9918 - val_loss: 0.1094 - val_accuracy: 0.9784\n",
      "Epoch 42/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0254 - accuracy: 0.9920 - val_loss: 0.1071 - val_accuracy: 0.9790\n",
      "Epoch 43/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0277 - accuracy: 0.9913 - val_loss: 0.0964 - val_accuracy: 0.9793\n",
      "Epoch 44/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0242 - accuracy: 0.9921 - val_loss: 0.0999 - val_accuracy: 0.9792\n",
      "Epoch 45/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0257 - accuracy: 0.9909 - val_loss: 0.0954 - val_accuracy: 0.9800\n",
      "Epoch 46/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0258 - accuracy: 0.9915 - val_loss: 0.1016 - val_accuracy: 0.9793\n",
      "Epoch 47/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0235 - accuracy: 0.9926 - val_loss: 0.0968 - val_accuracy: 0.9791\n",
      "Epoch 48/50\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.0256 - accuracy: 0.9917 - val_loss: 0.1003 - val_accuracy: 0.9803\n",
      "Epoch 49/50\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.0240 - accuracy: 0.9926 - val_loss: 0.1023 - val_accuracy: 0.9799\n",
      "Epoch 50/50\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.0283 - accuracy: 0.9909 - val_loss: 0.1021 - val_accuracy: 0.9802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd53c6a4210>"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "## 모델 학습 ##\n",
    "model.fit(X_train, Y_train,\n",
    "         batch_size=128, epochs=50, verbose=1, validation_split=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 23us/sample - loss: 0.0936 - accuracy: 0.9805\n",
      "Test accuracy: 0.9805\n"
     ]
    }
   ],
   "source": [
    "## 모델 평가 ##\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model/mnist_3(Adam).h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최적화기\n",
    "- 목적함수 : Adam\n",
    "- 드롭아웃 : 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 전개 ##\n",
    "predictions = model.predict(X_test)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.2925690e-19, 1.8679447e-17, 8.8403975e-13, 4.5772324e-09,\n",
       "       1.1199130e-22, 7.8696908e-15, 9.6924158e-29, 1.0000000e+00,\n",
       "       3.2315000e-16, 2.5692367e-12], dtype=float32)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0] # 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[0] # 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손글씨 데이터셋의 최적화기는 Adam\n",
    "v1-0.9224, v2-0.9646, drop(0.3)-0.9779, RMSProp-(0.9768, 0.9779), Adam-0.9809"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 내부 뉴런 증가(n_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## 모델 구축 ##\n",
    "model = tf.keras.Sequential()\n",
    "model.add(k.layers.Dense(128, input_shape=(784,), name='dense_layer', activation='relu'))\n",
    "model.add(k.layers.Dropout(.3))\n",
    "model.add(k.layers.Dense(128, name='dense_layer_2', activation='relu'))\n",
    "model.add(k.layers.Dropout(.3))\n",
    "model.add(k.layers.Dense(128, name='dense_layer_3', activation='relu'))\n",
    "model.add(k.layers.Dropout(.3))\n",
    "model.add(k.layers.Dense(128, name='dense_layer_4', activation='relu'))\n",
    "model.add(k.layers.Dropout(.3))\n",
    "model.add(k.layers.Dense(10, name='dense_layer_5', activation='softmax'))\n",
    "\n",
    "## 모델 컴파일 ##\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "## 모델 학습 ##\n",
    "model.fit(X_train, Y_train,\n",
    "         batch_size=128, epochs=50, verbose=1, validation_split=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.0963 - accuracy: 0.9788\n",
      "Test accuracy: 0.9788\n"
     ]
    }
   ],
   "source": [
    "## 모델 평가 ##\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 증가량 미미함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### batch_size 조절(128 -> 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "## 모델 학습 ##\n",
    "model.fit(X_train, Y_train,\n",
    "         batch_size=64, epochs=50, verbose=1, validation_split=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.1157 - accuracy: 0.9785\n",
      "Test accuracy: 0.9785\n"
     ]
    }
   ],
   "source": [
    "## 모델 평가 ##\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 증가량 미미함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 활성화 함수 교체(sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_layer (Dense)          (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer_2 (Dense)        (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer_3 (Dense)        (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## 모델 구축 ##\n",
    "model = tf.keras.Sequential()\n",
    "model.add(k.layers.Dense(128, input_shape=(784,), name='dense_layer', activation='sigmoid'))\n",
    "model.add(k.layers.Dropout(.3))\n",
    "model.add(k.layers.Dense(128, name='dense_layer_2', activation='sigmoid'))\n",
    "model.add(k.layers.Dropout(.3))\n",
    "model.add(k.layers.Dense(10, name='dense_layer_3', activation='softmax'))\n",
    "\n",
    "## 모델 요약 ##\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/200\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 2.3837 - accuracy: 0.1069 - val_loss: 2.2784 - val_accuracy: 0.1772\n",
      "Epoch 2/200\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 2.3419 - accuracy: 0.1194 - val_loss: 2.2484 - val_accuracy: 0.2607\n",
      "Epoch 3/200\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 2.3013 - accuracy: 0.1382 - val_loss: 2.2171 - val_accuracy: 0.4223\n",
      "Epoch 4/200\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 2.2650 - accuracy: 0.1602 - val_loss: 2.1817 - val_accuracy: 0.4963\n",
      "Epoch 5/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 2.2250 - accuracy: 0.1859 - val_loss: 2.1373 - val_accuracy: 0.5278\n",
      "Epoch 6/200\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 2.1759 - accuracy: 0.2229 - val_loss: 2.0803 - val_accuracy: 0.5428\n",
      "Epoch 7/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 2.1159 - accuracy: 0.2651 - val_loss: 2.0057 - val_accuracy: 0.5932\n",
      "Epoch 8/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 2.0371 - accuracy: 0.3109 - val_loss: 1.9106 - val_accuracy: 0.5782\n",
      "Epoch 9/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 1.9422 - accuracy: 0.3567 - val_loss: 1.7965 - val_accuracy: 0.6289\n",
      "Epoch 10/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 1.8386 - accuracy: 0.3937 - val_loss: 1.6687 - val_accuracy: 0.6332\n",
      "Epoch 11/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 1.7231 - accuracy: 0.4370 - val_loss: 1.5398 - val_accuracy: 0.6490\n",
      "Epoch 12/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 1.6115 - accuracy: 0.4728 - val_loss: 1.4176 - val_accuracy: 0.6878\n",
      "Epoch 13/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 1.5116 - accuracy: 0.5019 - val_loss: 1.3074 - val_accuracy: 0.7177\n",
      "Epoch 14/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 1.4157 - accuracy: 0.5375 - val_loss: 1.2084 - val_accuracy: 0.7253\n",
      "Epoch 15/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 1.3364 - accuracy: 0.5557 - val_loss: 1.1234 - val_accuracy: 0.7426\n",
      "Epoch 16/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 1.2585 - accuracy: 0.5805 - val_loss: 1.0501 - val_accuracy: 0.7566\n",
      "Epoch 17/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 1.1967 - accuracy: 0.6045 - val_loss: 0.9857 - val_accuracy: 0.7650\n",
      "Epoch 18/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 1.1409 - accuracy: 0.6225 - val_loss: 0.9311 - val_accuracy: 0.7779\n",
      "Epoch 19/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 1.0935 - accuracy: 0.6341 - val_loss: 0.8822 - val_accuracy: 0.7858\n",
      "Epoch 20/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 1.0481 - accuracy: 0.6523 - val_loss: 0.8395 - val_accuracy: 0.7947\n",
      "Epoch 21/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 1.0126 - accuracy: 0.6653 - val_loss: 0.8028 - val_accuracy: 0.8008\n",
      "Epoch 22/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.9764 - accuracy: 0.6786 - val_loss: 0.7693 - val_accuracy: 0.8053\n",
      "Epoch 23/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.9440 - accuracy: 0.6877 - val_loss: 0.7394 - val_accuracy: 0.8123\n",
      "Epoch 24/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.9134 - accuracy: 0.6990 - val_loss: 0.7112 - val_accuracy: 0.8186\n",
      "Epoch 25/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.8908 - accuracy: 0.7063 - val_loss: 0.6866 - val_accuracy: 0.8241\n",
      "Epoch 26/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.8664 - accuracy: 0.7160 - val_loss: 0.6639 - val_accuracy: 0.8292\n",
      "Epoch 27/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.8420 - accuracy: 0.7255 - val_loss: 0.6437 - val_accuracy: 0.8322\n",
      "Epoch 28/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.8199 - accuracy: 0.7337 - val_loss: 0.6239 - val_accuracy: 0.8373\n",
      "Epoch 29/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.8021 - accuracy: 0.7392 - val_loss: 0.6064 - val_accuracy: 0.8410\n",
      "Epoch 30/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.7832 - accuracy: 0.7479 - val_loss: 0.5907 - val_accuracy: 0.8428\n",
      "Epoch 31/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.7696 - accuracy: 0.7500 - val_loss: 0.5762 - val_accuracy: 0.8453\n",
      "Epoch 32/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.7530 - accuracy: 0.7568 - val_loss: 0.5634 - val_accuracy: 0.8482\n",
      "Epoch 33/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.7404 - accuracy: 0.7616 - val_loss: 0.5512 - val_accuracy: 0.8506\n",
      "Epoch 34/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.7267 - accuracy: 0.7657 - val_loss: 0.5394 - val_accuracy: 0.8545\n",
      "Epoch 35/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.7117 - accuracy: 0.7709 - val_loss: 0.5292 - val_accuracy: 0.8558\n",
      "Epoch 36/200\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.7019 - accuracy: 0.7770 - val_loss: 0.5193 - val_accuracy: 0.8576\n",
      "Epoch 37/200\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.6894 - accuracy: 0.7797 - val_loss: 0.5098 - val_accuracy: 0.8598\n",
      "Epoch 38/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.6741 - accuracy: 0.7843 - val_loss: 0.5018 - val_accuracy: 0.8620\n",
      "Epoch 39/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.6683 - accuracy: 0.7864 - val_loss: 0.4939 - val_accuracy: 0.8635\n",
      "Epoch 40/200\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.6614 - accuracy: 0.7871 - val_loss: 0.4862 - val_accuracy: 0.8662\n",
      "Epoch 41/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.6474 - accuracy: 0.7935 - val_loss: 0.4794 - val_accuracy: 0.8674\n",
      "Epoch 42/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.6446 - accuracy: 0.7965 - val_loss: 0.4731 - val_accuracy: 0.8673\n",
      "Epoch 43/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.6332 - accuracy: 0.7977 - val_loss: 0.4663 - val_accuracy: 0.8693\n",
      "Epoch 44/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.6255 - accuracy: 0.8011 - val_loss: 0.4605 - val_accuracy: 0.8709\n",
      "Epoch 45/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.6236 - accuracy: 0.8045 - val_loss: 0.4549 - val_accuracy: 0.8735\n",
      "Epoch 46/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.6166 - accuracy: 0.8069 - val_loss: 0.4498 - val_accuracy: 0.8740\n",
      "Epoch 47/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.6087 - accuracy: 0.8075 - val_loss: 0.4447 - val_accuracy: 0.8747\n",
      "Epoch 48/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.6029 - accuracy: 0.8097 - val_loss: 0.4396 - val_accuracy: 0.8763\n",
      "Epoch 49/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.5967 - accuracy: 0.8124 - val_loss: 0.4352 - val_accuracy: 0.8777\n",
      "Epoch 50/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.5901 - accuracy: 0.8157 - val_loss: 0.4306 - val_accuracy: 0.8792\n",
      "Epoch 51/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.5875 - accuracy: 0.8178 - val_loss: 0.4262 - val_accuracy: 0.8800\n",
      "Epoch 52/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.5781 - accuracy: 0.8199 - val_loss: 0.4230 - val_accuracy: 0.8805\n",
      "Epoch 53/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.5760 - accuracy: 0.8199 - val_loss: 0.4188 - val_accuracy: 0.8817\n",
      "Epoch 54/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.5690 - accuracy: 0.8218 - val_loss: 0.4150 - val_accuracy: 0.8826\n",
      "Epoch 55/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.5664 - accuracy: 0.8244 - val_loss: 0.4113 - val_accuracy: 0.8846\n",
      "Epoch 56/200\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.5579 - accuracy: 0.8270 - val_loss: 0.4080 - val_accuracy: 0.8856\n",
      "Epoch 57/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.5592 - accuracy: 0.8253 - val_loss: 0.4047 - val_accuracy: 0.8862\n",
      "Epoch 58/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.5538 - accuracy: 0.8285 - val_loss: 0.4015 - val_accuracy: 0.8868\n",
      "Epoch 59/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.5465 - accuracy: 0.8319 - val_loss: 0.3981 - val_accuracy: 0.8878\n",
      "Epoch 60/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.5432 - accuracy: 0.8314 - val_loss: 0.3954 - val_accuracy: 0.8882\n",
      "Epoch 61/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.5402 - accuracy: 0.8332 - val_loss: 0.3925 - val_accuracy: 0.8892\n",
      "Epoch 62/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.5342 - accuracy: 0.8327 - val_loss: 0.3894 - val_accuracy: 0.8893\n",
      "Epoch 63/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.5357 - accuracy: 0.8344 - val_loss: 0.3870 - val_accuracy: 0.8902\n",
      "Epoch 64/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.5275 - accuracy: 0.8359 - val_loss: 0.3846 - val_accuracy: 0.8907\n",
      "Epoch 65/200\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.5275 - accuracy: 0.8373 - val_loss: 0.3823 - val_accuracy: 0.8915\n",
      "Epoch 66/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.5245 - accuracy: 0.8380 - val_loss: 0.3798 - val_accuracy: 0.8913\n",
      "Epoch 67/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.5217 - accuracy: 0.8395 - val_loss: 0.3777 - val_accuracy: 0.8922\n",
      "Epoch 68/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.5144 - accuracy: 0.8438 - val_loss: 0.3749 - val_accuracy: 0.8932\n",
      "Epoch 69/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.5196 - accuracy: 0.8403 - val_loss: 0.3726 - val_accuracy: 0.8942\n",
      "Epoch 70/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.5115 - accuracy: 0.8436 - val_loss: 0.3705 - val_accuracy: 0.8939\n",
      "Epoch 71/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.5039 - accuracy: 0.8440 - val_loss: 0.3686 - val_accuracy: 0.8952\n",
      "Epoch 72/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.5068 - accuracy: 0.8449 - val_loss: 0.3666 - val_accuracy: 0.8954\n",
      "Epoch 73/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4990 - accuracy: 0.8461 - val_loss: 0.3641 - val_accuracy: 0.8956\n",
      "Epoch 74/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4965 - accuracy: 0.8479 - val_loss: 0.3628 - val_accuracy: 0.8966\n",
      "Epoch 75/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.4967 - accuracy: 0.8500 - val_loss: 0.3606 - val_accuracy: 0.8972\n",
      "Epoch 76/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4928 - accuracy: 0.8497 - val_loss: 0.3587 - val_accuracy: 0.8972\n",
      "Epoch 77/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4904 - accuracy: 0.8498 - val_loss: 0.3571 - val_accuracy: 0.8984\n",
      "Epoch 78/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4887 - accuracy: 0.8519 - val_loss: 0.3555 - val_accuracy: 0.8992\n",
      "Epoch 79/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.4890 - accuracy: 0.8491 - val_loss: 0.3537 - val_accuracy: 0.8991\n",
      "Epoch 80/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4870 - accuracy: 0.8498 - val_loss: 0.3528 - val_accuracy: 0.8978\n",
      "Epoch 81/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.4824 - accuracy: 0.8522 - val_loss: 0.3505 - val_accuracy: 0.8996\n",
      "Epoch 82/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4815 - accuracy: 0.8529 - val_loss: 0.3488 - val_accuracy: 0.9003\n",
      "Epoch 83/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4792 - accuracy: 0.8542 - val_loss: 0.3473 - val_accuracy: 0.9007\n",
      "Epoch 84/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.4773 - accuracy: 0.8536 - val_loss: 0.3460 - val_accuracy: 0.9013\n",
      "Epoch 85/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4731 - accuracy: 0.8584 - val_loss: 0.3446 - val_accuracy: 0.9011\n",
      "Epoch 86/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4704 - accuracy: 0.8570 - val_loss: 0.3431 - val_accuracy: 0.9009\n",
      "Epoch 87/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.4711 - accuracy: 0.8562 - val_loss: 0.3413 - val_accuracy: 0.9019\n",
      "Epoch 88/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4644 - accuracy: 0.8586 - val_loss: 0.3397 - val_accuracy: 0.9026\n",
      "Epoch 89/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4661 - accuracy: 0.8587 - val_loss: 0.3382 - val_accuracy: 0.9023\n",
      "Epoch 90/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.4658 - accuracy: 0.8580 - val_loss: 0.3373 - val_accuracy: 0.9028\n",
      "Epoch 91/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4603 - accuracy: 0.8610 - val_loss: 0.3358 - val_accuracy: 0.9032\n",
      "Epoch 92/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4579 - accuracy: 0.8614 - val_loss: 0.3344 - val_accuracy: 0.9031\n",
      "Epoch 93/200\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.4570 - accuracy: 0.8626 - val_loss: 0.3330 - val_accuracy: 0.9038\n",
      "Epoch 94/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4564 - accuracy: 0.8622 - val_loss: 0.3318 - val_accuracy: 0.9033\n",
      "Epoch 95/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4566 - accuracy: 0.8592 - val_loss: 0.3302 - val_accuracy: 0.9046\n",
      "Epoch 96/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.4544 - accuracy: 0.8617 - val_loss: 0.3292 - val_accuracy: 0.9039\n",
      "Epoch 97/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.4540 - accuracy: 0.8621 - val_loss: 0.3282 - val_accuracy: 0.9039\n",
      "Epoch 98/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.4495 - accuracy: 0.8638 - val_loss: 0.3267 - val_accuracy: 0.9050\n",
      "Epoch 99/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4485 - accuracy: 0.8650 - val_loss: 0.3256 - val_accuracy: 0.9045\n",
      "Epoch 100/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4478 - accuracy: 0.8638 - val_loss: 0.3247 - val_accuracy: 0.9047\n",
      "Epoch 101/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4478 - accuracy: 0.8639 - val_loss: 0.3232 - val_accuracy: 0.9057\n",
      "Epoch 102/200\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 0.4421 - accuracy: 0.8662 - val_loss: 0.3225 - val_accuracy: 0.9062\n",
      "Epoch 103/200\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.4401 - accuracy: 0.8685 - val_loss: 0.3213 - val_accuracy: 0.9063\n",
      "Epoch 104/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4407 - accuracy: 0.8662 - val_loss: 0.3200 - val_accuracy: 0.9068\n",
      "Epoch 105/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.4375 - accuracy: 0.8689 - val_loss: 0.3190 - val_accuracy: 0.9067\n",
      "Epoch 106/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.4390 - accuracy: 0.8661 - val_loss: 0.3181 - val_accuracy: 0.9071\n",
      "Epoch 107/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4365 - accuracy: 0.8678 - val_loss: 0.3169 - val_accuracy: 0.9075\n",
      "Epoch 108/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4354 - accuracy: 0.8677 - val_loss: 0.3160 - val_accuracy: 0.9074\n",
      "Epoch 109/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.4320 - accuracy: 0.8704 - val_loss: 0.3151 - val_accuracy: 0.9082\n",
      "Epoch 110/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.4315 - accuracy: 0.8696 - val_loss: 0.3141 - val_accuracy: 0.9081\n",
      "Epoch 111/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.4258 - accuracy: 0.8715 - val_loss: 0.3130 - val_accuracy: 0.9085\n",
      "Epoch 112/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4261 - accuracy: 0.8711 - val_loss: 0.3119 - val_accuracy: 0.9089\n",
      "Epoch 113/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4286 - accuracy: 0.8710 - val_loss: 0.3109 - val_accuracy: 0.9092\n",
      "Epoch 114/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4228 - accuracy: 0.8726 - val_loss: 0.3098 - val_accuracy: 0.9095\n",
      "Epoch 115/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4260 - accuracy: 0.8710 - val_loss: 0.3090 - val_accuracy: 0.9097\n",
      "Epoch 116/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.4265 - accuracy: 0.8712 - val_loss: 0.3080 - val_accuracy: 0.9103\n",
      "Epoch 117/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.4254 - accuracy: 0.8722 - val_loss: 0.3071 - val_accuracy: 0.9103\n",
      "Epoch 118/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4198 - accuracy: 0.8731 - val_loss: 0.3066 - val_accuracy: 0.9103\n",
      "Epoch 119/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.4229 - accuracy: 0.8724 - val_loss: 0.3054 - val_accuracy: 0.9109\n",
      "Epoch 120/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.4146 - accuracy: 0.8764 - val_loss: 0.3044 - val_accuracy: 0.9117\n",
      "Epoch 121/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.4174 - accuracy: 0.8734 - val_loss: 0.3033 - val_accuracy: 0.9116\n",
      "Epoch 122/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4133 - accuracy: 0.8757 - val_loss: 0.3025 - val_accuracy: 0.9118\n",
      "Epoch 123/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4140 - accuracy: 0.8766 - val_loss: 0.3014 - val_accuracy: 0.9123\n",
      "Epoch 124/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.4146 - accuracy: 0.8744 - val_loss: 0.3007 - val_accuracy: 0.9124\n",
      "Epoch 125/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4121 - accuracy: 0.8746 - val_loss: 0.2998 - val_accuracy: 0.9130\n",
      "Epoch 126/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4113 - accuracy: 0.8767 - val_loss: 0.2990 - val_accuracy: 0.9126\n",
      "Epoch 127/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4089 - accuracy: 0.8774 - val_loss: 0.2979 - val_accuracy: 0.9126\n",
      "Epoch 128/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.4091 - accuracy: 0.8767 - val_loss: 0.2970 - val_accuracy: 0.9133\n",
      "Epoch 129/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4054 - accuracy: 0.8785 - val_loss: 0.2966 - val_accuracy: 0.9141\n",
      "Epoch 130/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4061 - accuracy: 0.8777 - val_loss: 0.2955 - val_accuracy: 0.9137\n",
      "Epoch 131/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4034 - accuracy: 0.8792 - val_loss: 0.2948 - val_accuracy: 0.9144\n",
      "Epoch 132/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4055 - accuracy: 0.8781 - val_loss: 0.2941 - val_accuracy: 0.9149\n",
      "Epoch 133/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4041 - accuracy: 0.8786 - val_loss: 0.2930 - val_accuracy: 0.9155\n",
      "Epoch 134/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4037 - accuracy: 0.8776 - val_loss: 0.2923 - val_accuracy: 0.9153\n",
      "Epoch 135/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4021 - accuracy: 0.8794 - val_loss: 0.2914 - val_accuracy: 0.9152\n",
      "Epoch 136/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.4015 - accuracy: 0.8790 - val_loss: 0.2906 - val_accuracy: 0.9160\n",
      "Epoch 137/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3998 - accuracy: 0.8802 - val_loss: 0.2897 - val_accuracy: 0.9162\n",
      "Epoch 138/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3987 - accuracy: 0.8807 - val_loss: 0.2890 - val_accuracy: 0.9165\n",
      "Epoch 139/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3969 - accuracy: 0.8816 - val_loss: 0.2881 - val_accuracy: 0.9163\n",
      "Epoch 140/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3948 - accuracy: 0.8817 - val_loss: 0.2874 - val_accuracy: 0.9165\n",
      "Epoch 141/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3944 - accuracy: 0.8814 - val_loss: 0.2867 - val_accuracy: 0.9170\n",
      "Epoch 142/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3913 - accuracy: 0.8819 - val_loss: 0.2862 - val_accuracy: 0.9170\n",
      "Epoch 143/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3937 - accuracy: 0.8819 - val_loss: 0.2854 - val_accuracy: 0.9178\n",
      "Epoch 144/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3896 - accuracy: 0.8827 - val_loss: 0.2845 - val_accuracy: 0.9175\n",
      "Epoch 145/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3913 - accuracy: 0.8820 - val_loss: 0.2838 - val_accuracy: 0.9178\n",
      "Epoch 146/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3932 - accuracy: 0.8824 - val_loss: 0.2834 - val_accuracy: 0.9187\n",
      "Epoch 147/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3870 - accuracy: 0.8842 - val_loss: 0.2826 - val_accuracy: 0.9187\n",
      "Epoch 148/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3881 - accuracy: 0.8834 - val_loss: 0.2817 - val_accuracy: 0.9179\n",
      "Epoch 149/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3872 - accuracy: 0.8835 - val_loss: 0.2810 - val_accuracy: 0.9189\n",
      "Epoch 150/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3879 - accuracy: 0.8845 - val_loss: 0.2801 - val_accuracy: 0.9193\n",
      "Epoch 151/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3822 - accuracy: 0.8857 - val_loss: 0.2794 - val_accuracy: 0.9194\n",
      "Epoch 152/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3826 - accuracy: 0.8850 - val_loss: 0.2787 - val_accuracy: 0.9197\n",
      "Epoch 153/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3828 - accuracy: 0.8848 - val_loss: 0.2780 - val_accuracy: 0.9196\n",
      "Epoch 154/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3822 - accuracy: 0.8855 - val_loss: 0.2772 - val_accuracy: 0.9196\n",
      "Epoch 155/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3841 - accuracy: 0.8845 - val_loss: 0.2765 - val_accuracy: 0.9202\n",
      "Epoch 156/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3806 - accuracy: 0.8882 - val_loss: 0.2757 - val_accuracy: 0.9206\n",
      "Epoch 157/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3812 - accuracy: 0.8861 - val_loss: 0.2752 - val_accuracy: 0.9204\n",
      "Epoch 158/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3773 - accuracy: 0.8856 - val_loss: 0.2745 - val_accuracy: 0.9206\n",
      "Epoch 159/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3783 - accuracy: 0.8866 - val_loss: 0.2738 - val_accuracy: 0.9207\n",
      "Epoch 160/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3758 - accuracy: 0.8878 - val_loss: 0.2731 - val_accuracy: 0.9212\n",
      "Epoch 161/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3759 - accuracy: 0.8869 - val_loss: 0.2725 - val_accuracy: 0.9209\n",
      "Epoch 162/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3741 - accuracy: 0.8878 - val_loss: 0.2718 - val_accuracy: 0.9212\n",
      "Epoch 163/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3738 - accuracy: 0.8883 - val_loss: 0.2713 - val_accuracy: 0.9215\n",
      "Epoch 164/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3716 - accuracy: 0.8887 - val_loss: 0.2705 - val_accuracy: 0.9213\n",
      "Epoch 165/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3692 - accuracy: 0.8895 - val_loss: 0.2700 - val_accuracy: 0.9213\n",
      "Epoch 166/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3692 - accuracy: 0.8897 - val_loss: 0.2693 - val_accuracy: 0.9218\n",
      "Epoch 167/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3707 - accuracy: 0.8886 - val_loss: 0.2688 - val_accuracy: 0.9213\n",
      "Epoch 168/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3680 - accuracy: 0.8903 - val_loss: 0.2680 - val_accuracy: 0.9219\n",
      "Epoch 169/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3692 - accuracy: 0.8886 - val_loss: 0.2674 - val_accuracy: 0.9221\n",
      "Epoch 170/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3654 - accuracy: 0.8890 - val_loss: 0.2666 - val_accuracy: 0.9221\n",
      "Epoch 171/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3705 - accuracy: 0.8889 - val_loss: 0.2661 - val_accuracy: 0.9227\n",
      "Epoch 172/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3629 - accuracy: 0.8913 - val_loss: 0.2655 - val_accuracy: 0.9224\n",
      "Epoch 173/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3642 - accuracy: 0.8906 - val_loss: 0.2646 - val_accuracy: 0.9227\n",
      "Epoch 174/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3637 - accuracy: 0.8907 - val_loss: 0.2640 - val_accuracy: 0.9228\n",
      "Epoch 175/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3632 - accuracy: 0.8915 - val_loss: 0.2634 - val_accuracy: 0.9232\n",
      "Epoch 176/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3633 - accuracy: 0.8907 - val_loss: 0.2627 - val_accuracy: 0.9232\n",
      "Epoch 177/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3613 - accuracy: 0.8914 - val_loss: 0.2621 - val_accuracy: 0.9236\n",
      "Epoch 178/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3636 - accuracy: 0.8911 - val_loss: 0.2617 - val_accuracy: 0.9232\n",
      "Epoch 179/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3603 - accuracy: 0.8933 - val_loss: 0.2612 - val_accuracy: 0.9233\n",
      "Epoch 180/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3611 - accuracy: 0.8915 - val_loss: 0.2603 - val_accuracy: 0.9236\n",
      "Epoch 181/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3586 - accuracy: 0.8926 - val_loss: 0.2597 - val_accuracy: 0.9241\n",
      "Epoch 182/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3561 - accuracy: 0.8949 - val_loss: 0.2592 - val_accuracy: 0.9243\n",
      "Epoch 183/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3560 - accuracy: 0.8930 - val_loss: 0.2586 - val_accuracy: 0.9243\n",
      "Epoch 184/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3526 - accuracy: 0.8939 - val_loss: 0.2580 - val_accuracy: 0.9242\n",
      "Epoch 185/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3533 - accuracy: 0.8950 - val_loss: 0.2574 - val_accuracy: 0.9246\n",
      "Epoch 186/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3524 - accuracy: 0.8942 - val_loss: 0.2567 - val_accuracy: 0.9249\n",
      "Epoch 187/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3539 - accuracy: 0.8935 - val_loss: 0.2562 - val_accuracy: 0.9251\n",
      "Epoch 188/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3501 - accuracy: 0.8942 - val_loss: 0.2555 - val_accuracy: 0.9254\n",
      "Epoch 189/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3497 - accuracy: 0.8937 - val_loss: 0.2549 - val_accuracy: 0.9252\n",
      "Epoch 190/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3528 - accuracy: 0.8944 - val_loss: 0.2545 - val_accuracy: 0.9252\n",
      "Epoch 191/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3519 - accuracy: 0.8944 - val_loss: 0.2539 - val_accuracy: 0.9262\n",
      "Epoch 192/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3493 - accuracy: 0.8952 - val_loss: 0.2535 - val_accuracy: 0.9254\n",
      "Epoch 193/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3471 - accuracy: 0.8953 - val_loss: 0.2530 - val_accuracy: 0.9258\n",
      "Epoch 194/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3502 - accuracy: 0.8949 - val_loss: 0.2524 - val_accuracy: 0.9258\n",
      "Epoch 195/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3457 - accuracy: 0.8963 - val_loss: 0.2517 - val_accuracy: 0.9258\n",
      "Epoch 196/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3462 - accuracy: 0.8958 - val_loss: 0.2513 - val_accuracy: 0.9261\n",
      "Epoch 197/200\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3463 - accuracy: 0.8958 - val_loss: 0.2506 - val_accuracy: 0.9268\n",
      "Epoch 198/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3439 - accuracy: 0.8968 - val_loss: 0.2502 - val_accuracy: 0.9269\n",
      "Epoch 199/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3449 - accuracy: 0.8971 - val_loss: 0.2497 - val_accuracy: 0.9264\n",
      "Epoch 200/200\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.3456 - accuracy: 0.8956 - val_loss: 0.2491 - val_accuracy: 0.9269\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd53c26fa50>"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 모델 컴파일 ##\n",
    "model.compile(optimizer='SGD',  # 최적화기\n",
    "             loss='categorical_crossentropy',  # 목적함수\n",
    "             metrics=['accuracy'])  # 척도\n",
    "\n",
    "## 모델 학습 ##\n",
    "model.fit(X_train, Y_train,\n",
    "         batch_size=128, epochs=200, verbose=1, validation_split=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 22us/sample - loss: 0.2520 - accuracy: 0.9259\n",
      "Test accuracy: 0.9259\n"
     ]
    }
   ],
   "source": [
    "## 모델 평가 ##\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 활성화함수 sigmoid 모델링 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid, Adam, categorical_crossentropy, batch_size=64, 0.9805\n",
    "sigmoid, Adam, categorical_crossentropy, batch_size=128, 0.98\n",
    "\n",
    "sigmoid, RMSProp, categorical_crossentropy, batch_size=64, 0.9801\n",
    "sigmoid, RMSProp, categorical_crossentropy, batch_size=128, 0.9809\n",
    "\n",
    "sigmoid, SGD, categorical_crossentropy, batch_size=64, 0.9807\n",
    "sigmoid, SGD, categorical_crossentropy, batch_size=128, 0.9814 # 최적화기\n",
    "\n",
    "sigmoid, Adam, MSE, batch_size=64, 0.9802\n",
    "sigmoid, Adam, MSE, batch_size=128, 0.9796"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x7fd53e7a3410>"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = k.models.load_model('model/mnist_3(Adam).h5')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 정규화 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.regularizers.L1L2 at 0x7fd53f091a50>"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.regularizers.l1(l=0.01) # or 0.1 or 0.001\n",
    "tf.keras.regularizers.l2(l=0.01) # or 0.1 or 0.001\n",
    "tf.keras.regularizers.l1_l2(l1=0.01, l2=0.01) # or 0.1 or 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7fd53f088250>"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.layers.Dense(\n",
    "    128, input_dim=784,\n",
    "    kernel_initializer='ones',\n",
    "    kernel_regularizer=tf.keras.regularizers.l1(0.1),\n",
    "    activity_regularizer=tf.keras.regularizers.l2(0.1))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### learning rate 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_layer (Dense)          (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer_2 (Dense)        (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer_3 (Dense)        (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.9227 - accuracy: 0.7063 - val_loss: 0.3515 - val_accuracy: 0.9018\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.4482 - accuracy: 0.8656 - val_loss: 0.2635 - val_accuracy: 0.9238\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3649 - accuracy: 0.8930 - val_loss: 0.2243 - val_accuracy: 0.9350\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.3149 - accuracy: 0.9084 - val_loss: 0.2000 - val_accuracy: 0.9423\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.2822 - accuracy: 0.9181 - val_loss: 0.1774 - val_accuracy: 0.9488\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.2543 - accuracy: 0.9255 - val_loss: 0.1659 - val_accuracy: 0.9526\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.2331 - accuracy: 0.9309 - val_loss: 0.1571 - val_accuracy: 0.9542\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.2195 - accuracy: 0.9340 - val_loss: 0.1455 - val_accuracy: 0.9588\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.2090 - accuracy: 0.9386 - val_loss: 0.1400 - val_accuracy: 0.9596\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.1939 - accuracy: 0.9424 - val_loss: 0.1360 - val_accuracy: 0.9595\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1869 - accuracy: 0.9445 - val_loss: 0.1270 - val_accuracy: 0.9629\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.1777 - accuracy: 0.9478 - val_loss: 0.1232 - val_accuracy: 0.9644\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.1677 - accuracy: 0.9491 - val_loss: 0.1186 - val_accuracy: 0.9651\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1622 - accuracy: 0.9532 - val_loss: 0.1178 - val_accuracy: 0.9663\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1591 - accuracy: 0.9534 - val_loss: 0.1119 - val_accuracy: 0.9677\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.1519 - accuracy: 0.9544 - val_loss: 0.1111 - val_accuracy: 0.9674\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.1469 - accuracy: 0.9566 - val_loss: 0.1065 - val_accuracy: 0.9687\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1397 - accuracy: 0.9578 - val_loss: 0.1051 - val_accuracy: 0.9682\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1345 - accuracy: 0.9599 - val_loss: 0.1010 - val_accuracy: 0.9705\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1314 - accuracy: 0.9610 - val_loss: 0.1014 - val_accuracy: 0.9712\n",
      "10000/10000 [==============================] - 0s 22us/sample - loss: 0.0934 - accuracy: 0.9710\n",
      "Test accuracy: 0.971\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# network and training\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 256\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10   # number of outputs = number of digits\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT = 0.3\n",
    "OPTMIZER = SGD(lr=0.1)\n",
    "\n",
    "# loading MNIST dataset\n",
    "# verify\n",
    "# the split between train and test is 60,000, and 10,000 respectly \n",
    "# one-hot is automatically applied\n",
    "mnist = keras.datasets.mnist\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "#normalize in [0,1]\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "#one-hot\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
    "\n",
    "#build the model\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(N_HIDDEN,\n",
    "   \t\tinput_shape=(RESHAPED,),\n",
    "   \t\tname='dense_layer', activation='relu'))\n",
    "model.add(keras.layers.Dropout(DROPOUT))\n",
    "model.add(keras.layers.Dense(N_HIDDEN,\n",
    "   \t\tname='dense_layer_2', activation='relu'))\n",
    "model.add(keras.layers.Dropout(DROPOUT))\n",
    "model.add(keras.layers.Dense(NB_CLASSES,\n",
    "   \t\tname='dense_layer_3', activation='softmax'))\n",
    "\n",
    "# summary of the model\n",
    "model.summary()\n",
    "\n",
    "# compiling the model\n",
    "model.compile(optimizer=OPTMIZER, \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#training the moodel\n",
    "model.fit(X_train, Y_train,\n",
    "\t\tbatch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "\t\tverbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "#evalute the model\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 감정분석 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, preprocessing\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# max_len = 200\n",
    "# n_words = 10000\n",
    "# dim_enbedding = 256\n",
    "# epochs = 20\n",
    "# batch_size = 500 < 보폭 or 학습률\n",
    "\n",
    "def load_data():\n",
    "    # 데이터 로드\n",
    "    (X_train, y_train), (X_test, y_test) = datasets.imdb.load_data(num_words=10000)\n",
    "    # 문장을 max_len(200)이 되도록 채워 넣는다.\n",
    "    X_train = preprocessing.sequence.pad_sequences(X_train, maxlen=200)\n",
    "    X_test = preprocessing.sequence.pad_sequences(X_test, maxlen=200)\n",
    "    return (X_train, y_train), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    # 입력 : eEmbedding layer\n",
    "    # 모델은 크기의 정수 행렬을 입력으로 취한다(batch, input_length).\n",
    "    # 모델의 출력은 차원이다(input_length, dim_embedding).\n",
    "    # 입력 중 가장 큰 정수는 n_words(10000)보다 작거나 같다(어휘 크기).\n",
    "    model.add(layers.Embedding(10000, 256, input_length=200))  # n_words = 10000, dim_embedding = 256, max_len = 200\n",
    "    model.add(layers.Dropout(.3))\n",
    "    \n",
    "    # 각 n_words 특징에서 특징 벡터의 최댓값을 취한다.\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dropout(.5))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 200), (25000,), (25000, 200), (25000,))"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = load_data()\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   5,   25,  100,   43,  838,  112,   50,  670,    2,    9,   35,\n",
       "        480,  284,    5,  150,    4,  172,  112,  167,    2,  336,  385,\n",
       "         39,    4,  172, 4536, 1111,   17,  546,   38,   13,  447,    4,\n",
       "        192,   50,   16,    6,  147, 2025,   19,   14,   22,    4, 1920,\n",
       "       4613,  469,    4,   22,   71,   87,   12,   16,   43,  530,   38,\n",
       "         76,   15,   13, 1247,    4,   22,   17,  515,   17,   12,   16,\n",
       "        626,   18,    2,    5,   62,  386,   12,    8,  316,    8,  106,\n",
       "          5,    4, 2223, 5244,   16,  480,   66, 3785,   33,    4,  130,\n",
       "         12,   16,   38,  619,    5,   25,  124,   51,   36,  135,   48,\n",
       "         25, 1415,   33,    6,   22,   12,  215,   28,   77,   52,    5,\n",
       "         14,  407,   16,   82,    2,    8,    4,  107,  117, 5952,   15,\n",
       "        256,    4,    2,    7, 3766,    5,  723,   36,   71,   43,  530,\n",
       "        476,   26,  400,  317,   46,    7,    4,    2, 1029,   13,  104,\n",
       "         88,    4,  381,   15,  297,   98,   32, 2071,   56,   26,  141,\n",
       "          6,  194, 7486,   18,    4,  226,   22,   21,  134,  476,   26,\n",
       "        480,    5,  144,   30, 5535,   18,   51,   36,   28,  224,   92,\n",
       "         25,  104,    4,  226,   65,   16,   38, 1334,   88,   12,   16,\n",
       "        283,    5,   16, 4472,  113,  103,   32,   15,   16, 5345,   19,\n",
       "        178,   32], dtype=int32)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 256)          2560000   \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 200, 256)          0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,593,025\n",
      "Trainable params: 2,593,025\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## 설정한 함수를 이용해 모델 정의하기 ##\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모델 컴파일링 ##\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 14s 576us/sample - loss: 0.6715 - accuracy: 0.6174 - val_loss: 0.6271 - val_accuracy: 0.8381\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 15s 610us/sample - loss: 0.4564 - accuracy: 0.8405 - val_loss: 0.3620 - val_accuracy: 0.8591\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 14s 549us/sample - loss: 0.2792 - accuracy: 0.8856 - val_loss: 0.3062 - val_accuracy: 0.8754\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 14s 546us/sample - loss: 0.2150 - accuracy: 0.9172 - val_loss: 0.2899 - val_accuracy: 0.8790\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 14s 552us/sample - loss: 0.1705 - accuracy: 0.9374 - val_loss: 0.2879 - val_accuracy: 0.8779\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 14s 554us/sample - loss: 0.1336 - accuracy: 0.9554 - val_loss: 0.2949 - val_accuracy: 0.8728\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 14s 555us/sample - loss: 0.1022 - accuracy: 0.9666 - val_loss: 0.3064 - val_accuracy: 0.8682\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 14s 555us/sample - loss: 0.0780 - accuracy: 0.9777 - val_loss: 0.3205 - val_accuracy: 0.8650\n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 14s 556us/sample - loss: 0.0567 - accuracy: 0.9849 - val_loss: 0.3398 - val_accuracy: 0.8614\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 14s 553us/sample - loss: 0.0437 - accuracy: 0.9894 - val_loss: 0.3552 - val_accuracy: 0.8597\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 14s 553us/sample - loss: 0.0319 - accuracy: 0.9931 - val_loss: 0.3743 - val_accuracy: 0.8588\n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 14s 569us/sample - loss: 0.0253 - accuracy: 0.9946 - val_loss: 0.3898 - val_accuracy: 0.8559\n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 14s 541us/sample - loss: 0.0200 - accuracy: 0.9960 - val_loss: 0.4085 - val_accuracy: 0.8553\n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 13s 539us/sample - loss: 0.0156 - accuracy: 0.9974 - val_loss: 0.4232 - val_accuracy: 0.8554\n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 13s 539us/sample - loss: 0.0125 - accuracy: 0.9980 - val_loss: 0.4352 - val_accuracy: 0.8553\n",
      "Epoch 16/20\n",
      "25000/25000 [==============================] - 13s 539us/sample - loss: 0.0105 - accuracy: 0.9984 - val_loss: 0.4490 - val_accuracy: 0.8530\n",
      "Epoch 17/20\n",
      "25000/25000 [==============================] - 14s 546us/sample - loss: 0.0091 - accuracy: 0.9985 - val_loss: 0.4739 - val_accuracy: 0.8486\n",
      "Epoch 18/20\n",
      "25000/25000 [==============================] - 14s 544us/sample - loss: 0.0071 - accuracy: 0.9992 - val_loss: 0.4758 - val_accuracy: 0.8508\n",
      "Epoch 19/20\n",
      "25000/25000 [==============================] - 14s 553us/sample - loss: 0.0067 - accuracy: 0.9988 - val_loss: 0.4861 - val_accuracy: 0.8517\n",
      "Epoch 20/20\n",
      "25000/25000 [==============================] - 14s 551us/sample - loss: 0.0057 - accuracy: 0.9992 - val_loss: 0.5019 - val_accuracy: 0.8515\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'History' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-277-f373efae1b58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m score = model.fit(X_train, y_train,\n\u001b[1;32m      3\u001b[0m                  epochs=20, batch_size=500, validation_data=(X_test, y_test))\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nTest score:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'History' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "## 모델 학습 ##\n",
    "score = model.fit(X_train, y_train,\n",
    "                 epochs=20, batch_size=500, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1s 49us/sample - loss: 0.5019 - accuracy: 0.8515\n",
      "\n",
      "Test score: 0.5018726927042008\n",
      "Test accuracy: 0.85148\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, batch_size=500)\n",
    "print('\\nTest score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model/imdb.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prediction_1 = model.predict(X) # X : 예측할 데이터\\nprediction_2 = model.predict_proba(X) # X : 예측할 데이터'"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 예측하기 ##\n",
    "'''prediction_1 = model.predict(X) # X : 예측할 데이터\n",
    "prediction_2 = model.predict_proba(X) # X : 예측할 데이터'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
